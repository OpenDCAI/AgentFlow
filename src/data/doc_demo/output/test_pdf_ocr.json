[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    438,
                    326,
                    2116,
                    485
                ],
                "angle": 0,
                "content": "CAPRL: STIMULATING DENSE IMAGE CAPTION CAPABILITIES VIA REINFORCEMENT LEARNING"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    459,
                    557,
                    1756,
                    656
                ],
                "angle": 0,
                "content": "Long Xing\\(^{1,2}\\), Xiaoyi Dong\\(^{2,3}\\), Yuhang Zang\\(^{2}\\), Yuhang Cao\\(^{2}\\), Jianze Liang\\(^{2}\\), Qidong Huang\\(^{5}\\), Jiaqi Wang\\(^{2,4}\\), Feng Wu\\(^{1}\\), Dahua Lin\\(^{2,3}\\)."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    469,
                    660,
                    1734,
                    706
                ],
                "angle": 0,
                "content": "<sup>1</sup>University of Science and Technology of China, <sup>2</sup>Shanghai AI Laboratory,"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    469,
                    706,
                    1945,
                    752
                ],
                "angle": 0,
                "content": "<sup>3</sup>The Chinese University of Hong Kong, <sup>4</sup>Shanghai Innovation Institute, <sup>5</sup>Alibaba Cloud"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    469,
                    755,
                    1058,
                    798
                ],
                "angle": 0,
                "content": "xing_long@mail.ustc.edu.cn"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    469,
                    798,
                    1440,
                    844
                ],
                "angle": 0,
                "content": "Model & Data: CapRL HuggingFace Collection"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    469,
                    848,
                    1147,
                    894
                ],
                "angle": 0,
                "content": "Code: CapRL Github Repository"
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1150,
                    1013,
                    1392,
                    1059
                ],
                "angle": 0,
                "content": "ABSTRACT"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    586,
                    1105,
                    1960,
                    2303
                ],
                "angle": 0,
                "content": "Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \"good\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\(8.4\\%\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL"
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    443,
                    2376,
                    861,
                    2422
                ],
                "angle": 0,
                "content": "1 INTRODUCTION"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    433,
                    2475,
                    2108,
                    2798
                ],
                "angle": 0,
                "content": "The image captioning task (Karpathy & Fei-Fei 2015, Vinyals et al. 2015), which generates a natural language description for a given image, bridges the gap between the visual and linguistic worlds. The captioning capability is fundamental to various applications, including vision-language models like CLIP (Radford et al. 2021), which learn a shared embedding space for images and text. Furthermore, captions are often a core component in the pre-training stage of Large Vision-Language Models (LVLMs) (Liu et al. 2023b), where the model learns to align visual information with linguistic descriptions on a massive scale before being fine-tuned for other downstream tasks."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    433,
                    2821,
                    2111,
                    3055
                ],
                "angle": 0,
                "content": "Given the importance of image captioning, there is a strong need for captioning models that can provide dense and accurate descriptions. Most modern captioning models Chen et al. (2024b); Rotstein et al. (2024); Vasu et al. (2025) are trained based on LVLMs using Supervised Fine-Tuning (SFT). While effective, SFT requires large datasets annotated by humans or proprietary models, which are expensive and not scalable. Furthermore, image captioning is an inherently open-ended"
            },
            {
                "block_id": 13,
                "type": "aside_text",
                "bbox": [
                    58,
                    914,
                    155,
                    2379
                ],
                "angle": 270,
                "content": "arXiv:2509.22647v1 [cs.CV] 26 Sep 2025"
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "1"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    443,
                    333,
                    2101,
                    1518
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 1: (a) Existing Reward Models: Current LVLM-as-a-judge/reward models suffer from limitations like rewarding morbidity or brevity, leading to low-quality captions and reward hacking. (b) Our CapRL: CapRL uses a decoupled two-stage VQA approach to provide subjective rewards for captions. (c) CapRL's Advantage: CapRL outperforms previous subjective reward methods, as shown by training curves and higher performance in the Prism (Qiao et al. 2024) evaluation setting. problem, where a single image can be accurately described by a wide variety of captions. Since SFT models are trained to match a single ground-truth description for each image, they tend to memorize specific answers rather than learning the underlying concepts. As a result, the SFT models become less general and struggle to generate the diverse range of valid captions possible for a single image."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    436,
                    1993,
                    2103,
                    2362
                ],
                "angle": 0,
                "content": "The limitations of SFT have led to a recent paradigm shift in the post-training of LVLMs toward Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al. 2024). RLVR is the paradigm that trains models by providing clear and objective reward from the verifier, such as a binary signal of correctness for mathematical reasoning (e.g., DeepSeek-R1 (Guo et al. 2025)). Unlike SFT, which teaches a model to mimic a single ground-truth response, RLVR encourages the model to generate more diverse and robust outputs that meet the verifiable criteria. Our objective is to design a powerful and scalable RLVR training paradigm for the image captioning task to generate more creative and more general variety of accurate descriptions."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    436,
                    2382,
                    2108,
                    2937
                ],
                "angle": 0,
                "content": "However, applying RLVR to open-ended tasks like image captioning is challenging, primarily due to the difficulty of designing an objective reward function. A good caption can be subjective, with multiple valid descriptions possible for the same image. As shown in Fig.1(a), early studies fail to provide accurate reward signals for RL training. Using reward models (Liu et al. 2025b; Su et al. 2025; Lu 2025) or LLM-as-a-judge (Gunjal et al. 2025) to provide feedback is vulnerable to reward hacking. The captioning model learns to exploit weaknesses in the reward models (e.g., verbosity or brevity outputs) rather than producing a high-quality response. Moreover, it is difficult to create effective rubrics or evaluation prompts for LVLM-as-a-judge methods because captions are free-form and encode substantial information. Using reference answer as rewards (Gurung & Lapata, 2025; Yu et al. 2025) like ROUGE (Lin 2004) and BLEU (Papineni et al. 2002) is constrained when evaluating complex and long-form captions. Fig.1(c) further demonstrates the limitations of previous subjective caption rewards, showing reward hacking and unstable training curves."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    436,
                    2956,
                    2103,
                    3055
                ],
                "angle": 0,
                "content": "To design the objective RLVR reward function for the subjective image captioning task, we introduce a novel perspective, where a caption's quality is proportional to its utility. When the image caption"
            },
            {
                "block_id": 6,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "2"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    446,
                    336,
                    2085,
                    1029
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 2: Illustration of the captioning capability improvement CapRL brings to Qwen2.5-VL-3B."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    433,
                    1098,
                    2108,
                    1653
                ],
                "angle": 0,
                "content": "is detailed and accurate, a text-based LLM that can't directly \"see\" the image can still answer Visual Question Answering (VQA) questions about the image. For example, for the question \"What color is the frisbee?\", the LLM finds the phrase \"red frisbee\" in the caption and correctly answers \"red.\" Driven by this motivation, we present an effective decoupled two-stage pipeline, dubbed as Captioning Reinforcement Learning (CapRL), as shown in Fig. 1(b). Specifically, the reward of our CapRL framework is determined by how well a caption generated by an LVLM enables a separate non-visual LLM to answer Multiple-Choice Questions (MCQs) about the source image. The LLM's resulting accuracy serves as the objective reward for the RLVR training. To ensure the high-quality MCQs data that present enough knowledge required for VQA has been examined, we also developed a specific QA curation pipeline. The images are sampled from various sources, including natural images, charts, and documents. The questions and answers are filtered to ensure the questions can only be answered by analyzing the image content itself."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    436,
                    1673,
                    2111,
                    2138
                ],
                "angle": 0,
                "content": "We conduct a comprehensive evaluation of the significant benefits brought by CapRL. From a qualitative perspective, as shown in Fig. 2 applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From a quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al. 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of \\(8.4\\%\\). These results demonstrate that our CapRL framework, by leveraging objective reward design as a reliable optimization signal, effectively drives the model to produce dense and accurate captions."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    438,
                    2154,
                    1208,
                    2201
                ],
                "angle": 0,
                "content": "Our contributions are summarized as follows:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    436,
                    2224,
                    2103,
                    2415
                ],
                "angle": 0,
                "content": "1) We contribute the first study of applying Reinforcement Learning with Verifiable Rewards for the open-ended and subjective image captioning task. Unlike traditional Supervised Fine-Tuning, which can lead to models memorizing a limited set of annotated captions, our method allows the model to explore and generate a broader range of creative and general descriptions."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    436,
                    2432,
                    2103,
                    2620
                ],
                "angle": 0,
                "content": "2) We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task. We also created a specific QA curation pipeline to ensure the quality of the questions and answers used for the second stage."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    436,
                    2640,
                    2103,
                    2781
                ],
                "angle": 0,
                "content": "3) We carry out extensive experiments to verify the effectiveness of CapRL. Notably, both in the LVLM Pretraining setting for modality alignment and the Prism setting for caption informativeness evaluation, CapRL consistently exhibits superior performance compared to the baselines."
            },
            {
                "block_id": 9,
                "type": "list",
                "bbox": [
                    436,
                    2224,
                    2103,
                    2781
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    441,
                    2851,
                    879,
                    2900
                ],
                "angle": 0,
                "content": "2 RELATED WORK"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    433,
                    2956,
                    2111,
                    3059
                ],
                "angle": 0,
                "content": "Image Captioning. Early Large-scale image-text corpora (Schuhmann et al. 2022, Changpinyo et al. 2021, Thomee et al. 2016) have driven vision-language pretraining. To scale and improve"
            },
            {
                "block_id": 12,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "3"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    443,
                    339,
                    2101,
                    1072
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 3: Overview of CapRL. Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process. Captions generated by the LVLM, paired with curated MCQs (b), are used to query an LLM, whose resulting accuracy becomes the objective reward for the LVLM (a). Our CapRL offers a scalable framework for applying RLVR to the open-ended image captioning task."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    436,
                    1313,
                    2103,
                    1732
                ],
                "angle": 0,
                "content": "captions, researchers design advanced captioning pipelines: BLIP-LAION (Li et al. 2022) generates short synthetic captions, LaCLIP (Fan et al. 2023) uses ChatGPT to rewrite them, and CapsFusion (Yu et al. 2024) consolidates and refine information with fine-tuned models. Besides, there are many research projects which use GPT-4V and human-in-the-loop pipelines to produce richer, fine-grained annotations such as ShareGPT4V (Chen et al. 2024b) and ALLaVA (Chen et al. 2024a). Recent studies (Li et al. 2024b Sun et al. 2024) have explored multi-expert approaches to compensate for LVLM limitations. In summary, some works rely on complex pipelines with multiple models, training-free but costly at inference, while others require lots of expensive labeled data for SFT. In contrast, our CapRL achieves strong performance with remarkable data efficiency through RLVR."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    436,
                    1785,
                    2108,
                    2207
                ],
                "angle": 0,
                "content": "Reinforcement Learning with Verifiable Rewards (RLVR). RLVR (Lambert et al. 2024) represents a promising paradigm for training Large Language Models (LLMs) to tasks that have an objective, easily verifiable reward signal. For example, in mathematical problem-solving, the reward can be a binary signal of correctness (Shao et al. 2024), and for code generation, it can be whether the code passes unit tests (Team et al. 2025). Compared to the traditional Supervised Fine-Tuning (SFT), RLVR offers a more robust and scalable approach. While SFT trains models to imitate a set of provided ground-truth answers, often leading to models that memorize specific phrasings (Chu et al. 2025), RLVR encourages the model to explore and discover optimal solutions. This is particularly beneficial for problems with multiple valid answers or reasoning paths."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    441,
                    2277,
                    874,
                    2326
                ],
                "angle": 0,
                "content": "3 METHODOLOGY"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    436,
                    2385,
                    2103,
                    2626
                ],
                "angle": 0,
                "content": "An overview of our CapRL is shown in Fig.3 The CapRL framework consists of a novel, decoupled two-stage process. In the first stage, an LVLM generates a caption for an input image. In the second stage, this caption, along with a series of MCQs, is provided as input to an LLM. In the following, we will describe how to apply RLVR on the image captioning task via our CapRL in Section 3.1 Then we use the model trained with CapRL to construct the CapRL-5M dataset in Section 3.2"
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    441,
                    2682,
                    685,
                    2729
                ],
                "angle": 0,
                "content": "3.1 CAPRL"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    436,
                    2772,
                    2108,
                    3059
                ],
                "angle": 0,
                "content": "The design of the reward function is a pivotal factor in the success of RLVR-based approaches, since the reward function directly guides the optimization direction of the policy model. Although designing reward functions for objective tasks (Shao et al. 2024, Liu et al. 2025c, Luo et al. 2025) is straightforward, developing the reward function for the subjective image captioning task is challenging. While reward models (Liu et al. 2025b, Su et al. 2025, Lu 2025) or the “LLM-as-a-judge” approach (Gunjal et al. 2025) have been explored for RL training on open-ended tasks, these"
            },
            {
                "block_id": 9,
                "type": "page_number",
                "bbox": [
                    1257,
                    3131,
                    1287,
                    3168
                ],
                "angle": 0,
                "content": "4"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    589,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    436,
                    343,
                    2103,
                    438
                ],
                "angle": 0,
                "content": "models are still vulnerable to exploitation in captioning task, primarily owing to their intrinsic biases, which may unintentionally encourage the captioning model to produce verbose or brief results."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    436,
                    458,
                    2108,
                    877
                ],
                "angle": 0,
                "content": "To design a reliable verifiable reward module, we leverage a perception-reasoning decoupled VQA task as a proxy to evaluate the quality of captions. The overall process of our proposed method CapRL, is illustrated in Fig. During the GRPO training process, an image and an instruction are first provided as input to the policy model to sample a set of candidate captions. Each caption is then paired with corresponding questions and fed to a Large Language Model (LLM). We assign each caption a reward score based on the accuracy of answers generated by the LLM. Subsequently, we calculate the mean and variance of rewards across the group to derive the advantage for each caption. To ensure training stability, and consistent with the original GRPO framework, we incorporate a KL-divergence penalty. The policy model is then updated via policy gradient optimization."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    436,
                    894,
                    2103,
                    1082
                ],
                "angle": 0,
                "content": "To prepare the data for GRPO training, we constructed a VQA dataset composed exclusively of multiple-choice questions. This multiple-choice format facilitates the computation of verifiable rewards. Throughout this curation process, we utilized an LVLM to filter the data and prevent data leakage. Further details regarding our reward design and QA curation are provided below."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    436,
                    1102,
                    2108,
                    1564
                ],
                "angle": 0,
                "content": "Reward Design. Specifically, given an instruction and an image, the policy model \\(\\mathcal{M}_V\\) generates a set of captions \\(\\{c_1, c_2, \\ldots, c_G\\}\\). Each caption is then paired with questions related to the image and passed to a large language model (LLM), denoted as \\(\\mathcal{M}_L\\), for answering. Since the \\(\\mathcal{M}_L\\) does not have access to the image directly, its ability to answer the question correctly depends entirely on how comprehensive and accurate the caption is. Captions that include more relevant objects and detailed descriptions, are more likely to provide the necessary information for the LLM to answer a question correctly. In contrast, less informative captions are more likely to lead to incorrect answers. Since LLMs exhibit high stability in answering multiple-choice questions, and the evaluation of their responses only requires exact matching, the accuracy of the LLM's responses can therefore serve as a reliable indicator of caption quality. This question-answering process can be formulated as:"
            },
            {
                "block_id": 5,
                "type": "equation",
                "bbox": [
                    1099,
                    1574,
                    2103,
                    1623
                ],
                "angle": 0,
                "content": "\\[\na _ {m} = \\mathcal {M} _ {L} \\left(c _ {i}, q _ {m}\\right), \\tag {1}\n\\]"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    436,
                    1640,
                    2103,
                    1739
                ],
                "angle": 0,
                "content": "where \\( q_{m} \\) denotes the \\( m \\)-th question associated with current image \\( I \\), and \\( a_{m} \\) is the LLM's answer to that question. Then the reward for a single question is computed using a simple exact-match criterion:"
            },
            {
                "block_id": 7,
                "type": "equation",
                "bbox": [
                    1004,
                    1752,
                    2103,
                    1864
                ],
                "angle": 0,
                "content": "\\[\nr \\left(a _ {m}\\right) = \\left\\{ \\begin{array}{l l} 1, & \\text {i f} a _ {m} = \\mathrm {G T} _ {m}, \\\\ 0, & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {2}\n\\]"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    438,
                    1887,
                    1354,
                    1937
                ],
                "angle": 0,
                "content": "Here, \\(\\mathrm{GT}_m\\) is the ground-truth answer to question \\(q_m\\)."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    436,
                    1956,
                    2103,
                    2187
                ],
                "angle": 0,
                "content": "To eliminate potential bias in the LLM's preference for specific answer choices, we randomly shuffle the options each time a question is presented. Additionally, relying on a single answer to evaluate a caption lacks robustness. To ensure the stability of caption scoring, we sample \\( N \\) times from all the questions related to the image and let \\( \\mathcal{M}_L \\) answer them independently. The final reward for a caption is computed as the average accuracy over these \\( N \\) sampled questions. Formally:"
            },
            {
                "block_id": 10,
                "type": "equation",
                "bbox": [
                    724,
                    2191,
                    2103,
                    2326
                ],
                "angle": 0,
                "content": "\\[\nR _ {c _ {i}} = \\frac {1}{N} \\sum_ {k = 1} ^ {N} r \\left(\\mathcal {M} _ {L} \\left(c _ {i}, \\operatorname {S h u f f l e} \\left(q _ {m _ {k}}\\right)\\right)\\right), \\quad m _ {k} \\sim \\{1, \\dots , M \\}. \\tag {3}\n\\]"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    436,
                    2336,
                    2108,
                    2663
                ],
                "angle": 0,
                "content": "Here, \\(M\\) denotes the number of questions associated with the current image \\(I\\). Since we compute the caption reward directly from the original caption, there is no need to perform intermediate reasoning steps as in DeepSeek-R1, which first carries out a thinking process before formatting an answer. As a result, our method avoids the need for any format-specific rewards and retains a clean, flexible reward computation process that fully respects the free-form nature of the policy model's output. It is important to note that, in our GRPO training setup, Qwen2.5-3B-Instruct is used as \\(\\mathcal{M}_L\\) by default, which makes the overall training highly efficient."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    433,
                    2682,
                    2108,
                    3055
                ],
                "angle": 0,
                "content": "QA Curation. To train CapRL effectively, a high-quality VQA dataset \\((q, a)\\) with question \\(q\\) and answer \\(a\\) is required to provide reliable reward signals. We construct this VQA dataset using a structured three-stage curation pipeline. (1) Image Collection. We begin by sourcing diverse images from the web and existing open-source datasets, including natural scenes, charts, and documents, to maximize variety. (2) QA Generation. For each image, we then use Qwen2.5-VL-72B (Bai et al. 2025) to automatically generate multiple question-answer pairs. (3) QA Filtering. Finally, we implement a stringent QA filtering process to ensure the quality of the generated QA pairs. The QA filtering stage is to verify that all questions are strictly visually-grounded and answerable exclusively"
            },
            {
                "block_id": 13,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "5"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    589,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    436,
                    343,
                    2103,
                    491
                ],
                "angle": 0,
                "content": "through analysis of the image content. The final QA filtering stage is crucial to prevent information leakage and guarantees that the model must perform true visual understanding, rather than relying on external knowledge or cues within the question itself to answer the generated questions."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    436,
                    504,
                    1670,
                    554
                ],
                "angle": 0,
                "content": "Specifically, the filtered set of QA pairs, denoted as \\(\\mathcal{Q}\\), is then defined as:"
            },
            {
                "block_id": 3,
                "type": "equation",
                "bbox": [
                    823,
                    577,
                    2106,
                    636
                ],
                "angle": 0,
                "content": "\\[\n\\mathcal {Q} = \\{(q, a) \\in \\mathcal {D} \\mid \\mathcal {M} _ {V _ {f}} (q, I) = a \\wedge \\mathcal {M} _ {V _ {f}} (q) \\neq a \\}, \\tag {4}\n\\]"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    433,
                    686,
                    2103,
                    980
                ],
                "angle": 0,
                "content": "where \\((q, a)\\) is a question-answer pair from the initial generated dataset \\(\\mathcal{D}\\), \\(I\\) is the corresponding input image, \\(\\mathcal{M}_{V_f}\\) is the LVLM used in QA Filtering, \\(\\mathcal{M}_{V_f}(q, I)\\) represents the answer generated when conditioned on both the question \\(q\\) and the image \\(I\\), and \\(\\mathcal{M}_{V_f}(q)\\) is the answer generated when the image is omitted. According to Eq. (4), the QA filtering step ensures that each selected QA pair requires the image context to be answered correctly. To manage computational costs effectively, the QA filtering step is performed using the Qwen2.5-VL-3B model (Bai et al. 2025) as \\(\\mathcal{M}_{V_f}\\)."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    436,
                    990,
                    2103,
                    1092
                ],
                "angle": 0,
                "content": "After filtering, we retain approximately 75k images along with their corresponding QA pairs to train the CapRL captioning model. Please refer to Appendix C and E for the curation details."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    438,
                    1138,
                    938,
                    1184
                ],
                "angle": 0,
                "content": "3.2 CAPRL-5M DATASET"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    436,
                    1227,
                    2103,
                    1326
                ],
                "angle": 0,
                "content": "By employing our carefully designed CapRL training scheme, we obtained CapRL-3B, and further used this powerful captioner to annotate 5M images, ultimately forming CapRL-5M."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    433,
                    1343,
                    2108,
                    1808
                ],
                "angle": 0,
                "content": "Image Collection and Processing. In collecting images, we primarily considered diversity, quality, and safety. Among the currently high-quality open-source image datasets, ShareGPT4V-1M (Chen et al. 2024b) and DenseFusion-1M (Li et al. 2024b) are relatively large in scale. Since both datasets have already undergone extensive filtering and clustering to ensure image quality, we directly incorporated all images from them. To further enhance dataset diversity, we also gathered a large number of images from the web, spanning natural photographs, documents, charts, and user interfaces. However, the quality of web images is highly uneven, and they pose potential safety risks, which could severely impact both model training and deployment safety. To address this, we applied rigorous filtering and ultimately retained 3M high-quality images. Combined with the two open-source datasets, this yielded a total of 5M images. The detailed filtering process is described in Appendix D."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    436,
                    1824,
                    2106,
                    2059
                ],
                "angle": 0,
                "content": "Caption Model selection. In typical multimodal pretraining scenarios, the pretraining dataset often requires a massive number of image-text pairs, making annotation costs substantial. Considering practical applications, we decide to train a highly lightweight yet powerful captioner to keep annotation costs more acceptable. Specifically, we initialize the policy model with Qwen2.5-VL-3B and employ our CapRL framework, resulting in the powerful CapRL-3B model as the captioner."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    438,
                    2121,
                    836,
                    2171
                ],
                "angle": 0,
                "content": "4 EXPERIMENTS"
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    438,
                    2227,
                    969,
                    2273
                ],
                "angle": 0,
                "content": "4.1 PRETRAINING SETTING"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    436,
                    2313,
                    2103,
                    2412
                ],
                "angle": 0,
                "content": "To thoroughly evaluate the quality of the CapRL-5M dataset, we conduct comprehensive comparisons with widely used caption datasets from the open-source community."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    436,
                    2428,
                    2108,
                    2854
                ],
                "angle": 0,
                "content": "Implementation Details. In our setup, the language model is initialized with a pretrained LLM, the visual encoder with a pretrained ViT, and the MLP projector randomly, following a standard multimodal pretraining scheme. We conduct experiments under three settings: Qwen2.5-3B + Qwen2.5-ViT, Qwen2.5-7B + Qwen2.5-ViT, and InternLM2.5-7B + CLIP-ViT-L. Training follows the ShareGPT4V paradigm in three stages: Initial Alignment with BLIP-558K dataset (Li et al. 2022); Further Pretraining with diverse high-quality image-caption datasets; and SFT with Open-LLaVA-NeXT-1M (Chen & Xing 2024). For comparison, we adopt strong baselines including Vanilla, which skips Further Pretraining, ShareGPT4V-1M, DenseFusion-1M, and CapRL-1M (randomly sampled from CapRL-5M). Detailed training details are provided in Appendix F"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    436,
                    2864,
                    2108,
                    3055
                ],
                "angle": 0,
                "content": "Main Results. As shown in Table 1 when using CapRL-1M as the further pretraining dataset, performance on the vast majority of benchmarks surpasses both ShareGPT4V-1M and DenseFusion-1M. Specifically, under the Qwen2.5-3B + Qwen2.5-ViT setting, it exceeds DenseFusion-1M by \\(6.8\\%\\) on InfoVQA, and outperforms by \\(2.7\\%\\) and \\(3.6\\%\\) on DocVQA and ChartVQA. These remarkable"
            },
            {
                "block_id": 15,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "6"
            }
        ]
    },
    {
        "page_id": 6,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    453,
                    478,
                    2088,
                    1240
                ],
                "angle": 0,
                "content": "<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\"14\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-1M</td><td>56.2</td><td>87.3</td><td>78.0</td><td>55.1</td><td>45.5</td><td>62.0</td><td>30.3</td><td>70.5</td><td>47.0</td><td>50.0</td><td>72.9</td><td>61.6</td><td>59.7</td></tr><tr><td>CapRL-5M</td><td>61.5</td><td>90.0</td><td>80.5</td><td>57.6</td><td>48.1</td><td>63.2</td><td>30.9</td><td>73.1</td><td>50.4</td><td>52.6</td><td>74.7</td><td>62.6</td><td>62.0</td></tr><tr><td colspan=\"14\">Qwen2.5-7B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>47.6</td><td>83.7</td><td>77.1</td><td>55.9</td><td>47.4</td><td>60.4</td><td>29.4</td><td>72.1</td><td>48.1</td><td>47.1</td><td>72.4</td><td>62.7</td><td>58.7</td></tr><tr><td>ShareGPT4V-1M</td><td>49.8</td><td>85.1</td><td>75.7</td><td>56.8</td><td>46.6</td><td>60.9</td><td>31.8</td><td>71.9</td><td>48.4</td><td>45.9</td><td>72.2</td><td>62.7</td><td>59.0</td></tr><tr><td>DenseFusion-1M</td><td>53.5</td><td>87.8</td><td>76.7</td><td>58.6</td><td>46.3</td><td>61.0</td><td>31.1</td><td>72.6</td><td>48.6</td><td>49.7</td><td>72.5</td><td>63.1</td><td>60.2</td></tr><tr><td>CapRL-1M</td><td>59.9</td><td>89.5</td><td>80.6</td><td>58.9</td><td>50.4</td><td>63.1</td><td>32.2</td><td>72.1</td><td>51.3</td><td>50.5</td><td>75.3</td><td>63.2</td><td>62.2</td></tr><tr><td>CapRL-5M</td><td>63.4</td><td>91.4</td><td>81.5</td><td>61.4</td><td>50.8</td><td>63.2</td><td>34.9</td><td>72.7</td><td>52.6</td><td>52.6</td><td>76.9</td><td>63.8</td><td>63.8</td></tr><tr><td colspan=\"14\">InternLM2.5-7B + CLIP-ViT-L</td></tr><tr><td>Vanilla</td><td>37.4</td><td>73.2</td><td>68.7</td><td>56.9</td><td>44.2</td><td>58.2</td><td>30.7</td><td>70.7</td><td>47.0</td><td>43.1</td><td>71.8</td><td>64.9</td><td>55.6</td></tr><tr><td>ShareGPT4V-1M</td><td>38.9</td><td>73.8</td><td>69.8</td><td>56.3</td><td>44.8</td><td>59.9</td><td>33.2</td><td>72.6</td><td>46.2</td><td>43.3</td><td>72.7</td><td>65.0</td><td>56.4</td></tr><tr><td>DenseFusion-1M</td><td>39.3</td><td>76.4</td><td>70.8</td><td>59.7</td><td>44.5</td><td>60.3</td><td>34.1</td><td>72.2</td><td>47.9</td><td>44.0</td><td>73.7</td><td>65.5</td><td>57.4</td></tr><tr><td>CapRL-1M</td><td>43.3</td><td>80.0</td><td>75.8</td><td>58.0</td><td>49.6</td><td>62.8</td><td>34.1</td><td>73.4</td><td>50.2</td><td>46.6</td><td>76.0</td><td>65.8</td><td>59.6</td></tr><tr><td>CapRL-5M</td><td>47.0</td><td>83.5</td><td>77.7</td><td>59.7</td><td>50.4</td><td>63.5</td><td>38.9</td><td>73.7</td><td>53.3</td><td>54.3</td><td>77.6</td><td>66.3</td><td>62.2</td></tr></table>",
                "caption": "Table 2: Ablation on image sources. We annotate the images in ShareGPT4V-1M and DenseFusion-1M using CapRL-3B, and use them respectively as pretraining datasets for comparison."
            },
            {
                "block_id": 4,
                "type": "table",
                "bbox": [
                    479,
                    1356,
                    2057,
                    1719
                ],
                "angle": 0,
                "content": "<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\"14\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>CapRL-ShareGPT4V-1M</td><td>52.1</td><td>85.9</td><td>75.2</td><td>56.3</td><td>45.6</td><td>60.0</td><td>30.9</td><td>70.9</td><td>46.7</td><td>47.5</td><td>71.4</td><td>61.7</td><td>58.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-DenseFusion-1M</td><td>55.0</td><td>87.8</td><td>77.5</td><td>56.2</td><td>44.7</td><td>62.8</td><td>32.0</td><td>71.0</td><td>46.6</td><td>49.9</td><td>72.7</td><td>62.3</td><td>59.9</td></tr></table>"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    433,
                    1739,
                    2103,
                    2065
                ],
                "angle": 0,
                "content": "results indicate that CapRL-3B is effective for domains such as documents, charts, and infographics, which demand fine-grained perception and structured description. The captions in CapRL-1M are highly detailed and accurate for such image types, enabling LVLMs to achieve better modality alignment and a deeper understanding of the corresponding visual features. In addition, on natural image benchmarks such as MMStar and MMBench, CapRL-1M surpasses ShareGPT4V-1M by \\(1.6\\%\\) and \\(1.8\\%\\), suggesting that training with CapRL-1M enables multimodal models to acquire richer world knowledge for interpreting objects and their attributes in natural images."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    433,
                    2085,
                    2103,
                    2319
                ],
                "angle": 0,
                "content": "CapRL-5M further demonstrates consistently superior performance across all 12 benchmarks. These results highlight the strong scaling properties of the CapRL-3B-annotated dataset: as the training data size expands from 1M to 5M, model performance continues to improve steadily. This phenomenon underscores the practical value of CapRL for multimodal pretraining, as it enables the construction of high-quality, scalable datasets at very low annotation cost."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    433,
                    2339,
                    2103,
                    2709
                ],
                "angle": 0,
                "content": "Ablations about Image Sources. In the previous comparisons, the images used in each dataset are not identical. To better control for this variable, we fix the set of images and instead compare the effect of caption quality of different datasets under the Qwen2.5-3B + Qwen2.5-ViT setting. As shown in Table2 we compare CapRL with ShareGPT4V-1M and DenseFusion-1M. The results demonstrate that, when using the same set of images, further pretraining with the CapRL-3B-annotated dataset enables the LVLM to outperform the baselines by more than \\(2\\%\\). This finding indicates that the substantial advantage of the CapRL dataset over the baselines largely stems from the superior quality of its captions, rather than from differences in image diversity."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    433,
                    2729,
                    2108,
                    3055
                ],
                "angle": 0,
                "content": "Scaling Trend Comparison of Different Datasets. We further compare the scaling trend of CapRL and DenseFusion under Qwen2.5-3B + Qwen2.5-ViT setting. Specifically, we sample different numbers of image-caption pairs from each dataset for pretraining. As shown in Figure4 the CapRL dataset consistently outperforms the corresponding DenseFusion dataset across various scales of pretraining data. Moreover, the overall trend indicates that this performance gap continues to widen as the data size increases. This phenomenon highlights the strong scaling properties of the CapRL dataset, thanks to its high-quality captions, LVLMs continue to benefit as the dataset size grows."
            },
            {
                "block_id": 9,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "7"
            }
        ]
    },
    {
        "page_id": 7,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    446,
                    333,
                    872,
                    660
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    874,
                    333,
                    1277,
                    660
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1282,
                    333,
                    1680,
                    660
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1685,
                    336,
                    2085,
                    660
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 4: The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend."
            },
            {
                "block_id": 7,
                "type": "table",
                "bbox": [
                    443,
                    933,
                    2096,
                    1250
                ],
                "angle": 0,
                "content": "<table><tr><td>Caption\nModel</td><td>GRPO\nTrained</td><td>Chart\nQA</td><td>ChartQA\nPro</td><td>Info\nVQA</td><td>MMMU\nPro</td><td>Math\nVerse</td><td>Char\nXiv</td><td>We\nMath</td><td>Math\nVision</td><td>MMStar</td><td>SEED</td><td>MMMU</td><td>Average</td></tr><tr><td>Qwen2.5-VL-3B</td><td>X</td><td>65.6</td><td>27.1</td><td>40.2</td><td>28.6</td><td>32.8</td><td>21.8</td><td>54.4</td><td>22.6</td><td>46.4</td><td>64.1</td><td>35.1</td><td>39.9</td></tr><tr><td>Qwen2.5-VL-7B</td><td>X</td><td>74.9</td><td>35.4</td><td>56.4</td><td>30.1</td><td>36.4</td><td>24.8</td><td>57.0</td><td>23.3</td><td>50.7</td><td>67.1</td><td>37.9</td><td>44.9</td></tr><tr><td>Qwen2.5-VL-72B</td><td>X</td><td>80.2</td><td>38.0</td><td>60.8</td><td>34.1</td><td>39.9</td><td>30.7</td><td>60.2</td><td>24.5</td><td>55.0</td><td>69.3</td><td>39.4</td><td>48.3</td></tr><tr><td>UnifiedRW-as-Judge-3B</td><td>✓</td><td>54.9</td><td>25.1</td><td>33.6</td><td>28.1</td><td>34.6</td><td>20.4</td><td>58.2</td><td>24.5</td><td>45.4</td><td>61.2</td><td>36.3</td><td>38.4</td></tr><tr><td>Qwen2.5VL-as-Judge-3B</td><td>✓</td><td>71.4</td><td>34.2</td><td>49.3</td><td>29.1</td><td>33.8</td><td>22.9</td><td>54.3</td><td>24.1</td><td>47.7</td><td>64.5</td><td>36.4</td><td>42.5</td></tr><tr><td>CapRL-3B</td><td>✓</td><td>80.5</td><td>39.9</td><td>64.8</td><td>30.7</td><td>36.4</td><td>32.4</td><td>60.1</td><td>23.4</td><td>55.0</td><td>70.6</td><td>38.1</td><td>48.3</td></tr></table>",
                "caption": "Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-as-a-Judge as the reward. The best results are bold and the second-best results are underlined."
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    438,
                    1287,
                    1512,
                    1633
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1537,
                    1287,
                    2101,
                    1640
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 5: (Left) CapRL demonstrates strong generalization even when trained on images from a single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability. (Right) CapRL demonstrates promising scaling performance on QA training datasets."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    438,
                    1900,
                    833,
                    1947
                ],
                "angle": 0,
                "content": "4.2 PRISM SETTING"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    433,
                    1993,
                    2103,
                    2187
                ],
                "angle": 0,
                "content": "In the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al. 2024), and compare our CapRL-3B against other captioning models."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    433,
                    2201,
                    2103,
                    2620
                ],
                "angle": 0,
                "content": "Implementation details. Similar to our caption reward design, the Prism Framework decouples VQA into two stages. In Stage 1, the captioner generates captions about the input image. In Stage 2, an LLM answers questions based solely on the generated caption. We leverage the Prism framework primarily because it can evaluate caption quality in an objective and stable manner. In our setup, we fix Stage 2 with a fine-tuned Qwen2.5-3B-Instruct as the answering LLM, ensuring that benchmark performance directly reflects the quality of captions produced by the captioner. To assess the effect of different reward designs in GRPO, we include two other baseline models: one trained with UnifiedReward-2.0-qwen-3b (Wang et al. 2025), and the other with Qwen2.5-VL-3B as the judge for caption quality evaluation. The corresponding prompts are provided in Appendix C."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    433,
                    2636,
                    2108,
                    3052
                ],
                "angle": 0,
                "content": "Comparison with Qwen2.5-VL series. As shown in Table3 CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model. In chart and infographic understanding, CapRL-3B surpasses Qwen2.5-VL-3B by \\(14.9\\%\\), \\(12.8\\%\\), and \\(24.6\\%\\) on ChartQA, ChartQAPro, and InfoVQA, respectively. For natural image understanding, it leads Qwen2.5-VL-3B by \\(9.6\\%\\) and \\(6.5\\%\\) on MMStar and SEED. These results demonstrate that GRPO training has substantially unlocked the potential of Qwen2.5-VL-3B, enabling it to fully leverage its inherent knowledge to organize all objects and their attributes within an image into comprehensive and detailed captions. As a result, its perception capability is pushed to the limit, reaching a level comparable to that of the 72B model."
            },
            {
                "block_id": 15,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1285,
                    3168
                ],
                "angle": 0,
                "content": "8"
            }
        ]
    },
    {
        "page_id": 8,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    589,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    443,
                    386,
                    1275,
                    640
                ],
                "angle": 0,
                "content": "<table><tr><td>Caption \nModel</td><td>ChartQA \nPro</td><td>Info \nVQA</td><td>MMMU</td><td>MMStar</td><td>WeMath</td><td>Avg</td></tr><tr><td>Qwen2.5-VL-3B</td><td>27.1</td><td>40.2</td><td>35.1</td><td>46.4</td><td>54.4</td><td>40.6</td></tr><tr><td>CapRL-1QA-20k</td><td>35.5</td><td>59.8</td><td>36.6</td><td>50.8</td><td>57.3</td><td>48.0</td></tr><tr><td>CapRL-2QA-20k</td><td>36.8</td><td>60.2</td><td>37.6</td><td>51.1</td><td>56.6</td><td>48.5</td></tr><tr><td>CapRL-3QA-20k</td><td>36.9</td><td>60.3</td><td>36.9</td><td>51.3</td><td>56.8</td><td>48.5</td></tr></table>",
                "caption": "Table 5: Ablations about Sampling Rounds N."
            },
            {
                "block_id": 4,
                "type": "table",
                "bbox": [
                    1308,
                    386,
                    2073,
                    640
                ],
                "angle": 0,
                "content": "<table><tr><td>Sampling Rounds</td><td>ChartQA Pro</td><td>Info VQA</td><td>MMMU</td><td>MMStar</td><td>WeMath</td><td>Avg</td></tr><tr><td>N=1</td><td>35.4</td><td>58.1</td><td>36.5</td><td>50.2</td><td>56.1</td><td>47.3</td></tr><tr><td>N=2</td><td>36.2</td><td>59.1</td><td>36.3</td><td>49.3</td><td>56.9</td><td>47.6</td></tr><tr><td>N=4</td><td>36.7</td><td>59.9</td><td>37.1</td><td>50.9</td><td>57.3</td><td>48.4</td></tr><tr><td>N=8</td><td>36.9</td><td>59.6</td><td>36.5</td><td>50.8</td><td>57.7</td><td>48.3</td></tr></table>"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    436,
                    699,
                    2108,
                    1349
                ],
                "angle": 0,
                "content": "Comparison with LVLM-as-a-Judge reward. In our comparison with other reward design methods, we observe that when using UnifiedReward-2.0-qwen-3b as the judge to evaluate caption quality, the model's captioning ability actually deteriorates during GRPO training. We attribute this to the severe bias present in UnifiedReward-2.0-qwen-3b: during its training, it was exposed to lots of captions from text-to-image datasets, which are typically short and only describe the main objects. As a result, the UnifiedReward model tends to favor shorter captions. As shown in Fig.1 the average caption length during training continuously decreases and eventually collapses to producing only “:description”. Conversely, when using Qwen2.5-VL-3B as the judge, the bias is in the opposite direction: it prefers overly verbose captions. This makes the policy model prone to exploiting the bias by generating long passages of content irrelevant to the image, thereby satisfying the judge model's preference. As shown in Table3 the captioning ability under this reward shows significantly inferior to CapRL. Specific examples of such cases are illustrated in Fig.9 Fig.10 Fig.11 All these observations highlight that LVLM-as-a-Judge reward is fundamentally unreliable. This further underscores the advantage of CapRL, which converts subjective evaluations into objective assessments."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    438,
                    1465,
                    1377,
                    1511
                ],
                "angle": 0,
                "content": "4.3 COMPREHENSIVE DISCUSSION ABOUT CAPRL"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    436,
                    1577,
                    2103,
                    1673
                ],
                "angle": 0,
                "content": "In this section, we provide a comprehensive analysis and discussion of CapRL. These results further confirm CapRL's general applicability, robustness, and effectiveness."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    436,
                    1692,
                    2108,
                    2065
                ],
                "angle": 0,
                "content": "CapRL demonstrates strong generalization even when trained on images from a single domain. We further investigate the effect of different image sources in the QA dataset used for GRPO training. To this end, we classify the images into two categories using Qwen2.5-VL-3B: (1) documents, charts, or infographics, and (2) natural images. From each category, we sample 20k images for comparison. As illustrated in Fig. 5 (Left), models trained exclusively on chart-type images via GRPO exhibit substantial gains over Qwen2.5-VL-3B, not only in document and chart understanding but also in general VQA tasks. This demonstrates the strong generalization of CapRL-induced captioning improvements beyond the domains encountered during training."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    436,
                    2085,
                    2103,
                    2412
                ],
                "angle": 0,
                "content": "CapRL demonstrates promising scaling performance on training data. We conduct training on different amounts of QA data to evaluate the scaling behavior. As shown in Fig. [5] (Right), the model's performance improves steadily as the amount of QA data increases. These results indicate that our CapRL framework exhibits highly promising scaling potential. With the continued expansion of the training data, the captioning ability can be further enhanced, unlocking additional potential of Qwen2.5-VL-3B. Given its relatively small parameter size and excellent scaling properties, this approach holds strong promise for application in industrial-scale multimodal pretraining."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    436,
                    2428,
                    2108,
                    2801
                ],
                "angle": 0,
                "content": "Sparse QA supervision is sufficient for CapRL. We further examine the effect of varying the number of QA pairs per image. Specifically, we randomly sample 20k images that retain three QA pairs after filtering, obtain CapRL-3QA-20k after training. By controlling the number of QA pairs per image, we also construct CapRL-1QA-20k and CapRL-2QA-20k. The results, presented in Table4 show that even with only a single QA pair per image, Qwen2.5-VL-3B achieves a substantial improvement in captioning performance, averaging 7.4% higher than the baseline and only 0.5% lower than CapRL-2QA-20k. This highlights the remarkable efficiency of CapRL: highly sparse QA supervision is sufficient to unlock significant gains in captioning ability."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    436,
                    2818,
                    2113,
                    3059
                ],
                "angle": 0,
                "content": "Ablations about sampling rounds N. Results are shown in Table5 performance improves steadily when \\(N\\) increases from 1 to 4, and reaches saturation at \\(N = 8\\). The relatively poor performance at \\(N = 1\\) can be explained by the fact that each question is answered by the LLM only once, without sufficient shuffling of the options. Due to inherent option biases in the LLM, the measured accuracy fails to serve as a reliable proxy for reward, thereby misdirecting the optimization of the policy model."
            },
            {
                "block_id": 12,
                "type": "page_number",
                "bbox": [
                    1259,
                    3131,
                    1290,
                    3168
                ],
                "angle": 0,
                "content": "9"
            }
        ]
    },
    {
        "page_id": 9,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    155
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    441,
                    339,
                    818,
                    386
                ],
                "angle": 0,
                "content": "5 CONCLUSION"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    436,
                    438,
                    2111,
                    719
                ],
                "angle": 0,
                "content": "In this work, we introduce CapRL, a novel framework that successfully applies RLVR to the subjective task of image captioning. By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal. Our results show that CapRL effectively encourages models to generate dense and precise image descriptions, which in turn substantially promote modality alignment in LVLM pretraining. This work marks a significant step away from the restrictive, data-hungry SFT paradigm for RLVR in open-ended tasks."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    443,
                    785,
                    734,
                    831
                ],
                "angle": 0,
                "content": "REFERENCES"
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    443,
                    858,
                    2113,
                    996
                ],
                "angle": 0,
                "content": "Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic dedduplication. arXiv preprint arXiv:2303.09540, 2023."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    443,
                    1023,
                    2111,
                    1161
                ],
                "angle": 0,
                "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    443,
                    1191,
                    2111,
                    1376
                ],
                "angle": 0,
                "content": "Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Bytheway: Boost your text-to-video generation model to higher quality in a training-free way. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 12999-13008, 2025."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    443,
                    1405,
                    2103,
                    1547
                ],
                "angle": 0,
                "content": "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3558-3568, 2021."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    443,
                    1574,
                    2103,
                    1716
                ],
                "angle": 0,
                "content": "Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. arXiv preprint arXiv:2402.11684, 2024a."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    443,
                    1739,
                    2111,
                    1884
                ],
                "angle": 0,
                "content": "Lin Chen and Long Xing. Open-llava- next: An open-source implementation of llava- next series for fac- cilitating the large multi-modal model community. https://github.com/xiaoachen98/ Open-LLaVA-NeXT, 2024."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    446,
                    1907,
                    2111,
                    2006
                ],
                "angle": 0,
                "content": "Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. ShareGPT4V: Improving large multi-modal models with better captions. In ECCV, 2024b."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    443,
                    2026,
                    2111,
                    2171
                ],
                "angle": 0,
                "content": "Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:19472-19495, 2024c."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    443,
                    2194,
                    2111,
                    2339
                ],
                "angle": 0,
                "content": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma. SFT Memorizes, RL Generalizes: A comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    443,
                    2359,
                    2111,
                    2547
                ],
                "angle": 0,
                "content": "Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    443,
                    2574,
                    2111,
                    2715
                ],
                "angle": 0,
                "content": "Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36:35544-35575, 2023."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    443,
                    2739,
                    2111,
                    2884
                ],
                "angle": 0,
                "content": "Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    443,
                    2907,
                    2111,
                    3052
                ],
                "angle": 0,
                "content": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    443,
                    858,
                    2113,
                    3052
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "10"
            }
        ]
    },
    {
        "page_id": 10,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    446,
                    339,
                    2106,
                    438
                ],
                "angle": 0,
                "content": "Alexander Gurung and Mirella Lapata. Learning to reason for long-form story generation. arXiv preprint arXiv:2503.22828, 2025."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    446,
                    465,
                    2108,
                    564
                ],
                "angle": 0,
                "content": "Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    441,
                    590,
                    2108,
                    735
                ],
                "angle": 0,
                "content": "Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    441,
                    759,
                    2108,
                    904
                ],
                "angle": 0,
                "content": "Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, and Dahua Lin. Beyond fixed: Training-free variable-length denoising for diffusion large language models. arXiv preprint arXiv:2508.00819, 2025."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    441,
                    930,
                    2108,
                    1072
                ],
                "angle": 0,
                "content": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 12888-12900. PMLR, 2022."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    441,
                    1102,
                    2103,
                    1244
                ],
                "angle": 0,
                "content": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 19730-19742. PMLR, 2023."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    441,
                    1270,
                    2108,
                    1412
                ],
                "angle": 0,
                "content": "Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, and Qi Liu. Temporal reasoning transfer from text to video. arXiv preprint arXiv:2410.06166, 2024a."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    441,
                    1442,
                    2108,
                    1584
                ],
                "angle": 0,
                "content": "Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024b."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    441,
                    1610,
                    1958,
                    1666
                ],
                "angle": 0,
                "content": "Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In ACL, 2004."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    441,
                    1692,
                    2108,
                    1834
                ],
                "angle": 0,
                "content": "Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    441,
                    1861,
                    2024,
                    1917
                ],
                "angle": 0,
                "content": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    441,
                    1943,
                    2108,
                    2036
                ],
                "angle": 0,
                "content": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023b."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    441,
                    2065,
                    2108,
                    2211
                ],
                "angle": 0,
                "content": "Zihan Liu, Shuangrui Ding, Zhixiong Zhang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Songgen: A single stage auto-regressive transformer for text-to-song generation. arXiv preprint arXiv:2502.13128, 2025a."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    441,
                    2237,
                    2108,
                    2339
                ],
                "angle": 0,
                "content": "Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025b."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    441,
                    2362,
                    2103,
                    2461
                ],
                "angle": 0,
                "content": "Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Rar: Retrieving and ranking augmented mllms for visual recognition, 2024a."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    441,
                    2488,
                    2108,
                    2630
                ],
                "angle": 0,
                "content": "Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Mia-dpo: Multi-image augmented direct preference optimization for large vision-language models. arXiv preprint arXiv:2410.17637, 2024b."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    441,
                    2656,
                    2103,
                    2758
                ],
                "angle": 0,
                "content": "Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025c."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    441,
                    2785,
                    2108,
                    2930
                ],
                "angle": 0,
                "content": "Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning, 2025d. URL https://arxiv.org/abs/2505.14246"
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    441,
                    2953,
                    2103,
                    3052
                ],
                "angle": 0,
                "content": "Xun Lu. Writing-zero: Bridge the gap between non-verifiable problems and verifiable rewards. arXiv preprint arXiv:2506.00103, 2025."
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    441,
                    339,
                    2108,
                    3052
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1292,
                    3168
                ],
                "angle": 0,
                "content": "11"
            }
        ]
    },
    {
        "page_id": 11,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    446,
                    339,
                    2106,
                    438
                ],
                "angle": 0,
                "content": "Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    441,
                    468,
                    2113,
                    617
                ],
                "angle": 0,
                "content": "Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    446,
                    650,
                    2103,
                    749
                ],
                "angle": 0,
                "content": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL, 2002."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    443,
                    778,
                    2108,
                    927
                ],
                "angle": 0,
                "content": "Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3d: Customized 3d assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191, 2024."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    446,
                    960,
                    2108,
                    1098
                ],
                "angle": 0,
                "content": "Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Prism: A framework for decoupling and assessing the capabilities of vlms. In NeurIPS, 2024."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    446,
                    1135,
                    2108,
                    1280
                ],
                "angle": 0,
                "content": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    446,
                    1313,
                    2103,
                    1455
                ],
                "angle": 0,
                "content": "Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14313-14323, 2024."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    446,
                    1491,
                    2101,
                    1587
                ],
                "angle": 0,
                "content": "Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, and Ron Kimmel. FuseCap: Leveraging large language models for enriched fused image captions. In WCAV, 2024."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    446,
                    1620,
                    2106,
                    1808
                ],
                "angle": 0,
                "content": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:25278-25294, 2022."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    446,
                    1848,
                    2108,
                    1986
                ],
                "angle": 0,
                "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepseekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    446,
                    2022,
                    2106,
                    2161
                ],
                "angle": 0,
                "content": "Kai Shu, Le Wu, Yuchang Zhao, Aiping Liu, Ruobing Qian, and Xun Chen. Data augmentation for seizure prediction with generative diffusion model. IEEE Transactions on Cognitive and Developmental Systems, 2024."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    446,
                    2201,
                    2108,
                    2343
                ],
                "angle": 0,
                "content": "Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    446,
                    2379,
                    2103,
                    2517
                ],
                "angle": 0,
                "content": "Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, and Jingdong Wang. Descriptive caption enhancement with visual specialists for multimodal perception. arXiv preprint arXiv:2412.14233, 2024."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    446,
                    2554,
                    2108,
                    2696
                ],
                "angle": 0,
                "content": "Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. Seagent: Self-evolving computer use agent with autonomous learning from experience. arXiv preprint arXiv:2508.04700, 2025."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    446,
                    2732,
                    2103,
                    2874
                ],
                "angle": 0,
                "content": "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi K1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    446,
                    2910,
                    2103,
                    3052
                ],
                "angle": 0,
                "content": "Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64-73, 2016."
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    441,
                    339,
                    2113,
                    3052
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "12"
            }
        ]
    },
    {
        "page_id": 12,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    446,
                    339,
                    2106,
                    485
                ],
                "angle": 0,
                "content": "Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. FastVLM: Efficient vision encoding for vision language models. In CVPR, 2025."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    446,
                    511,
                    2103,
                    610
                ],
                "angle": 0,
                "content": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    448,
                    640,
                    2101,
                    735
                ],
                "angle": 0,
                "content": "Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    446,
                    762,
                    2101,
                    904
                ],
                "angle": 0,
                "content": "Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? In International Conference on Machine Learning, 2025."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    446,
                    933,
                    2101,
                    1072
                ],
                "angle": 0,
                "content": "Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    446,
                    1102,
                    2106,
                    1240
                ],
                "angle": 0,
                "content": "Linli Yao, Weiying Wang, and Qin Jin. Image difference captioning with pre-training and contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3108-3116, 2022."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    446,
                    1273,
                    2101,
                    1412
                ],
                "angle": 0,
                "content": "Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985, 2024."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    446,
                    1442,
                    2101,
                    1584
                ],
                "angle": 0,
                "content": "Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, et al. Timechat-online: \\(80\\%\\) visual tokens are naturally redundant in streaming videos. arXiv preprint arXiv:2504.17343, 2025a."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    446,
                    1610,
                    2106,
                    1752
                ],
                "angle": 0,
                "content": "Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, and Junnan Li. Generative frame sampler for long video understanding. arXiv preprint arXiv:2503.09146, 2025b."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    446,
                    1782,
                    2101,
                    1880
                ],
                "angle": 0,
                "content": "Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In CVPR, 2024."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    446,
                    1907,
                    2101,
                    2049
                ],
                "angle": 0,
                "content": "Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. RLPR: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    446,
                    2079,
                    2106,
                    2220
                ],
                "angle": 0,
                "content": "Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. Advances in Neural Information Processing Systems, 36:23570-23589, 2023."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    446,
                    2247,
                    2106,
                    2385
                ],
                "angle": 0,
                "content": "Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European conference on computer vision, pp. 310-325. Springer, 2024a."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    446,
                    2418,
                    2106,
                    2560
                ],
                "angle": 0,
                "content": "Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Booststep: Boosting mathematical capability of large language models via improved single-step reasoning. arXiv preprint arXiv:2501.03226, 2025."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    446,
                    2590,
                    2106,
                    2772
                ],
                "angle": 0,
                "content": "Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: A comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024b."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    446,
                    2805,
                    2106,
                    2946
                ],
                "angle": 0,
                "content": "Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, et al. Light-a-video: Training-free video relighting via progressive light fusion. arXiv preprint arXiv:2502.08590, 2025."
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    446,
                    339,
                    2106,
                    2946
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "13"
            }
        ]
    },
    {
        "page_id": 13,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    589,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    441,
                    339,
                    854,
                    386
                ],
                "angle": 0,
                "content": "A CAPRL CASES"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    436,
                    452,
                    2108,
                    544
                ],
                "angle": 0,
                "content": "We provide further illustrative examples of CapRL-3B to highlight its surprising captioning capabilities in this section."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    436,
                    567,
                    2103,
                    851
                ],
                "angle": 0,
                "content": "Comparison with Qwen2.5-VL-3B. As illustrated in Fig. 6 CapRL-3B demonstrates remarkable capability in understanding infographics, providing information that is both comprehensive and accurate. In contrast, Qwen2.5-VL-3B, as shown in Fig. 7 makes numerous errors in identifying key information within infographics. Furthermore, Fig. 8 highlights that CapRL-3B achieves substantially higher accuracy in chart understanding compared to Qwen2.5-VL-3B. Similarly, in the case of physical image understanding in Fig. 12 CapRL-3B also demonstrates clear superiority."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    436,
                    867,
                    2108,
                    1188
                ],
                "angle": 0,
                "content": "Comparison with UnifiedRW-as-Judge-3B and Qwen2.5VL-as-Judge-3B. To intuitively illustrate the issues introduced by LVLM-as-a-Judge rewards, we present examples of captions produced by models trained with different methods in Fig.9, Fig.11 and 10. The Qwen2.5VL-as-a-Judge-3B model tends to ignore key visual information in the image and outputs lengthy, irrelevant content, such as repeatedly asserting that its caption is of high quality in order to exploit reward hacking. In contrast, UnifiedRW-as-Judge-3B produces overly short captions that omit substantial amounts of critical chart information."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    436,
                    1211,
                    2113,
                    1310
                ],
                "angle": 0,
                "content": "More cases of CapRL-3B in understanding infographics and natural images. Fig. 13 and Fig. 14 provide additional evidence of the impressive perceptual capacity demonstrated by CapRL-3B."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    438,
                    1389,
                    1545,
                    1438
                ],
                "angle": 0,
                "content": "B MORE ANALYSIS EXPERIMENTS ABOUT CAPRL"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    436,
                    1504,
                    1188,
                    1871
                ],
                "angle": 0,
                "content": "Leaking QA Data Leads to Performance Degradation. We randomly sample \\(20 \\mathrm{~k}\\) instances and construct two training conditions: one using the retained QA and the other using the filtered QA. As shown in Table 6, the model trained on the leaking data performs on average \\(1.1 \\%\\) worse than the one trained on high-quality data. This indicates that leak-"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1211,
                    1514,
                    2108,
                    1656
                ],
                "angle": 0,
                "content": "Table 6: Comparison between training with data containing leakage issues and training with filtered data. Leaking data leads to an obvious performance drop."
            },
            {
                "block_id": 9,
                "type": "table",
                "bbox": [
                    1280,
                    1669,
                    2037,
                    1854
                ],
                "angle": 0,
                "content": "<table><tr><td>Training Data</td><td>ChartQA Pro</td><td>Info VQA</td><td>MMMU</td><td>MMStar</td><td>WeMath</td><td>Avg</td></tr><tr><td>Leaking20k</td><td>36.4</td><td>58.9</td><td>36.1</td><td>50.7</td><td>55.1</td><td>47.4</td></tr><tr><td>Refined20k</td><td>36.8</td><td>60.2</td><td>37.6</td><td>51.1</td><td>56.6</td><td>48.5</td></tr></table>"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    436,
                    1874,
                    2103,
                    2055
                ],
                "angle": 0,
                "content": "ing QA introduces spurious reward signals that mislead the optimization of the policy model. Even when captions are not closely aligned with the image content, the LLM may still achieve high answer accuracy, thereby preventing higher rewards from being correctly assigned to genuinely better captions."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    438,
                    2141,
                    851,
                    2187
                ],
                "angle": 0,
                "content": "C PROMPT USED"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    425,
                    2253,
                    2103,
                    2448
                ],
                "angle": 0,
                "content": "We provide all the prompts employed in our experiments in this section. Specifically, the prompt used in CapRL for guiding the LLM to answer questions conditioned on captions is illustrated in Figure 15; the prompt for utilizing the Unified Reward Model as the reward model is shown in Figure 16, and the prompt for adopting Qwen2.5-VL-3B as the reward model is presented in Figure 17"
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    438,
                    2524,
                    938,
                    2570
                ],
                "angle": 0,
                "content": "D DATA PROCESSING"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    436,
                    2636,
                    2108,
                    3055
                ],
                "angle": 0,
                "content": "To ensure both quality and safety, our data processing pipeline consists of three main stages. First, inspired by SemDeDup (Abbas et al. 2023), we construct clusters to identify and remove images with redundant semantics. During this step, we also discard low-resolution and overly simple images, while filtering out content that involves violence, pornography, or other safety concerns. Second, to avoid benchmark leakage, we integrate the images used in commonly referenced evaluation datasets and form clusters with them. Any images from our collection that are overly similar to benchmark samples are eliminated. Third, we conduct a safety inspection through human verification. Annotators perform sample-based screening, and once the proportion of unsafe images falls to a negligible level, we stop filtering. Following this process, we obtain the final dataset, CapRL-5M."
            },
            {
                "block_id": 15,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "14"
            }
        ]
    },
    {
        "page_id": 14,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    466,
                    356,
                    650,
                    396
                ],
                "angle": 0,
                "content": "CapRL-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    464,
                    402,
                    1287,
                    534
                ],
                "angle": 0,
                "content": "The image is an infographic titled \"Kred 2012: 10 Influential Team USA Olympians,\" highlighting the most influential Olympic athletes from Team USA in 2012, particularly focusing on their Olympic medals and social media influence scores. Here is a detailed description:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    471,
                    534,
                    790,
                    564
                ],
                "angle": 0,
                "content": "1. **Header and Awards:**"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    466,
                    567,
                    1272,
                    633
                ],
                "angle": 0,
                "content": "- The image features three main athletes with their respective medals and Kred scores:"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    466,
                    636,
                    1244,
                    699
                ],
                "angle": 0,
                "content": "- LeBron James (Gold Medal in Basketball) with a Kred score of 999 and a 6 medal."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    466,
                    702,
                    1275,
                    765
                ],
                "angle": 0,
                "content": "- Michael Phelps (Silver Medal in Swimming) with a Kred score of 977 and a 6 medal."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    466,
                    768,
                    1277,
                    834
                ],
                "angle": 0,
                "content": "- Hope Solo (Bronze Medal in Soccer) with a Kred score of 977 and a 5 medal."
            },
            {
                "block_id": 8,
                "type": "list",
                "bbox": [
                    466,
                    636,
                    1277,
                    834
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    466,
                    867,
                    762,
                    897
                ],
                "angle": 0,
                "content": "2. \\*\\*Staircase of Medals:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    464,
                    904,
                    1280,
                    1039
                ],
                "angle": 0,
                "content": "- The image uses a graphical representation with a blue background adorned with stars, indicative of an American theme. The top shows Lebron James with the highest Kred score, followed by Michael Phelps and Hope Solo with identical scores."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    466,
                    1069,
                    785,
                    1098
                ],
                "angle": 0,
                "content": "3. **Social Media Metrics:"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    489,
                    1102,
                    897,
                    1131
                ],
                "angle": 0,
                "content": "- Followers counts for each athlete:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    502,
                    1135,
                    920,
                    1168
                ],
                "angle": 0,
                "content": "- Michael Phelps: 689,591 followers"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    502,
                    1171,
                    1035,
                    1204
                ],
                "angle": 0,
                "content": "- LeBron James: 5,627,723 followers (highest)"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    502,
                    1204,
                    869,
                    1234
                ],
                "angle": 0,
                "content": "- Hope Solo: 542,813 followers"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    499,
                    1237,
                    940,
                    1270
                ],
                "angle": 0,
                "content": "- Retweeted times and replies received:"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    502,
                    1273,
                    969,
                    1303
                ],
                "angle": 0,
                "content": "- Phelps: 63,983 retweets, 75,695 replies"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    502,
                    1306,
                    1032,
                    1336
                ],
                "angle": 0,
                "content": "James: 1,072,729 retweets, 1,250,351 replies"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    502,
                    1339,
                    958,
                    1369
                ],
                "angle": 0,
                "content": "-Solo:75,014retweets,148,310 replies"
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    499,
                    1102,
                    1035,
                    1369
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    466,
                    1402,
                    1119,
                    1438
                ],
                "angle": 0,
                "content": "4. **Athletes List with Kred Scores for Different Events:"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    466,
                    1442,
                    1229,
                    1501
                ],
                "angle": 0,
                "content": "- The bottom section lists ten athletes along with their events and Kred scores:"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    502,
                    1504,
                    999,
                    1537
                ],
                "angle": 0,
                "content": "- Durant (Basketball) with 992 and 8 points"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    502,
                    1541,
                    1007,
                    1570
                ],
                "angle": 0,
                "content": "- Lochte (Swimming) with 975 and 6 points"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    502,
                    1574,
                    1020,
                    1603
                ],
                "angle": 0,
                "content": "- Wieber (Gymnastics) with 937 and 5 points"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    502,
                    1610,
                    1027,
                    1643
                ],
                "angle": 0,
                "content": "- Raisman (Gymnastics) with 912 and 6 points"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    502,
                    1646,
                    1022,
                    1676
                ],
                "angle": 0,
                "content": "- Douglas (Gymnastics) with 899 and 7 points"
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    502,
                    1679,
                    1020,
                    1709
                ],
                "angle": 0,
                "content": "- Franklin (Swimming) with 891 and 5 points"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    502,
                    1712,
                    1119,
                    1742
                ],
                "angle": 0,
                "content": "- Richards-Roes (Track & Field) with 880 and 8 points"
            },
            {
                "block_id": 30,
                "type": "list",
                "bbox": [
                    502,
                    1504,
                    1119,
                    1742
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 31,
                "type": "image",
                "bbox": [
                    1290,
                    376,
                    2080,
                    1765
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 32,
                "type": "title",
                "bbox": [
                    466,
                    1775,
                    836,
                    1808
                ],
                "angle": 0,
                "content": "5. **Celebrity Attention Events:"
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    499,
                    1811,
                    1241,
                    1841
                ],
                "angle": 0,
                "content": "- A section highlights which events are winning celebrity attention:"
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    466,
                    1844,
                    2042,
                    1907
                ],
                "angle": 0,
                "content": "- Wendy Williams (@WendyWilliams) on 30 July: Talking about \"GIRL POWER, GO TEAMUSA. #LONDONOLYMPICS\" with a link to usagym.org/pages/post.htm"
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    499,
                    1910,
                    1700,
                    1943
                ],
                "angle": 0,
                "content": "- Samantha Giancola (@MTVsammi) on 30 July: Loving swimming, Soccer track, and gymnastics the best"
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    466,
                    1943,
                    2001,
                    2006
                ],
                "angle": 0,
                "content": "- Piers Morgan on 31 July: Commenting on Michael Phelps being the \"official Greatest medal-winner in Olympic history,\" pointing to @MichaelPhelps"
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    466,
                    2009,
                    2001,
                    2075
                ],
                "angle": 0,
                "content": "- Bill Simmons (@sportsguy33) on 30 July:mentioned the Swedish-Norway women's handball match coming up in 30 minutes, noting plenty of seats available on the Swedish bandwagon"
            },
            {
                "block_id": 38,
                "type": "list",
                "bbox": [
                    466,
                    1811,
                    2042,
                    2075
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 39,
                "type": "title",
                "bbox": [
                    466,
                    2105,
                    800,
                    2138
                ],
                "angle": 0,
                "content": "6. **Kred Score Calculation:"
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    466,
                    2141,
                    2060,
                    2211
                ],
                "angle": 0,
                "content": "- The image explains that Kred scores are calculated based on influence, which is the ability to inspire action through social media, scored on a 1,000-point scale. Outreach is measured by generosity in engaging with others and helping spread messages through social media."
            },
            {
                "block_id": 41,
                "type": "text",
                "bbox": [
                    466,
                    2237,
                    1846,
                    2273
                ],
                "angle": 0,
                "content": "7. **Logo and Credits: The Kr logo is in the bottom left, and the infographic is credited to @Neez duoJindex at the bottom."
            },
            {
                "block_id": 42,
                "type": "list",
                "bbox": [
                    466,
                    2141,
                    2060,
                    2273
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 43,
                "type": "image_caption",
                "bbox": [
                    510,
                    2333,
                    2029,
                    2385
                ],
                "angle": 0,
                "content": "Figure 6: An illustrative example of CapRL-3B applied to infographic understanding."
            },
            {
                "block_id": 44,
                "type": "title",
                "bbox": [
                    438,
                    2481,
                    897,
                    2537
                ],
                "angle": 0,
                "content": "E QA PROCESSING"
            },
            {
                "block_id": 45,
                "type": "text",
                "bbox": [
                    428,
                    2610,
                    2103,
                    2844
                ],
                "angle": 0,
                "content": "In constructing the QA pairs, we employ the Qwen2.5-VL-72B model with prompts shown in Figure 18. For each image, we generate five questions and retain those without leakage issues. We do not deliberately control the number of QA pairs per image, prioritizing instead the overall dataset size and diversity. As revealed in later ablation studies 4 although even a single QA per image proves highly effective, adding more QA pairs still brings marginal improvements."
            },
            {
                "block_id": 46,
                "type": "text",
                "bbox": [
                    433,
                    2867,
                    2108,
                    3055
                ],
                "angle": 0,
                "content": "During QA filtering, since the model's answers carry uncertainty due to temperature parameter, we do not filter questions solely based on the correctness of a single response. Instead, we sample responses four times for each question, shuffling the answer options each time, and then measure the accuracy of the LVLM's answers both based on the image and based only on the question itself."
            },
            {
                "block_id": 47,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "15"
            }
        ]
    },
    {
        "page_id": 15,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    469,
                    356,
                    749,
                    396
                ],
                "angle": 0,
                "content": "Owen2.5-VL-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    464,
                    402,
                    1152,
                    650
                ],
                "angle": 0,
                "content": "This image is a graphical representation showcasing the most influential Team USA Olympians in 2012, as ranked by Twitter's \"Kred\" platform. The ranking is based on both influence and outreach metrics. The image predominantly features athletes from the USA Olympic team, including LeBron James, Hope Solo, Michael Phelps, and others."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    464,
                    683,
                    1101,
                    752
                ],
                "angle": 0,
                "content": "At the top of the image, there are three main athletes highlighted with star ratings and scores:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    469,
                    755,
                    1147,
                    821
                ],
                "angle": 0,
                "content": "1. **Fred the Teacher**: Has a score of 999 and is rated a \"Golden Medal.\""
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    471,
                    825,
                    1106,
                    891
                ],
                "angle": 0,
                "content": "2. **Mary Smith**: Has a score of 977 and is rated a \"Silent Medal.\""
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    469,
                    894,
                    1106,
                    963
                ],
                "angle": 0,
                "content": "3. **Christopher Brown**: Has a score of 977 and is rated a \"Subtle Medal.\""
            },
            {
                "block_id": 7,
                "type": "list",
                "bbox": [
                    469,
                    755,
                    1147,
                    963
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    466,
                    999,
                    912,
                    1036
                ],
                "angle": 0,
                "content": "Below these main athletes, there are:"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    469,
                    1039,
                    1093,
                    1105
                ],
                "angle": 0,
                "content": "4. **Frankie Lawrence**: Has a score of 977 and is rated a \"Yellow Medal,\" representing swimming."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    469,
                    1108,
                    1111,
                    1138
                ],
                "angle": 0,
                "content": "5. **Honey CEO**: Has a score of 992 and is rated a"
            },
            {
                "block_id": 11,
                "type": "list",
                "bbox": [
                    469,
                    1039,
                    1111,
                    1138
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    469,
                    1141,
                    938,
                    1174
                ],
                "angle": 0,
                "content": "\"Gold Medal,\" representing basketball."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    464,
                    1211,
                    1139,
                    1280
                ],
                "angle": 0,
                "content": "Additionally, there is a section titled \"Which events and athletes are winning celebrity attention?\" where:"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    469,
                    1283,
                    1132,
                    1353
                ],
                "angle": 0,
                "content": "- \\(^{**}\\) @ Fisheries**: Talks about the team's performance and the Olympics."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    469,
                    1356,
                    1088,
                    1422
                ],
                "angle": 0,
                "content": "- \\(* * @\\) - fitting Howard\\*\\*: An article by Howard, discussing the Swedish-Norwegian handball match."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    469,
                    1425,
                    1147,
                    1491
                ],
                "angle": 0,
                "content": "- **@ -Matching**: Discusses the Mexico-US Handball match."
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    464,
                    1211,
                    1147,
                    1491
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    464,
                    1527,
                    1157,
                    1673
                ],
                "angle": 0,
                "content": "There is also a section explaining \"How Kred scores are calculated,\" which details that influence and outreach are factors in the scoring system, defining it as \"the ability to inspire action through social media.\""
            },
            {
                "block_id": 19,
                "type": "image",
                "bbox": [
                    545,
                    1692,
                    627,
                    1768
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    629,
                    1719,
                    874,
                    1762
                ],
                "angle": 0,
                "content": "Hallucinations!"
            },
            {
                "block_id": 21,
                "type": "image",
                "bbox": [
                    1173,
                    353,
                    2075,
                    1917
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 7: An illustrative example of Qwen2.5-VL-3B applied to infographic understanding."
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    436,
                    2118,
                    2103,
                    2214
                ],
                "angle": 0,
                "content": "We ultimately apply a threshold to filter out questions with high image-based accuracy but low question-only accuracy."
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    436,
                    2234,
                    2103,
                    2376
                ],
                "angle": 0,
                "content": "It is worth noting that, because our filtering criteria are quite strict, some discarded QA pairs contain only mild or even negligible leakage. This also explains why, as shown in Figure 6 training with the leaked data does not cause training collapse but merely led to degraded performance."
            },
            {
                "block_id": 25,
                "type": "title",
                "bbox": [
                    438,
                    2455,
                    1027,
                    2504
                ],
                "angle": 0,
                "content": "F PRETRAINING DETAILS"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    436,
                    2567,
                    2108,
                    2890
                ],
                "angle": 0,
                "content": "Model Architecture. In our experimental setup, the language model component is initialized with a pretrained LLM, the visual encoder is initialized with a pretrained ViT, and the MLP projector is randomly initialized. This setup corresponds to a commonly adopted starting point in multimodal pretraining. To ensure the robustness of our conclusions, we evaluate three groups of architectures: (1) Qwen2.5-3B + Qwen2.5-ViT, (2) Qwen2.5-7B + Qwen2.5-ViT, and (3) InternLM2.5-7B + CLIP-ViT-L. This selection jointly considers differences in parameter scale, LLM backbone, and visual encoder type."
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    436,
                    2910,
                    2106,
                    3055
                ],
                "angle": 0,
                "content": "Training Setting. Following the training paradigm of ShareGPT4V, our training process consists of three stages: Initial Alignment - Further Pretraining - SFT. (1) In the Initial Alignment stage, we unfreeze the MLP and perform preliminary alignment using the BLIP-558K dataset. We adopt a"
            },
            {
                "block_id": 28,
                "type": "page_number",
                "bbox": [
                    1249,
                    3131,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "16"
            }
        ]
    },
    {
        "page_id": 16,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    487,
                    372,
                    668,
                    412
                ],
                "angle": 0,
                "content": "CapRL-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    484,
                    415,
                    1188,
                    613
                ],
                "angle": 0,
                "content": "The image is a bar chart that presents survey results regarding opinions on allowing more, about the same, or fewer Syrian refugees into various countries, specifically Jordan, Lebanon, and Turkey. The survey was conducted by the Pew Research Center in Spring 2017 as part of the Global Attitudes Survey, focusing on question Q61. Here is a detailed description:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    484,
                    646,
                    1185,
                    716
                ],
                "angle": 0,
                "content": "**Title: Jordan, Lebanon, Turkey want fewer Syrian refugees allowed into their country**"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    484,
                    745,
                    1203,
                    848
                ],
                "angle": 0,
                "content": "**Question:**** In your opinion, should we allow more refugees from Syria into our country, fewer refugees or about the same as we do now?"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    484,
                    881,
                    1190,
                    950
                ],
                "angle": 0,
                "content": "The chart shows percentages for each country and religious or ethnic group regarding their preferred number of refugees:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    489,
                    983,
                    660,
                    1013
                ],
                "angle": 0,
                "content": "1. **Jordan:**"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    510,
                    1016,
                    721,
                    1046
                ],
                "angle": 0,
                "content": "- \\(8\\%\\) want \"More\""
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    510,
                    1046,
                    833,
                    1075
                ],
                "angle": 0,
                "content": "- \\(8\\%\\) want \"About the same\""
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    512,
                    1082,
                    744,
                    1112
                ],
                "angle": 0,
                "content": "- \\(60\\%\\) want \"Fewer\""
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    512,
                    1115,
                    816,
                    1148
                ],
                "angle": 0,
                "content": "- \\(23\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 11,
                "type": "list",
                "bbox": [
                    510,
                    1016,
                    833,
                    1148
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    489,
                    1181,
                    657,
                    1214
                ],
                "angle": 0,
                "content": "2. **Syrian:**"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    510,
                    1217,
                    734,
                    1247
                ],
                "angle": 0,
                "content": "- \\(64\\%\\) want \"More\""
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    510,
                    1250,
                    851,
                    1280
                ],
                "angle": 0,
                "content": "- \\(11\\%\\) want \"About the same\""
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    510,
                    1283,
                    744,
                    1313
                ],
                "angle": 0,
                "content": "- \\(26\\%\\) want \"Fewer\""
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    510,
                    1217,
                    851,
                    1313
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    520,
                    1316,
                    1106,
                    1353
                ],
                "angle": 0,
                "content": "(This category includes Syrian respondents overall)"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    489,
                    1382,
                    693,
                    1412
                ],
                "angle": 0,
                "content": "3. **Jordanian:**"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    510,
                    1415,
                    721,
                    1445
                ],
                "angle": 0,
                "content": "- \\(3\\%\\) want \"More\""
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    510,
                    1448,
                    833,
                    1478
                ],
                "angle": 0,
                "content": "- \\(6\\%\\) want \"About the same\""
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    510,
                    1481,
                    744,
                    1511
                ],
                "angle": 0,
                "content": "- \\(63\\%\\) want \"Fewer\""
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    510,
                    1518,
                    816,
                    1547
                ],
                "angle": 0,
                "content": "- \\(27\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 23,
                "type": "list",
                "bbox": [
                    510,
                    1415,
                    833,
                    1547
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    489,
                    1584,
                    683,
                    1613
                ],
                "angle": 0,
                "content": "4. **Lebanon:**"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    510,
                    1617,
                    721,
                    1646
                ],
                "angle": 0,
                "content": "- \\(4\\%\\) want \"More\""
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    510,
                    1650,
                    849,
                    1679
                ],
                "angle": 0,
                "content": "- \\(13\\%\\) want \"About the same\""
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    510,
                    1683,
                    744,
                    1712
                ],
                "angle": 0,
                "content": "- \\(40\\%\\) want \"Fewer\""
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    510,
                    1716,
                    816,
                    1749
                ],
                "angle": 0,
                "content": "- \\(42\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 29,
                "type": "list",
                "bbox": [
                    510,
                    1617,
                    849,
                    1749
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 30,
                "type": "text",
                "bbox": [
                    489,
                    1782,
                    637,
                    1815
                ],
                "angle": 0,
                "content": "5. **Shia:**"
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    510,
                    1818,
                    719,
                    1844
                ],
                "angle": 0,
                "content": "- \\(4\\%\\) want \"More\""
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    510,
                    1851,
                    849,
                    1880
                ],
                "angle": 0,
                "content": "- \\(17\\%\\) want \"About the same\""
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    510,
                    1884,
                    744,
                    1913
                ],
                "angle": 0,
                "content": "- \\(44\\%\\) want \"Fewer\""
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    510,
                    1917,
                    816,
                    1950
                ],
                "angle": 0,
                "content": "- \\(33\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 35,
                "type": "list",
                "bbox": [
                    510,
                    1818,
                    849,
                    1950
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    489,
                    1983,
                    650,
                    2013
                ],
                "angle": 0,
                "content": "6. \\*\\*Sunni:\\*\\*"
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    510,
                    2016,
                    721,
                    2049
                ],
                "angle": 0,
                "content": "- \\(4\\%\\) want \"More\""
            },
            {
                "block_id": 38,
                "type": "text",
                "bbox": [
                    510,
                    2052,
                    849,
                    2082
                ],
                "angle": 0,
                "content": "- \\(12\\%\\) want \"About the same\""
            },
            {
                "block_id": 39,
                "type": "text",
                "bbox": [
                    510,
                    2085,
                    744,
                    2115
                ],
                "angle": 0,
                "content": "- \\(46\\%\\) want \"Fewer\""
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    510,
                    2118,
                    816,
                    2151
                ],
                "angle": 0,
                "content": "- \\(36\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 41,
                "type": "list",
                "bbox": [
                    510,
                    2016,
                    849,
                    2151
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 42,
                "type": "text",
                "bbox": [
                    489,
                    2181,
                    688,
                    2214
                ],
                "angle": 0,
                "content": "7. **Christian:**"
            },
            {
                "block_id": 43,
                "type": "text",
                "bbox": [
                    510,
                    2217,
                    719,
                    2247
                ],
                "angle": 0,
                "content": "- \\(3\\%\\) want \"More\""
            },
            {
                "block_id": 44,
                "type": "text",
                "bbox": [
                    510,
                    2247,
                    849,
                    2280
                ],
                "angle": 0,
                "content": "- \\(10\\%\\) want \"About the same\""
            },
            {
                "block_id": 45,
                "type": "text",
                "bbox": [
                    510,
                    2283,
                    744,
                    2313
                ],
                "angle": 0,
                "content": "- \\(33\\%\\) want \"Fewer\""
            },
            {
                "block_id": 46,
                "type": "text",
                "bbox": [
                    510,
                    2316,
                    816,
                    2349
                ],
                "angle": 0,
                "content": "- \\(53\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 47,
                "type": "list",
                "bbox": [
                    510,
                    2217,
                    849,
                    2349
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 48,
                "type": "text",
                "bbox": [
                    489,
                    2382,
                    665,
                    2415
                ],
                "angle": 0,
                "content": "8. \\*\\*Turkey:\\*\\*"
            },
            {
                "block_id": 49,
                "type": "text",
                "bbox": [
                    510,
                    2418,
                    721,
                    2448
                ],
                "angle": 0,
                "content": "- \\(4\\%\\) want \"More\""
            },
            {
                "block_id": 50,
                "type": "text",
                "bbox": [
                    510,
                    2451,
                    833,
                    2481
                ],
                "angle": 0,
                "content": "- \\(9\\%\\) want \"About the same\""
            },
            {
                "block_id": 51,
                "type": "text",
                "bbox": [
                    510,
                    2484,
                    744,
                    2514
                ],
                "angle": 0,
                "content": "- \\(30\\%\\) want \"Fewer\""
            },
            {
                "block_id": 52,
                "type": "text",
                "bbox": [
                    510,
                    2517,
                    816,
                    2550
                ],
                "angle": 0,
                "content": "- \\(53\\%\\) want \"None (VCL)\""
            },
            {
                "block_id": 53,
                "type": "list",
                "bbox": [
                    510,
                    2418,
                    833,
                    2550
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 54,
                "type": "text",
                "bbox": [
                    489,
                    2583,
                    650,
                    2613
                ],
                "angle": 0,
                "content": "**Sources:**"
            },
            {
                "block_id": 55,
                "type": "text",
                "bbox": [
                    489,
                    2616,
                    925,
                    2649
                ],
                "angle": 0,
                "content": "- Spring 2017 Global Attitudes Survey"
            },
            {
                "block_id": 56,
                "type": "text",
                "bbox": [
                    489,
                    2653,
                    665,
                    2686
                ],
                "angle": 0,
                "content": "-QuestionQ61"
            },
            {
                "block_id": 57,
                "type": "title",
                "bbox": [
                    1229,
                    396,
                    1948,
                    471
                ],
                "angle": 0,
                "content": "Jordan, Lebanon, Turkey want fewer Syrian refugees allowed into their country"
            },
            {
                "block_id": 58,
                "type": "text",
                "bbox": [
                    1229,
                    478,
                    1981,
                    541
                ],
                "angle": 0,
                "content": "In your opinion, should we allow more refugees from Syria into our country, fewer refugees or about the same as we do now?"
            },
            {
                "block_id": 60,
                "type": "image",
                "bbox": [
                    1234,
                    597,
                    1981,
                    1069
                ],
                "angle": 0,
                "content": null,
                "caption": "More About the same Fewer None (VOL)"
            },
            {
                "block_id": 61,
                "type": "text",
                "bbox": [
                    1231,
                    1085,
                    1652,
                    1115
                ],
                "angle": 0,
                "content": "Source: Spring 2017 Global Attitudes Survey. Q61."
            },
            {
                "block_id": 62,
                "type": "text",
                "bbox": [
                    1231,
                    1122,
                    1450,
                    1148
                ],
                "angle": 0,
                "content": "PEW RESEARCH CENTER"
            },
            {
                "block_id": 63,
                "type": "title",
                "bbox": [
                    1269,
                    1237,
                    1532,
                    1277
                ],
                "angle": 0,
                "content": "Qwen2.5-VL-3B:"
            },
            {
                "block_id": 64,
                "type": "text",
                "bbox": [
                    1252,
                    1300,
                    1275,
                    1316
                ],
                "angle": 0,
                "content": "··"
            },
            {
                "block_id": 65,
                "type": "text",
                "bbox": [
                    1252,
                    1316,
                    1777,
                    1353
                ],
                "angle": 0,
                "content": "Here's a summary of the data for each country:"
            },
            {
                "block_id": 66,
                "type": "text",
                "bbox": [
                    1254,
                    1382,
                    1422,
                    1415
                ],
                "angle": 0,
                "content": "1. **Jordan**:"
            },
            {
                "block_id": 67,
                "type": "text",
                "bbox": [
                    1272,
                    1419,
                    1407,
                    1448
                ],
                "angle": 0,
                "content": "- More: \\(8\\%\\)"
            },
            {
                "block_id": 68,
                "type": "text",
                "bbox": [
                    1272,
                    1452,
                    1532,
                    1481
                ],
                "angle": 0,
                "content": "- About the same: \\(23\\%\\)"
            },
            {
                "block_id": 69,
                "type": "text",
                "bbox": [
                    1272,
                    1485,
                    1433,
                    1514
                ],
                "angle": 0,
                "content": "- Fewer: \\(68\\%\\)"
            },
            {
                "block_id": 70,
                "type": "list",
                "bbox": [
                    1272,
                    1419,
                    1532,
                    1514
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 71,
                "type": "text",
                "bbox": [
                    1252,
                    1544,
                    1861,
                    1580
                ],
                "angle": 0,
                "content": "2. \\*\\*Saudi Arabia\\*\\* (assumed to be the \"Syrian\" bar):"
            },
            {
                "block_id": 72,
                "type": "text",
                "bbox": [
                    1272,
                    1584,
                    1422,
                    1613
                ],
                "angle": 0,
                "content": "- More: \\(64\\%\\)"
            },
            {
                "block_id": 73,
                "type": "text",
                "bbox": [
                    1272,
                    1617,
                    1532,
                    1646
                ],
                "angle": 0,
                "content": "- About the same: \\(11\\%\\)"
            },
            {
                "block_id": 74,
                "type": "text",
                "bbox": [
                    1272,
                    1650,
                    1433,
                    1679
                ],
                "angle": 0,
                "content": "- Fewer: \\(25\\%\\)"
            },
            {
                "block_id": 75,
                "type": "list",
                "bbox": [
                    1272,
                    1584,
                    1532,
                    1679
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 76,
                "type": "text",
                "bbox": [
                    1252,
                    1712,
                    1445,
                    1745
                ],
                "angle": 0,
                "content": "3. \\*\\*Lebanon\\*\\*:"
            },
            {
                "block_id": 77,
                "type": "text",
                "bbox": [
                    1272,
                    1749,
                    1422,
                    1778
                ],
                "angle": 0,
                "content": "- More: \\(40\\%\\)"
            },
            {
                "block_id": 78,
                "type": "text",
                "bbox": [
                    1272,
                    1782,
                    1532,
                    1811
                ],
                "angle": 0,
                "content": "- About the same: \\(35\\%\\)"
            },
            {
                "block_id": 79,
                "type": "text",
                "bbox": [
                    1272,
                    1815,
                    1433,
                    1844
                ],
                "angle": 0,
                "content": "- Fewer: \\(25\\%\\)"
            },
            {
                "block_id": 80,
                "type": "list",
                "bbox": [
                    1272,
                    1749,
                    1532,
                    1844
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 81,
                "type": "text",
                "bbox": [
                    1252,
                    1880,
                    1716,
                    1913
                ],
                "angle": 0,
                "content": "4. \\*\\*Hijazi Shia\\*\\* (assumed as separate):"
            },
            {
                "block_id": 82,
                "type": "text",
                "bbox": [
                    1272,
                    1917,
                    1422,
                    1943
                ],
                "angle": 0,
                "content": "- More: \\(46\\%\\)"
            },
            {
                "block_id": 83,
                "type": "text",
                "bbox": [
                    1272,
                    1947,
                    1532,
                    1980
                ],
                "angle": 0,
                "content": "- About the same: \\(40\\%\\)"
            },
            {
                "block_id": 84,
                "type": "text",
                "bbox": [
                    1272,
                    1983,
                    1433,
                    2009
                ],
                "angle": 0,
                "content": "- Fewer: \\(14\\%\\)"
            },
            {
                "block_id": 85,
                "type": "list",
                "bbox": [
                    1272,
                    1917,
                    1532,
                    2009
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 86,
                "type": "text",
                "bbox": [
                    1252,
                    2046,
                    1762,
                    2079
                ],
                "angle": 0,
                "content": "5. **American Shia** (assumed as separate):"
            },
            {
                "block_id": 87,
                "type": "text",
                "bbox": [
                    1272,
                    2082,
                    1422,
                    2112
                ],
                "angle": 0,
                "content": "- More: \\(31\\%\\)"
            },
            {
                "block_id": 88,
                "type": "text",
                "bbox": [
                    1272,
                    2115,
                    1532,
                    2141
                ],
                "angle": 0,
                "content": "- About the same: \\(38\\%\\)"
            },
            {
                "block_id": 89,
                "type": "text",
                "bbox": [
                    1272,
                    2148,
                    1433,
                    2174
                ],
                "angle": 0,
                "content": "- Fewer: \\(31\\%\\)"
            },
            {
                "block_id": 90,
                "type": "list",
                "bbox": [
                    1272,
                    2082,
                    1532,
                    2174
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 91,
                "type": "text",
                "bbox": [
                    1252,
                    2211,
                    1519,
                    2244
                ],
                "angle": 0,
                "content": "6. **Sunni Republic**:"
            },
            {
                "block_id": 92,
                "type": "text",
                "bbox": [
                    1272,
                    2247,
                    1422,
                    2273
                ],
                "angle": 0,
                "content": "- More: \\(20\\%\\)"
            },
            {
                "block_id": 93,
                "type": "text",
                "bbox": [
                    1272,
                    2280,
                    1532,
                    2310
                ],
                "angle": 0,
                "content": "- About the same: \\(44\\%\\)"
            },
            {
                "block_id": 94,
                "type": "text",
                "bbox": [
                    1272,
                    2313,
                    1433,
                    2343
                ],
                "angle": 0,
                "content": "- Fewer: \\(36\\%\\)"
            },
            {
                "block_id": 95,
                "type": "list",
                "bbox": [
                    1272,
                    2247,
                    1532,
                    2343
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 96,
                "type": "text",
                "bbox": [
                    1252,
                    2376,
                    1496,
                    2409
                ],
                "angle": 0,
                "content": "7. **Christian Fiji**:"
            },
            {
                "block_id": 97,
                "type": "text",
                "bbox": [
                    1272,
                    2412,
                    1422,
                    2442
                ],
                "angle": 0,
                "content": "- More: \\(33\\%\\)"
            },
            {
                "block_id": 98,
                "type": "text",
                "bbox": [
                    1272,
                    2445,
                    1532,
                    2471
                ],
                "angle": 0,
                "content": "- About the same: \\(38\\%\\)"
            },
            {
                "block_id": 99,
                "type": "text",
                "bbox": [
                    1272,
                    2478,
                    1433,
                    2504
                ],
                "angle": 0,
                "content": "- Fewer: \\(31\\%\\)"
            },
            {
                "block_id": 100,
                "type": "list",
                "bbox": [
                    1272,
                    2412,
                    1532,
                    2504
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 101,
                "type": "text",
                "bbox": [
                    1252,
                    2541,
                    1430,
                    2574
                ],
                "angle": 0,
                "content": "8. \\*\\*Turkey\\*\\*:"
            },
            {
                "block_id": 102,
                "type": "text",
                "bbox": [
                    1272,
                    2577,
                    1422,
                    2607
                ],
                "angle": 0,
                "content": "- More: \\(30\\%\\)"
            },
            {
                "block_id": 103,
                "type": "text",
                "bbox": [
                    1272,
                    2610,
                    1532,
                    2640
                ],
                "angle": 0,
                "content": "- About the same: \\(31\\%\\)"
            },
            {
                "block_id": 104,
                "type": "text",
                "bbox": [
                    1272,
                    2643,
                    1433,
                    2673
                ],
                "angle": 0,
                "content": "- Fewer: \\( {39}\\% \\)"
            },
            {
                "block_id": 105,
                "type": "list",
                "bbox": [
                    1272,
                    2577,
                    1532,
                    2673
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 106,
                "type": "image_caption",
                "bbox": [
                    510,
                    2742,
                    2027,
                    2795
                ],
                "angle": 0,
                "content": "Figure 8: Chart understanding comparison between CapRL-3B and Qwen2.5-VL-3B."
            },
            {
                "block_id": 107,
                "type": "text",
                "bbox": [
                    436,
                    2864,
                    2103,
                    3052
                ],
                "angle": 0,
                "content": "learning rate of 1e-3 and a batch size of 256. (2) In the Further Pretraining stage, we unfreeze all parameters including the LLM, MLP, and ViT. This stage facilitates further alignment with various high-quality image-caption datasets, enabling the LLM to better understand visual features. We set the learning rate to 4e-5 and the batch size to 256. (3) In the SFT stage, we again unfreeze all"
            },
            {
                "block_id": 108,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "17"
            },
            {
                "block_id": 109,
                "type": "image",
                "bbox": [
                    1652,
                    1633,
                    1996,
                    1702
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 17,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    484,
                    359,
                    668,
                    396
                ],
                "angle": 0,
                "content": "CapRL-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    481,
                    402,
                    1165,
                    702
                ],
                "angle": 0,
                "content": "The image is a bar chart from the Pew Research Center that illustrates how different groups of Republicans and Republican-leaning individuals view President Trump, specifically focusing on how they see him as \"fighting for what I believe in.\" The chart is broken down by several categories, including age groups, educational levels, political ideology, and whether respondents are full Republicans or leaners towards the Republican Party. Here is a detailed description:"
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    484,
                    732,
                    601,
                    765
                ],
                "angle": 0,
                "content": "Title:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    484,
                    768,
                    1160,
                    904
                ],
                "angle": 0,
                "content": "\"Older Republicans especially likely to see Trump as fighting for their beliefs\\nAmong Republicans and Republican leaners, % who say the phrase 'fights for what I believe in' describes Trump...\""
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    484,
                    933,
                    701,
                    970
                ],
                "angle": 0,
                "content": "Key Findings:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    487,
                    999,
                    1027,
                    1036
                ],
                "angle": 0,
                "content": "1. **Overall Percentage (All Rep/Lean Rep):**"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    507,
                    1036,
                    711,
                    1065
                ],
                "angle": 0,
                "content": "- Very well: \\(51\\%\\)"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    507,
                    1069,
                    721,
                    1102
                ],
                "angle": 0,
                "content": "- Fairly well: \\(36\\%\\)"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    507,
                    1102,
                    867,
                    1138
                ],
                "angle": 0,
                "content": "- NET (Total percentage): \\(87\\%\\)"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    487,
                    1168,
                    721,
                    1201
                ],
                "angle": 0,
                "content": "2. **Age Groups:**"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    507,
                    1204,
                    1150,
                    1234
                ],
                "angle": 0,
                "content": "- Ages 18-29: \\(31\\%\\) very well, \\(45\\%\\) fairly well, total \\(76\\%\\)"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    510,
                    1237,
                    1086,
                    1267
                ],
                "angle": 0,
                "content": "- 30-49: \\(41\\%\\) very well, \\(42\\%\\) fairly well, total \\(82\\%\\)"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    510,
                    1270,
                    1083,
                    1300
                ],
                "angle": 0,
                "content": "- 50-64: \\(58\\%\\) very well, \\(33\\%\\) fairly well, total \\(92\\%\\)"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    510,
                    1303,
                    1068,
                    1336
                ],
                "angle": 0,
                "content": "- \\(65+\\): \\(68\\%\\) very well, \\(26\\%\\) fairly well, total \\(94\\%\\)"
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    487,
                    999,
                    1150,
                    1336
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    484,
                    1369,
                    1167,
                    1471
                ],
                "angle": 0,
                "content": "Older age groups (65 and above) are the most likely to see Trump as actively fighting for their beliefs, with the \\(65+\\) age group having the highest net percentage at \\(94\\%\\)."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    487,
                    1501,
                    782,
                    1534
                ],
                "angle": 0,
                "content": "3. **Education Levels:**"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    507,
                    1537,
                    1122,
                    1567
                ],
                "angle": 0,
                "content": "- Postgrad: \\(42\\%\\) very well, \\(38\\%\\) fairly well, total \\(80\\%\\)"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    507,
                    1570,
                    1165,
                    1603
                ],
                "angle": 0,
                "content": "- College grad: \\(45\\%\\) very well, \\(40\\%\\) fairly well, total \\(85\\%\\)"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    507,
                    1603,
                    1111,
                    1666
                ],
                "angle": 0,
                "content": "- Some college: \\(51\\%\\) very well, \\(36\\%\\) fairly well, total \\(7\\%\\)"
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    507,
                    1537,
                    1165,
                    1666
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    507,
                    1669,
                    1111,
                    1702
                ],
                "angle": 0,
                "content": "- HS or less: \\(56\\%\\) very well, \\(33\\%\\) fairly well, total 89"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    484,
                    1712,
                    2006,
                    1782
                ],
                "angle": 0,
                "content": "Individuals with a high school diploma or less education are most likely to see Trump fighting for their beliefs (89%), while those with postgrad degrees are least likely (80%)."
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    487,
                    1811,
                    836,
                    1844
                ],
                "angle": 0,
                "content": "4. **Political Conservatism:**"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    507,
                    1848,
                    1277,
                    1880
                ],
                "angle": 0,
                "content": "- Conserv (Conservative): \\(63\\%\\) very well, \\(31\\%\\) fairly well, total \\(94\\%\\)"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    507,
                    1884,
                    1333,
                    1913
                ],
                "angle": 0,
                "content": "- Mod/Lib (Moderate or Liberal): \\(32\\%\\) very well, \\(44\\%\\) fairly well, total 75"
            },
            {
                "block_id": 27,
                "type": "list",
                "bbox": [
                    507,
                    1848,
                    1333,
                    1913
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    502,
                    1943,
                    1591,
                    1980
                ],
                "angle": 0,
                "content": "Conservatives are the most likely to see Trump fighting for beliefs, with a net percentage of \\(94\\%\\)."
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    487,
                    2013,
                    721,
                    2046
                ],
                "angle": 0,
                "content": "5. **Partisanship:**"
            },
            {
                "block_id": 30,
                "type": "text",
                "bbox": [
                    507,
                    2049,
                    1137,
                    2079
                ],
                "angle": 0,
                "content": "- Republican: \\(61\\%\\) very well, \\(32\\%\\) fairly well, total \\(93\\%\\)"
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    507,
                    2082,
                    1175,
                    2112
                ],
                "angle": 0,
                "content": "- Lean Republican: \\(36\\%\\) very well, \\(41\\%\\) fairly well, total 77"
            },
            {
                "block_id": 32,
                "type": "list",
                "bbox": [
                    507,
                    2049,
                    1175,
                    2112
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    502,
                    2145,
                    1688,
                    2181
                ],
                "angle": 0,
                "content": "Leaners towards the Republican Party are less likely to see Trump this way, with a net percentage of \\(77\\%\\)."
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    487,
                    2211,
                    619,
                    2240
                ],
                "angle": 0,
                "content": "6. **Notes:"
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    507,
                    2244,
                    1739,
                    2277
                ],
                "angle": 0,
                "content": "- The survey was conducted February 4-15, 2020, based on Republicans and Republican-leaning independents."
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    507,
                    2280,
                    1091,
                    2310
                ],
                "angle": 0,
                "content": "- The Pew Research Center is the source of the data."
            },
            {
                "block_id": 37,
                "type": "list",
                "bbox": [
                    507,
                    2244,
                    1739,
                    2310
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 38,
                "type": "text",
                "bbox": [
                    484,
                    2343,
                    974,
                    2376
                ],
                "angle": 0,
                "content": "##### Percentages Breakdown by Categories:"
            },
            {
                "block_id": 39,
                "type": "text",
                "bbox": [
                    484,
                    2379,
                    1407,
                    2412
                ],
                "angle": 0,
                "content": "- The highest net percentage (94%) is seen in the \\(65+\\) age group and Conservatives."
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    484,
                    2412,
                    1173,
                    2442
                ],
                "angle": 0,
                "content": "- The lowest net percentage is in the 18-29 age group at \\(76\\%\\)."
            },
            {
                "block_id": 41,
                "type": "text",
                "bbox": [
                    484,
                    2445,
                    1415,
                    2478
                ],
                "angle": 0,
                "content": "- Full Republicans \\((93\\%)\\) and those leaning Republican \\((77\\%)\\) are moderately high."
            },
            {
                "block_id": 42,
                "type": "list",
                "bbox": [
                    484,
                    2379,
                    1415,
                    2478
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 43,
                "type": "image_caption",
                "bbox": [
                    785,
                    2517,
                    1754,
                    2570
                ],
                "angle": 0,
                "content": "Figure 9: Example of CapRL-3B chart understanding."
            },
            {
                "block_id": 44,
                "type": "text",
                "bbox": [
                    436,
                    2702,
                    2103,
                    2795
                ],
                "angle": 0,
                "content": "parameters and train on the OpenLLaVA-Next dataset. We set the learning rate to 2e-5 and the batch size to 128."
            },
            {
                "block_id": 45,
                "type": "text",
                "bbox": [
                    436,
                    2818,
                    2108,
                    3055
                ],
                "angle": 0,
                "content": "Baselines. We selected several strong baselines for comparison. (1) Vanilla, which skips the Further Pretraining stage and only goes through the first and third stages. Additionally, we constructed two more baselines by varying the dataset used in the Further Pretraining stage: (2) ShareGPT4V-1M, and (3) DenseFusion-1M. To ensure a fair comparison by controlling the number of samples, we randomly sampled 1 million image-caption pairs from the 5M dataset to form CapRL-1M."
            },
            {
                "block_id": 46,
                "type": "title",
                "bbox": [
                    1249,
                    382,
                    1966,
                    481
                ],
                "angle": 0,
                "content": "Older Republicans especially likely to see Trump as fighting for their beliefs"
            },
            {
                "block_id": 47,
                "type": "text",
                "bbox": [
                    1246,
                    495,
                    2019,
                    613
                ],
                "angle": 0,
                "content": "Among Republicans and Republican leaners, % who say the phrase 'fights for what I believe in' describes Trump ..."
            },
            {
                "block_id": 48,
                "type": "image",
                "bbox": [
                    1249,
                    640,
                    1986,
                    1428
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 49,
                "type": "text",
                "bbox": [
                    1246,
                    1455,
                    2014,
                    1527
                ],
                "angle": 0,
                "content": "Note: Based on Republicans and Republican-leaning independents. Source: Survey of U.S. adults conducted Feb. 4-15, 2020."
            },
            {
                "block_id": 50,
                "type": "title",
                "bbox": [
                    1249,
                    1537,
                    1545,
                    1574
                ],
                "angle": 0,
                "content": "PEW RESEARCH CENTER"
            },
            {
                "block_id": 51,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "18"
            }
        ]
    },
    {
        "page_id": 18,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    484,
                    399,
                    963,
                    438
                ],
                "angle": 0,
                "content": "UnifiedRewardModel-as-Judge-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    479,
                    511,
                    1346,
                    1145
                ],
                "angle": 0,
                "content": "This image appears to be a bar chart from a study conducted by the Pew Research Center. The title \"Older Republicans especially likely to see Trump as fighting for their beliefs\" is prominently displayed at the top. The chart compares percentages among Republicans and Republican leaners on how well the phrase 'fights for what I believe in' describes Donald Trump, across different categories. Key data points are listed with \"Very well,\" \"Fairly well,\" and \"NET\" columns for each category. Specific numbers are provided for age groups (e.g., All Rep/Lean Rep: 51 for 'Very well', 36 for 'Fairly well'), education levels (postgrad, college grad, etc.), political orientation (conservative, moderate/liberal), and Republican affiliation (Republican, Lean Republican). There are numerical values and percentages (e.g., 87, 92, 85) indicating net results for each category. The source and survey details (U.S. adults, Feb. 4-15, 2020) are noted at the bottom. The description captures the structured, comparative data format and the specifics of the study's findings in detail."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    731,
                    1181,
                    1262,
                    1273
                ],
                "angle": 0,
                "content": "Image-related key information is incomplete and inaccurate."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    1412,
                    379,
                    1986,
                    462
                ],
                "angle": 0,
                "content": "Older Republicans especially likely to see Trump as fighting for their beliefs"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1412,
                    468,
                    2019,
                    564
                ],
                "angle": 0,
                "content": "Among Republicans and Republican leaners, % who say the phrase 'fights for what I believe in' describes Trump ..."
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    1417,
                    587,
                    1994,
                    1207
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1412,
                    1224,
                    2014,
                    1280
                ],
                "angle": 0,
                "content": "Note: Based on Republicans and Republican-leaning independents. Source: Survey of U.S. adults conducted Feb. 4-15, 2020."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1415,
                    1287,
                    1647,
                    1316
                ],
                "angle": 0,
                "content": "PEW RESEARCH CENTER"
            },
            {
                "block_id": 9,
                "type": "image_caption",
                "bbox": [
                    650,
                    1405,
                    1892,
                    1458
                ],
                "angle": 0,
                "content": "Figure 10: Example of Qwen2.5VL-as-Judge-3B chart understanding."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    438,
                    1541,
                    1986,
                    1650
                ],
                "angle": 0,
                "content": "G CURRENT LANDSCAPE AND FUTURE DIRECTIONS OF MULTIMODAL MODELS"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    433,
                    1719,
                    2111,
                    2323
                ],
                "angle": 0,
                "content": "Multimodal learning has accelerated rapidly in the past two years, driven by systems that couple language with perception and action at scale (Liu et al. 2023a 2025c; Bai et al. 2025; Dong et al. 2024; Liu et al. 2025d). These advances have produced broad empirical breakthroughs across benchmarks and real-world tasks (Zhang et al. 2024b; Liu et al. 2024a; Sun et al. 2025; Shu et al. 2024; Xing et al. 2024; Qi et al. 2024). Compared to earlier eras that focused mainly on images paired with text (Zhang et al. 2024a; Liu et al. 2024b; Yao et al. 2022), today's models are larger, trained on more diverse corpora, and natively support additional modalities such as video and audio. In video understanding and generation, in particular, we have seen a surge of meaningful progress spanning long-horizon temporal modeling, spatial-temporal grounding, and instruction following (Chen et al. 2024c; Wei et al. 2025; Yao et al. 2025b; Li et al. 2024a). Complementary work expands audio modeling, bridging speech, music, and cross-modal alignment (Liu et al. 2025a). The community has also introduced interactive video agents that reason over time and act, further stressing the importance of temporal memory and tool use (Ren et al. 2024; Yao et al. 2025a)."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    433,
                    2336,
                    2103,
                    2805
                ],
                "angle": 0,
                "content": "On the reasoning front, multimodal LLMs increasingly exhibit structured problem solving, calibrated self-reflection, and tool-augmented inference (Zhang et al. 2023 2025). Architecturally, research explores how to best couple visual encoders and language backbones—ranging from tightly integrated fusion layers to decoupled, composable adapters—while preserving scalability and transfer (Li et al. 2023, Yao et al. 2024). Historically, image generation and understanding evolved as largely separate tracks; however, the field is now converging toward unified, bidirectional models that both parse and synthesize content under a shared representation space (Ling et al. 2024, Zhou et al. 2025, Bu et al. 2025). In parallel, diffusion-based approaches continue to look especially promising for controllable, high-fidelity synthesis and cross-modal conditioning, offering a principled path to training generative-discriminative hybrids (Nie et al. 2025, Li et al. 2025)."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    433,
                    2818,
                    2103,
                    3055
                ],
                "angle": 0,
                "content": "Looking forward, we anticipate four themes to shape the landscape: (1) long-context multimodality, where models maintain persistent memory across hours-long video and streaming audio; (2) agentic behavior, combining perception with planning and tool execution in open environments; (3) unified pretraining objectives that align understanding and generation, reducing modality gaps; and (4) efficient adaptation—via lightweight finetuning and retrieval—to safely deploy systems across"
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "19"
            }
        ]
    },
    {
        "page_id": 19,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    589,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    436,
                    343,
                    2111,
                    442
                ],
                "angle": 0,
                "content": "domains and devices. Together, these directions suggest a move from static perception to interactive, end-to-end multimodal intelligence."
            },
            {
                "block_id": 2,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1300,
                    3168
                ],
                "angle": 0,
                "content": "20"
            }
        ]
    },
    {
        "page_id": 20,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    487,
                    386,
                    884,
                    419
                ],
                "angle": 0,
                "content": "Owen2.5VL-as-Judge-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    484,
                    425,
                    1287,
                    594
                ],
                "angle": 0,
                "content": "This image is a detailed bar graph from a survey conducted by the Pew Research Center, which examines how Republicans and Republican-leaning individuals view Donald Trump's political leadership using a specific phrase, \"fights for what I believe in.\" Here's a comprehensive description and analysis:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    487,
                    623,
                    1277,
                    729
                ],
                "angle": 0,
                "content": "1. **Title and Subject**: The study asks how Republicans and Republican-leaning individuals view Donald Trump, focusing on how they see him \"fighting\" for their beliefs."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    484,
                    759,
                    1254,
                    861
                ],
                "angle": 0,
                "content": "2. **Survey Data**: The data is divided into several categories and metric variables, including political ideologies, and part of Image-related key information"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    487,
                    894,
                    785,
                    924
                ],
                "angle": 0,
                "content": "3. **Key Observations**:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    484,
                    927,
                    1267,
                    996
                ],
                "angle": 0,
                "content": "- Among All Rep/Lean Reps, \\(87\\%\\) see Trump as very well or fairly well described as someone who \"fights for what I believe in.\""
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    484,
                    996,
                    1262,
                    1095
                ],
                "angle": 0,
                "content": "- Younger age groups (18-29 and 30-49) have lower percentages (76% for ages 18-29 and 82% for ages 30-49) compared to the older groups (51% for ages 50-64 and 68% for ages 65+)."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    484,
                    1095,
                    1249,
                    1161
                ],
                "angle": 0,
                "content": "- Education level: Postgrads see Trump this way \\(80\\%\\) of the time, while those with some college education see him \\(87\\%\\) of the time."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    484,
                    1164,
                    1249,
                    1227
                ],
                "angle": 0,
                "content": "- Political ideology: Conservative individuals see Trump this way \\(94\\%\\) of the time, with moderate/liberal \\((32 - 44\\%\\) of the time)."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1430,
                    386,
                    1915,
                    455
                ],
                "angle": 0,
                "content": "Older Republicans especially likely to see Trump as fighting for their beliefs"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1430,
                    462,
                    1950,
                    544
                ],
                "angle": 0,
                "content": "Among Republicans and Republican leaners, % who say the phrase 'fights for what I believe in' describes Trump ..."
            },
            {
                "block_id": 12,
                "type": "image",
                "bbox": [
                    1433,
                    564,
                    1927,
                    1098
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1433,
                    1115,
                    1948,
                    1161
                ],
                "angle": 0,
                "content": "Note: Based on Republicans and Republican-leaning independents. Source: Survey of U.S. adults conducted Feb. 4-15, 2020."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1433,
                    1171,
                    1632,
                    1194
                ],
                "angle": 0,
                "content": "PEW RESEARCH CENTER"
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    464,
                    1247,
                    726,
                    1283
                ],
                "angle": 0,
                "content": "4. **Age and Hubri:**"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    464,
                    1283,
                    2042,
                    1349
                ],
                "angle": 0,
                "content": "- Younger politically-aware generations (e.g., 18-29, 30-49) show less association with Trump's leadership, indicating a generational shift in perception."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    481,
                    1353,
                    1989,
                    1386
                ],
                "angle": 0,
                "content": "- Older generations (50-64 and \\(65+\\)) associate Trump more strongly with this phrase, suggesting a deeper understanding of his motives."
            },
            {
                "block_id": 18,
                "type": "title",
                "bbox": [
                    464,
                    1415,
                    1104,
                    1448
                ],
                "angle": 0,
                "content": "5. **Educational Background and Political Affiliation**:"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    484,
                    1452,
                    1626,
                    1481
                ],
                "angle": 0,
                "content": "- Higher levels of education correlate with a stronger perception of Trump's commitment to his beliefs."
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    464,
                    1481,
                    2052,
                    1551
                ],
                "angle": 0,
                "content": "- Conservative individuals overwhelmingly see Trump as fighting for their beliefs, which contradicts or signifies a significant portion of their loyalty."
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    464,
                    1452,
                    2052,
                    1551
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "title",
                "bbox": [
                    464,
                    1580,
                    744,
                    1613
                ],
                "angle": 0,
                "content": "6. **Party Affiliation**:"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    464,
                    1617,
                    2068,
                    1683
                ],
                "angle": 0,
                "content": "- Republicans and Republican-leaning individuals see Trump as fighting for their beliefs \\(93\\%\\) of the time, with leaning Republicans (36% very well, 41% fairly well) joining this perception essentially."
            },
            {
                "block_id": 24,
                "type": "title",
                "bbox": [
                    464,
                    1712,
                    895,
                    1745
                ],
                "angle": 0,
                "content": "7. **Percentages and Interpretation**:"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    464,
                    1749,
                    2060,
                    1811
                ],
                "angle": 0,
                "content": "- The highest percentage (94%) seeing Trump in this light belongs to conservative respondents, indicating their strong belief in his ideological commitment."
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    464,
                    1811,
                    2062,
                    1884
                ],
                "angle": 0,
                "content": "- The lowest percentage \\((36\\%)\\) is among moderate/liberal Republicans, signaling that these individuals feel less connected to Trump's political ethos compared to the conservative and leaning Republican groups."
            },
            {
                "block_id": 27,
                "type": "list",
                "bbox": [
                    464,
                    1749,
                    2062,
                    1884
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 28,
                "type": "title",
                "bbox": [
                    464,
                    1910,
                    734,
                    1943
                ],
                "angle": 0,
                "content": "8. **Survey Context**:"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    464,
                    1947,
                    2052,
                    2016
                ],
                "angle": 0,
                "content": "- The survey was conducted in February 2020, reflecting current political sentiment and personal belief perceptions about Trump's leadership style."
            },
            {
                "block_id": 30,
                "type": "title",
                "bbox": [
                    464,
                    2042,
                    800,
                    2079
                ],
                "angle": 0,
                "content": "9. **Possible Implications**:"
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    464,
                    2079,
                    2040,
                    2145
                ],
                "angle": 0,
                "content": "- This indicates changes in age-related political denominations within the Republican party, with older generations having a more consistent and steadfast belief in Trump's dedication to his principles."
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    484,
                    2145,
                    1764,
                    2178
                ],
                "angle": 0,
                "content": "- Education appears to serve as a crucial factor in shaping how individuals perceive Trump's political commitment."
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    484,
                    2178,
                    1555,
                    2211
                ],
                "angle": 0,
                "content": "- There is a strong correlation between party affiliation and belief in Trump's ideological fervor."
            },
            {
                "block_id": 34,
                "type": "list",
                "bbox": [
                    464,
                    2042,
                    2040,
                    2211
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    464,
                    2240,
                    2073,
                    2346
                ],
                "angle": 0,
                "content": "10. **Conclusion**: The data highlights the diverse perceptions of how Donald Trump represents his targets' beliefs, showing variations by age, education, and political alignment. It also underscores the strong ideological and short-term political connection among conservatives, while revealing generational and educational shifts impacting views on Trump's leadership style."
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    459,
                    2376,
                    2065,
                    2676
                ],
                "angle": 0,
                "content": "This detailed description includes all aspects of the image, providing a comprehensive understanding of the data and its implications. It covers various key demographic categories, their perceptions, and the overall narrative they present, enabling a pure text model to answer a wide range of questions related to this graph effectively. The explanation is structured logically, covering different dimensions of the data, from the percentages observed to the implications of these observations, thereby offering a thorough analysis of the given information. The description also includes relevant inferences and conclusions, which are derived from the data presented. The use of text blocks and a structured format aids clarity and ease of comprehension, making it easy for a text model to understand and effectively generate answers to queries based on this description. The inclusion of societal and political context enriches the understanding of the image, providing a broader perspective on the subject matter. This comprehensive approach ensures that the description is both informative and engaging, covering all necessary details and providing a well-rounded understanding of the graph."
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    459,
                    2706,
                    2070,
                    2940
                ],
                "angle": 0,
                "content": "This level of detail also enables other AI systems to process and analyze the information accurately, ensuring that they can answer questions such as: What age groups see Trump most strongly? How does educational background influence perception? What is the political affinity link? These questions can be answered based on the coherent and detailed description provided. The description is based on all visible information in the image, ensuring that no part of the content is overlooked or misunderstood. The structured and logical flow makes it easy to follow, with each segment contributing to a comprehensive understanding of the graph and its implications. The description is both factual and insightful, providing a robust basis for any further inquiries or question-answering-related tasks. Thus, it serves as an excellent example of how to approach and describe an image with precision and thoroughness."
            },
            {
                "block_id": 38,
                "type": "image_caption",
                "bbox": [
                    650,
                    2989,
                    1889,
                    3039
                ],
                "angle": 0,
                "content": "Figure 11: Example of Qwen2.5VL-as-Judge-3B chart understanding."
            },
            {
                "block_id": 39,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1292,
                    3168
                ],
                "angle": 0,
                "content": "21"
            }
        ]
    },
    {
        "page_id": 21,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    446,
                    1560,
                    777,
                    1732
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    1221,
                    1326,
                    1675,
                    1356
                ],
                "angle": 0,
                "content": "Fewer Hallucinations and More Details"
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    821,
                    1379,
                    1104,
                    1412
                ],
                "angle": 0,
                "content": "Original Qwen2.5VL-3B:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    798,
                    1419,
                    1412,
                    1560
                ],
                "angle": 0,
                "content": "...The image depicts a mechanical or engineering schematic of a simple mechanical system. The diagram shows a solid rectangular block that is stationary. The block has dimensions noted in millimeters: a width of \\(200\\mathrm{mm}\\) and a height of \\(400\\mathrm{mm}\\). The block is shown resting on a flat surface."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    798,
                    1584,
                    1415,
                    1758
                ],
                "angle": 0,
                "content": "To the right of the block, there is a single horizontal force labeled \\(\\backslash (\\mathrm{P} = 2\\backslash \\mathrm{text}\\{\\mathrm{kN}\\} \\backslash)\\) (kilonewtons). This force is applied at the center of the block horizontally, indicating it is a horizontal tensile or compressive load depending on the sense of action, which could be pointing either between the left and right faces of the block or parallel to their faces."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    800,
                    1785,
                    1397,
                    1950
                ],
                "angle": 0,
                "content": "There are specific dimensions marked, indicating:  \n- The force \\(\\langle \\mathrm{P} = 2\\langle \\mathrm{text}\\{\\mathrm{kN}\\} \\rangle \\rangle\\) is applied between points that are \\(400\\mathrm{mm}\\) apart horizontally.  \n- The block's bottom surface is part of the foundation or base environment needed to support the weight of the block and the force."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1137,
                    1940,
                    1361,
                    1986
                ],
                "angle": 0,
                "content": "Hallucinations!"
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1476,
                    1376,
                    1624,
                    1402
                ],
                "angle": 0,
                "content": "After CapRL"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1468,
                    1402,
                    1739,
                    1425
                ],
                "angle": 0,
                "content": "...**Dimensions and Labels:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1466,
                    1428,
                    2057,
                    1567
                ],
                "angle": 0,
                "content": "- The block has a height of \\(200\\mathrm{mm}\\), as indicated by a line labeled \"200 mm\" pointing downwards from the top left.\n- The block has a width of \\(400\\mathrm{mm}\\), as indicated by a line labeled \"400 mm\" running horizontally from left to right at the top of the block."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1466,
                    1593,
                    1680,
                    1623
                ],
                "angle": 0,
                "content": "3. **Shape and Color:"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1466,
                    1626,
                    2050,
                    1706
                ],
                "angle": 0,
                "content": "- The block is colored maroon (magenta) and is solid, likely representing a concrete or metal slab, possibly a pavement or foundation."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1466,
                    1735,
                    1657,
                    1765
                ],
                "angle": 0,
                "content": "4. **Force Applied:"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1466,
                    1765,
                    2065,
                    1844
                ],
                "angle": 0,
                "content": "- There is a force \\(\\left(\\mathrm{P} = 2 \\mathrm{~N}\\right)\\) kN (2 kilonewtons) applied to the right side of the block. This force is shown as an arrow labeled \"P = 2 kN\" pointing to the right."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1466,
                    1848,
                    2037,
                    1933
                ],
                "angle": 0,
                "content": "- The force is applied at the top right corner of the block, positioned \\(400\\mathrm{mm}\\) from the left edge and \\(200\\mathrm{mm}\\) from the bottom (this is the top surface of the block)."
            },
            {
                "block_id": 16,
                "type": "image_caption",
                "bbox": [
                    530,
                    2036,
                    2009,
                    2092
                ],
                "angle": 0,
                "content": "Figure 12: Example of CapRL enhancing the captioning ability of Qwen2.5-VL-3B."
            },
            {
                "block_id": 17,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1297,
                    3168
                ],
                "angle": 0,
                "content": "22"
            }
        ]
    },
    {
        "page_id": 22,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    476,
                    653,
                    660,
                    689
                ],
                "angle": 0,
                "content": "CapRL-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    474,
                    693,
                    1331,
                    795
                ],
                "angle": 0,
                "content": "The image is an infographic titled \"Social Media Statistics: Breaking Down the Major Social Networks,\" specifically focusing on Facebook, Twitter, and Pinterest. Below is a detailed description:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    476,
                    834,
                    515,
                    851
                ],
                "angle": 0,
                "content": "-"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    479,
                    891,
                    798,
                    924
                ],
                "angle": 0,
                "content": "**Social Media Statistics**"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    479,
                    960,
                    1208,
                    993
                ],
                "angle": 0,
                "content": "**BREAKING DOWN THE MAJOR SOCIAL NETWORKS**"
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    476,
                    1026,
                    642,
                    1056
                ],
                "angle": 0,
                "content": "Facebook"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    476,
                    1062,
                    1300,
                    1089
                ],
                "angle": 0,
                "content": "- \\(77 \\%\\)**: B2C companies have acquired customers through Facebook."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    476,
                    1089,
                    1300,
                    1122
                ],
                "angle": 0,
                "content": "- \\(43\\%\\): B2B companies have acquired customers through Facebook."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    476,
                    1125,
                    1190,
                    1158
                ],
                "angle": 0,
                "content": "- Facebook accounts for \\(1/2\\) of all retargeted clicks on the web."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    476,
                    1161,
                    1198,
                    1191
                ],
                "angle": 0,
                "content": "- 189 million \"mobile only\" users and 488 million mobile users."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    476,
                    1194,
                    897,
                    1227
                ],
                "angle": 0,
                "content": "- \\(23\\%\\) login 5 or more times per day."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    476,
                    1230,
                    1050,
                    1260
                ],
                "angle": 0,
                "content": "- \\(47\\%\\) of users say Facebook influences purchases."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    476,
                    1263,
                    1290,
                    1293
                ],
                "angle": 0,
                "content": "- \\(80\\%\\) of social media users prefer connecting with brands on Facebook."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    476,
                    1296,
                    1147,
                    1326
                ],
                "angle": 0,
                "content": "- Facebook is made up of \\(43\\%\\) male and \\(57\\%\\) female users."
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    476,
                    1062,
                    1300,
                    1326
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "title",
                "bbox": [
                    476,
                    1362,
                    617,
                    1392
                ],
                "angle": 0,
                "content": "Twitter"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    476,
                    1395,
                    1183,
                    1425
                ],
                "angle": 0,
                "content": "- 55-64 largest growing age demographic, up \\(76\\%\\) since 2013."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    476,
                    1428,
                    1198,
                    1458
                ],
                "angle": 0,
                "content": "- 500 million daily users, producing 175 million tweets per day."
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    476,
                    1461,
                    1091,
                    1491
                ],
                "angle": 0,
                "content": "- \\(69\\%\\) of all reported follows are suggested by friends."
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    476,
                    1494,
                    1155,
                    1527
                ],
                "angle": 0,
                "content": "- \\(34\\%\\) of web marketers have generated leads using Twitter."
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    476,
                    1531,
                    940,
                    1560
                ],
                "angle": 0,
                "content": "- \\(32\\%\\) of all web users are using Twitter."
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    476,
                    1560,
                    1318,
                    1626
                ],
                "angle": 0,
                "content": "- \\(56\\%\\) of all customer tweets to brands are ignored. Research shows the highest CTR (Click-through Rate) comes between 4:30 and 5:30 PM EST."
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    476,
                    1630,
                    1025,
                    1663
                ],
                "angle": 0,
                "content": "- A green icon with \\(56\\%\\) highlights this statistic."
            },
            {
                "block_id": 24,
                "type": "list",
                "bbox": [
                    476,
                    1395,
                    1318,
                    1663
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 25,
                "type": "title",
                "bbox": [
                    476,
                    1706,
                    632,
                    1735
                ],
                "angle": 0,
                "content": "Pinterest"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    476,
                    1739,
                    1303,
                    1768
                ],
                "angle": 0,
                "content": "- \\(80\\%\\) of registered users are female (specifically highlighted with a icon)."
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    476,
                    1772,
                    818,
                    1801
                ],
                "angle": 0,
                "content": "- \\(80\\%\\) of all pins are \"re-pins.\""
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    476,
                    1805,
                    1144,
                    1834
                ],
                "angle": 0,
                "content": "- Pinterest referrals spend \\(70\\%\\) more than non-social traffic."
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    476,
                    1838,
                    1180,
                    1867
                ],
                "angle": 0,
                "content": "- Pins with listed prices get \\(36\\%\\) more likes than those without."
            },
            {
                "block_id": 30,
                "type": "text",
                "bbox": [
                    476,
                    1871,
                    1252,
                    1900
                ],
                "angle": 0,
                "content": "- With 17MIL+ engagements, only \\(15\\%\\) are on brand-specific boards."
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    476,
                    1904,
                    1104,
                    1937
                ],
                "angle": 0,
                "content": "- \\(69\\%\\) have purchased or want to purchase via Pinterest."
            },
            {
                "block_id": 32,
                "type": "list",
                "bbox": [
                    476,
                    1739,
                    1303,
                    1937
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 33,
                "type": "title",
                "bbox": [
                    476,
                    1966,
                    915,
                    1999
                ],
                "angle": 0,
                "content": "Additional Details about Pinterest:"
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    476,
                    2003,
                    800,
                    2032
                ],
                "angle": 0,
                "content": "- Key points about Pinterest:"
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    476,
                    2036,
                    838,
                    2065
                ],
                "angle": 0,
                "content": "- \\(80\\%\\) of all pins are \"re-pins.\""
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    476,
                    2069,
                    938,
                    2098
                ],
                "angle": 0,
                "content": "- Pinterest referrals are highly effective."
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    476,
                    2102,
                    1132,
                    2131
                ],
                "angle": 0,
                "content": "- Pins with prices perform better, getting \\(36\\%\\) more likes."
            },
            {
                "block_id": 38,
                "type": "text",
                "bbox": [
                    476,
                    2135,
                    1132,
                    2164
                ],
                "angle": 0,
                "content": "- Only \\(15\\%\\) of engagements are on brand-specific boards."
            },
            {
                "block_id": 39,
                "type": "text",
                "bbox": [
                    476,
                    2168,
                    1208,
                    2197
                ],
                "angle": 0,
                "content": "- \\(69\\%\\) of users have purchased or want to purchase via Pinterest."
            },
            {
                "block_id": 40,
                "type": "list",
                "bbox": [
                    476,
                    2036,
                    1208,
                    2197
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 41,
                "type": "title",
                "bbox": [
                    476,
                    2234,
                    752,
                    2263
                ],
                "angle": 0,
                "content": "Footer Information:"
            },
            {
                "block_id": 42,
                "type": "text",
                "bbox": [
                    476,
                    2267,
                    2011,
                    2329
                ],
                "angle": 0,
                "content": "- Queen City Media specializes in brand and content marketing on all major social media platforms. Contact them today for a FREE social media evaluation."
            },
            {
                "block_id": 43,
                "type": "text",
                "bbox": [
                    476,
                    2333,
                    673,
                    2362
                ],
                "angle": 0,
                "content": "- Contact details:"
            },
            {
                "block_id": 44,
                "type": "list",
                "bbox": [
                    476,
                    2267,
                    2011,
                    2362
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 45,
                "type": "text",
                "bbox": [
                    476,
                    2366,
                    711,
                    2395
                ],
                "angle": 0,
                "content": "- www.qcmny.com"
            },
            {
                "block_id": 46,
                "type": "text",
                "bbox": [
                    476,
                    2399,
                    714,
                    2428
                ],
                "angle": 0,
                "content": "- info@qcmny.com"
            },
            {
                "block_id": 47,
                "type": "text",
                "bbox": [
                    476,
                    2432,
                    754,
                    2461
                ],
                "angle": 0,
                "content": "- Phone: 716-845-6500"
            },
            {
                "block_id": 48,
                "type": "text",
                "bbox": [
                    476,
                    2465,
                    981,
                    2494
                ],
                "angle": 0,
                "content": "- Address: 515 Main St, Buffalo, NY 14203"
            },
            {
                "block_id": 49,
                "type": "text",
                "bbox": [
                    476,
                    2498,
                    816,
                    2527
                ],
                "angle": 0,
                "content": "- Social media links provided:"
            },
            {
                "block_id": 50,
                "type": "text",
                "bbox": [
                    476,
                    2531,
                    775,
                    2560
                ],
                "angle": 0,
                "content": "- fb.com/queencitymedia"
            },
            {
                "block_id": 51,
                "type": "text",
                "bbox": [
                    476,
                    2564,
                    726,
                    2593
                ],
                "angle": 0,
                "content": "- twitter.com/qcmny"
            },
            {
                "block_id": 52,
                "type": "text",
                "bbox": [
                    476,
                    2597,
                    767,
                    2630
                ],
                "angle": 0,
                "content": "- pinterest.com/qcmny..."
            },
            {
                "block_id": 53,
                "type": "list",
                "bbox": [
                    476,
                    2366,
                    981,
                    2630
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 54,
                "type": "title",
                "bbox": [
                    1420,
                    646,
                    2004,
                    709
                ],
                "angle": 0,
                "content": "Social Media Statistics"
            },
            {
                "block_id": 55,
                "type": "text",
                "bbox": [
                    1535,
                    712,
                    1902,
                    735
                ],
                "angle": 0,
                "content": "BREAKING DOWN THE MAJOR SOCIAL NETWORKS"
            },
            {
                "block_id": 56,
                "type": "image",
                "bbox": [
                    1417,
                    762,
                    1552,
                    828
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 57,
                "type": "text",
                "bbox": [
                    1417,
                    831,
                    1542,
                    854
                ],
                "angle": 0,
                "content": "BXC COMPANIES HAVE ACQUIRED"
            },
            {
                "block_id": 58,
                "type": "text",
                "bbox": [
                    1417,
                    858,
                    1552,
                    877
                ],
                "angle": 0,
                "content": "CUSTOMERS THROUGH FACEBOOK"
            },
            {
                "block_id": 59,
                "type": "image",
                "bbox": [
                    1417,
                    877,
                    1552,
                    933
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 60,
                "type": "text",
                "bbox": [
                    1417,
                    937,
                    1542,
                    956
                ],
                "angle": 0,
                "content": "PDB COMPLEX NUCLELS ASCOHRED"
            },
            {
                "block_id": 61,
                "type": "text",
                "bbox": [
                    1417,
                    956,
                    1542,
                    976
                ],
                "angle": 0,
                "content": "S72 COMPANIES HAVE ALQUCEDCUSTOMERS THROUGH FACEBOOK"
            },
            {
                "block_id": 62,
                "type": "title",
                "bbox": [
                    1693,
                    749,
                    1912,
                    801
                ],
                "angle": 0,
                "content": "facebook"
            },
            {
                "block_id": 63,
                "type": "text",
                "bbox": [
                    1609,
                    811,
                    1963,
                    834
                ],
                "angle": 0,
                "content": "ACCOUNTS FOR 1/2 OF ALL RETARGETED Clicks ON THE WEB"
            },
            {
                "block_id": 64,
                "type": "text",
                "bbox": [
                    1609,
                    841,
                    2024,
                    867
                ],
                "angle": 0,
                "content": "189 MILLION \"MOBILE ONLY\" USERS AND 488 MILLION MOBILE USERS"
            },
            {
                "block_id": 65,
                "type": "text",
                "bbox": [
                    1609,
                    871,
                    1841,
                    891
                ],
                "angle": 0,
                "content": "23% LOGIN 5 OR MORE TIMES PER DAY"
            },
            {
                "block_id": 66,
                "type": "list",
                "bbox": [
                    1609,
                    811,
                    2024,
                    891
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 67,
                "type": "text",
                "bbox": [
                    1609,
                    904,
                    1938,
                    927
                ],
                "angle": 0,
                "content": "47% OF USERS SAY FACEBOOK INFLUENCES PURCHASES"
            },
            {
                "block_id": 68,
                "type": "text",
                "bbox": [
                    1609,
                    937,
                    2050,
                    960
                ],
                "angle": 0,
                "content": "80% SOCIAL MEDIA USERS PREFER CONNECTING W/ BRANDS ON FACEBOOK"
            },
            {
                "block_id": 69,
                "type": "text",
                "bbox": [
                    1609,
                    963,
                    1978,
                    990
                ],
                "angle": 0,
                "content": "FACEBOOK IS MADE UP OF \\(43\\%\\) MALE AND \\(57\\%\\) FEMALE USERS"
            },
            {
                "block_id": 70,
                "type": "list",
                "bbox": [
                    1609,
                    904,
                    2050,
                    990
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 71,
                "type": "title",
                "bbox": [
                    1456,
                    1016,
                    1721,
                    1065
                ],
                "angle": 0,
                "content": "twitter"
            },
            {
                "block_id": 72,
                "type": "text",
                "bbox": [
                    1377,
                    1072,
                    1769,
                    1098
                ],
                "angle": 0,
                "content": "5S-64 LARGEST GROWING AGE DEMOGRAPHIC, UP 76% SINE 2013"
            },
            {
                "block_id": 73,
                "type": "text",
                "bbox": [
                    1377,
                    1102,
                    1698,
                    1125
                ],
                "angle": 0,
                "content": "500 MILLION DAILY USERS - 175 MILLION TWEETS/DAY"
            },
            {
                "block_id": 74,
                "type": "text",
                "bbox": [
                    1377,
                    1135,
                    1744,
                    1158
                ],
                "angle": 0,
                "content": "69% OF ALL REPORTED FOLLOWS ARE SUGGESTED BY FRIENDS"
            },
            {
                "block_id": 75,
                "type": "text",
                "bbox": [
                    1377,
                    1168,
                    1772,
                    1191
                ],
                "angle": 0,
                "content": "34% OF WEB MARKETERS HAVE GENERATED LEADS USING TWITTER"
            },
            {
                "block_id": 76,
                "type": "text",
                "bbox": [
                    1377,
                    1201,
                    1644,
                    1224
                ],
                "angle": 0,
                "content": "32% OF ALL WEB USERS ARE USING TWITTER"
            },
            {
                "block_id": 77,
                "type": "text",
                "bbox": [
                    1377,
                    1230,
                    1825,
                    1254
                ],
                "angle": 0,
                "content": "RESEARCH SHOWS THE HIGHEST CTR COMES BETWEEN 4:30 AND 5:30PM EST"
            },
            {
                "block_id": 78,
                "type": "list",
                "bbox": [
                    1377,
                    1072,
                    1825,
                    1254
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 79,
                "type": "image",
                "bbox": [
                    1889,
                    1052,
                    2027,
                    1201
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 80,
                "type": "text",
                "bbox": [
                    1864,
                    1201,
                    2052,
                    1224
                ],
                "angle": 0,
                "content": "OF ALL CUSTOMER TWEETS TO BRANDS"
            },
            {
                "block_id": 81,
                "type": "text",
                "bbox": [
                    1889,
                    1224,
                    2011,
                    1247
                ],
                "angle": 0,
                "content": "ARE Ignored"
            },
            {
                "block_id": 82,
                "type": "image",
                "bbox": [
                    1377,
                    1306,
                    1519,
                    1471
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 83,
                "type": "image",
                "bbox": [
                    1448,
                    1339,
                    1565,
                    1425
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 84,
                "type": "text",
                "bbox": [
                    1448,
                    1428,
                    1603,
                    1452
                ],
                "angle": 0,
                "content": "80% OF REGISTERED"
            },
            {
                "block_id": 85,
                "type": "text",
                "bbox": [
                    1448,
                    1452,
                    1601,
                    1471
                ],
                "angle": 0,
                "content": "USERS ARE FEMALE"
            },
            {
                "block_id": 86,
                "type": "title",
                "bbox": [
                    1713,
                    1280,
                    1909,
                    1336
                ],
                "angle": 0,
                "content": "Pinterest"
            },
            {
                "block_id": 87,
                "type": "text",
                "bbox": [
                    1619,
                    1349,
                    1813,
                    1369
                ],
                "angle": 0,
                "content": "80% OF ALL PINS ARE \"RE-PINS\""
            },
            {
                "block_id": 88,
                "type": "text",
                "bbox": [
                    1619,
                    1379,
                    2027,
                    1402
                ],
                "angle": 0,
                "content": "PINTEREST REFERRALS SPEND 70% MORE THAN NON-SOCIAL TRAFFIC"
            },
            {
                "block_id": 89,
                "type": "text",
                "bbox": [
                    1619,
                    1409,
                    2027,
                    1432
                ],
                "angle": 0,
                "content": "PINS WITH LISTED PRICES GET 36% MORE LIKES THAN THOSE WITHOUT"
            },
            {
                "block_id": 90,
                "type": "text",
                "bbox": [
                    1619,
                    1442,
                    2057,
                    1465
                ],
                "angle": 0,
                "content": "WITH 17MIL+ ENGAGEMENTS ONLY 15% ARE ON BRAND-SPECIFIC BOARDS"
            },
            {
                "block_id": 91,
                "type": "text",
                "bbox": [
                    1619,
                    1475,
                    1991,
                    1494
                ],
                "angle": 0,
                "content": "69% HAVE PURCHASED OR WANT TO PURCHASE VIA PINTEREST"
            },
            {
                "block_id": 92,
                "type": "text",
                "bbox": [
                    1402,
                    1531,
                    2027,
                    1577
                ],
                "angle": 0,
                "content": "Queen City Media specializes in brand and content marketing on all of the major social media platforms. Contact us today for a FREE social media evaluation."
            },
            {
                "block_id": 93,
                "type": "text",
                "bbox": [
                    1366,
                    1597,
                    1473,
                    1620
                ],
                "angle": 0,
                "content": "www.qcmny.com"
            },
            {
                "block_id": 94,
                "type": "text",
                "bbox": [
                    1366,
                    1623,
                    1443,
                    1643
                ],
                "angle": 0,
                "content": "mice memy 716.845.6500"
            },
            {
                "block_id": 95,
                "type": "text",
                "bbox": [
                    1366,
                    1643,
                    1473,
                    1666
                ],
                "angle": 0,
                "content": "515 Main St. Buffalo, NY 14203"
            },
            {
                "block_id": 96,
                "type": "list",
                "bbox": [
                    1366,
                    1597,
                    1473,
                    1666
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 97,
                "type": "text",
                "bbox": [
                    1606,
                    1597,
                    1820,
                    1620
                ],
                "angle": 0,
                "content": "fb.com/queencitymedia"
            },
            {
                "block_id": 98,
                "type": "text",
                "bbox": [
                    1606,
                    1623,
                    1797,
                    1643
                ],
                "angle": 0,
                "content": "Twitter.com/qcmny "
            },
            {
                "block_id": 99,
                "type": "text",
                "bbox": [
                    1606,
                    1646,
                    1810,
                    1669
                ],
                "angle": 0,
                "content": "pinterest.com/qcmny"
            },
            {
                "block_id": 100,
                "type": "list",
                "bbox": [
                    1606,
                    1597,
                    1820,
                    1669
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 101,
                "type": "image",
                "bbox": [
                    1981,
                    1587,
                    2062,
                    1676
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 13: An illustrative example of CapRL applied to infographic understanding."
            },
            {
                "block_id": 103,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "23"
            }
        ]
    },
    {
        "page_id": 23,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    469,
                    683,
                    650,
                    722
                ],
                "angle": 0,
                "content": "CapRL-3B:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    464,
                    726,
                    1086,
                    927
                ],
                "angle": 0,
                "content": "The image depicts a serene outdoor scene featuring two individuals, likely a couple, sitting on a wooden bench in what appears to be a garden or a courtyard decorated in patriotic themes, probably for a historical or commemorative event given the context. Here is a detailed description:"
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    469,
                    956,
                    803,
                    990
                ],
                "angle": 0,
                "content": "1. **Setting and Location:**"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    464,
                    993,
                    1058,
                    1191
                ],
                "angle": 0,
                "content": "- The image is taken in a garden or patio area, evidenced by the presence of plants, flowers, and a garden bench. The setting is adorned with British flags, specifically Union Jacks, suggesting a British context, possibly near a location in the United Kingdom."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    466,
                    1224,
                    665,
                    1257
                ],
                "angle": 0,
                "content": "2. **Subjects:**"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    466,
                    1260,
                    1073,
                    1326
                ],
                "angle": 0,
                "content": "- The image shows a man and a woman sitting side by side on a wooden bench."
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    1101,
                    689,
                    2073,
                    1339
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    466,
                    1349,
                    2022,
                    1419
                ],
                "angle": 0,
                "content": "- The woman is on the left, wearing a green dress that is a vintage style, likely from the 1940s or 1950s, given its emerald green color and full skirt. She has blonde hair and is wearing sunglasses and dark brown shoes. She is holding the man's hand."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    469,
                    1448,
                    632,
                    1478
                ],
                "angle": 0,
                "content": "3. **Attire:**"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    484,
                    1481,
                    2017,
                    1518
                ],
                "angle": 0,
                "content": "- The woman is dressed in a green dress, which is greenish-blue in color, typical of a retro or wartime era outfit, possibly from the 1940s."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    466,
                    1547,
                    619,
                    1577
                ],
                "angle": 0,
                "content": "4. **Man:**"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    464,
                    1580,
                    2042,
                    1683
                ],
                "angle": 0,
                "content": "- The man is on the right, dressed in a military uniform, specifically a cap (possibly aailor's cap) and a dark blue military uniform with anchor pins, indicating he might be a naval or air force officer. He is wearing a cap with a badge and a jacket with buttons, typical of military attire from that period."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    466,
                    1716,
                    861,
                    1745
                ],
                "angle": 0,
                "content": "5. **Furniture and Background:**"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    466,
                    1749,
                    1973,
                    1815
                ],
                "angle": 0,
                "content": "- They are sitting on a wooden bench that is outdoors, likely in a garden or park, as indicated by the various potted plants and flowers around it. The bench is outdoors, possibly on a sidewalk or patio."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    466,
                    1848,
                    617,
                    1880
                ],
                "angle": 0,
                "content": "6. **Flag:**"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    464,
                    1880,
                    2029,
                    1983
                ],
                "angle": 0,
                "content": "- A large British flag (Union Jack) is prominently displayed behind the couple, draped over the back of the bench. The flag is red, white, and blue, with remnants of text or inscriptions on it, suggesting it might be old or vintage, possibly from a ship or military context. This flag spans across the image from left to right."
            },
            {
                "block_id": 17,
                "type": "title",
                "bbox": [
                    466,
                    2013,
                    851,
                    2046
                ],
                "angle": 0,
                "content": "7. **Flag Display: Union Jack:**"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    464,
                    2046,
                    1999,
                    2115
                ],
                "angle": 0,
                "content": "- The flag is large and appears to be old, with hues of red, white, and blue, typical of the British national flag. It has a mix of pink in the middle, indicating it might be from a historical or naval setting."
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    466,
                    2145,
                    757,
                    2178
                ],
                "angle": 0,
                "content": "8. **Plants and Decor:**"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    464,
                    2178,
                    1983,
                    2247
                ],
                "angle": 0,
                "content": "- The garden is lush with various plants and flowers, including a pot on the left containing greenery in a blue pot, likely herbs or small plants. There are also other planted areas with tall green onions or garlic chives (likely Allium fistulosum)."
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    466,
                    2277,
                    935,
                    2310
                ],
                "angle": 0,
                "content": "9. **Ground and Additional Elements:**"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    464,
                    2310,
                    2022,
                    2382
                ],
                "angle": 0,
                "content": "- The ground is paved with a brick or concrete sidewalk, and there is a small drain or cover on the right side of the image. The bottom left corner mentions \"Mercury Press & Media Ltd,\" suggesting the image is from a media source."
            },
            {
                "block_id": 23,
                "type": "title",
                "bbox": [
                    469,
                    2409,
                    775,
                    2445
                ],
                "angle": 0,
                "content": "10. **Text and Settings:**"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    494,
                    2445,
                    2052,
                    2481
                ],
                "angle": 0,
                "content": "- The sky is bright, indicating it is a sunny day, likely spring or summer given the well-maintained garden and the couple's vintage clothing."
            },
            {
                "block_id": 25,
                "type": "title",
                "bbox": [
                    469,
                    2511,
                    665,
                    2544
                ],
                "angle": 0,
                "content": "11. **Objects:**"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    464,
                    2544,
                    2022,
                    2610
                ],
                "angle": 0,
                "content": "- The man is wearing dark navy blue pants and shoes typical of military uniformity, complete with belt and decorations typical of a naval officer."
            },
            {
                "block_id": 27,
                "type": "image_caption",
                "bbox": [
                    510,
                    2669,
                    2029,
                    2722
                ],
                "angle": 0,
                "content": "Figure 14: An illustrative example of CapRL applied to natural image understanding."
            },
            {
                "block_id": 28,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1297,
                    3168
                ],
                "angle": 0,
                "content": "24"
            }
        ]
    },
    {
        "page_id": 24,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    155
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    502,
                    455,
                    1558,
                    511
                ],
                "angle": 0,
                "content": "Prompt for LLM to answer questions based on Caption"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    499,
                    570,
                    1338,
                    610
                ],
                "angle": 0,
                "content": "You will be given an image caption describing the visual content."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    499,
                    610,
                    1899,
                    689
                ],
                "angle": 0,
                "content": "Your task is to answer the multiple-choice question **strictly based on the caption**, even if the answer may seem obvious from prior knowledge or question wording."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    497,
                    722,
                    1861,
                    798
                ],
                "angle": 0,
                "content": "Ignore any external knowledge. Do not make assumptions beyond what the caption explicitly or implicitly states."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    499,
                    838,
                    657,
                    874
                ],
                "angle": 0,
                "content": "Example 1:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    499,
                    874,
                    1894,
                    953
                ],
                "angle": 0,
                "content": "Caption: <Caption Start> A woman in a red coat is walking a black dog across a snowy park. <Caption End> Question: What color is the dog?"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    502,
                    953,
                    657,
                    990
                ],
                "angle": 0,
                "content": "-A) Brown"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    502,
                    993,
                    647,
                    1026
                ],
                "angle": 0,
                "content": "- B) White"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    502,
                    1029,
                    645,
                    1062
                ],
                "angle": 0,
                "content": "- C) Black"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    502,
                    1069,
                    634,
                    1105
                ],
                "angle": 0,
                "content": "- D) Gray"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    502,
                    1105,
                    1030,
                    1145
                ],
                "angle": 0,
                "content": "- E) Can not answer based on the caption"
            },
            {
                "block_id": 12,
                "type": "list",
                "bbox": [
                    502,
                    953,
                    1030,
                    1145
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    499,
                    1178,
                    724,
                    1217
                ],
                "angle": 0,
                "content": "The answer is C."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    499,
                    1254,
                    657,
                    1290
                ],
                "angle": 0,
                "content": "Example 2:"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    499,
                    1293,
                    1652,
                    1369
                ],
                "angle": 0,
                "content": "Caption: <Caption Start> A child is waving a British flag during a parade. <Caption End>  \nQuestion: What color is the flag?"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    502,
                    1372,
                    624,
                    1405
                ],
                "angle": 0,
                "content": "-A)Red"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    502,
                    1409,
                    629,
                    1445
                ],
                "angle": 0,
                "content": "-B) Blue"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    502,
                    1448,
                    831,
                    1485
                ],
                "angle": 0,
                "content": "- C) Red, white, and blue"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    502,
                    1488,
                    647,
                    1521
                ],
                "angle": 0,
                "content": "- D) White"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    502,
                    1524,
                    1030,
                    1560
                ],
                "angle": 0,
                "content": "- E) Can not answer based on the caption"
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    502,
                    1372,
                    1030,
                    1560
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    499,
                    1597,
                    721,
                    1633
                ],
                "angle": 0,
                "content": "The answer is E."
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    497,
                    1673,
                    1246,
                    1716
                ],
                "angle": 0,
                "content": "Now, answer the question based on the following caption:"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    497,
                    1749,
                    1073,
                    1788
                ],
                "angle": 0,
                "content": "Caption: <Caption Start> {} <Caption End>"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    502,
                    1788,
                    670,
                    1828
                ],
                "angle": 0,
                "content": "Question: {}"
            },
            {
                "block_id": 26,
                "type": "image_caption",
                "bbox": [
                    675,
                    1999,
                    1864,
                    2052
                ],
                "angle": 0,
                "content": "Figure 15: Prompt for LLM to answer questions based on Caption."
            },
            {
                "block_id": 27,
                "type": "title",
                "bbox": [
                    502,
                    2267,
                    1379,
                    2319
                ],
                "angle": 0,
                "content": "Prompt for Unified Reward Model as a Judge"
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    484,
                    2382,
                    1887,
                    2461
                ],
                "angle": 0,
                "content": "You are presented with an image along with its generated text caption. Your task is to comparatively evaluate the caption quality based on the image."
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    484,
                    2461,
                    1282,
                    2498
                ],
                "angle": 0,
                "content": "Provide overall assessment for the caption (rated from 1 to 5):"
            },
            {
                "block_id": 30,
                "type": "text",
                "bbox": [
                    484,
                    2498,
                    1897,
                    2577
                ],
                "angle": 0,
                "content": "- Overall Score: How well the caption comprehensively covers the information in the image, provides detailed descriptions of each object, and maintains accuracy in the conveyed information."
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    484,
                    2610,
                    1099,
                    2653
                ],
                "angle": 0,
                "content": "Output your evaluation using the format below:"
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    484,
                    2686,
                    800,
                    2729
                ],
                "angle": 0,
                "content": "- Overall Score (1-5): X"
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    484,
                    2762,
                    912,
                    2805
                ],
                "angle": 0,
                "content": "Your task is provided as follows:"
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    484,
                    2805,
                    729,
                    2844
                ],
                "angle": 0,
                "content": "Text Caption: \\(\\left[\\{\\}\\right]\\)"
            },
            {
                "block_id": 35,
                "type": "image_caption",
                "bbox": [
                    762,
                    2923,
                    1779,
                    2976
                ],
                "angle": 0,
                "content": "Figure 16: Prompt for Unified Reward Model as a Judge."
            },
            {
                "block_id": 36,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1295,
                    3168
                ],
                "angle": 0,
                "content": "25"
            }
        ]
    },
    {
        "page_id": 25,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    443,
                    112,
                    586,
                    158
                ],
                "angle": 0,
                "content": "Preprint"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    502,
                    594,
                    1259,
                    653
                ],
                "angle": 0,
                "content": "Prompt for Qwen2.5-VL-3B as a Judge"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    481,
                    709,
                    1887,
                    788
                ],
                "angle": 0,
                "content": "You are presented with an image along with its generated text caption. Your task is to comparatively evaluate the caption quality based on the image."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    484,
                    788,
                    1282,
                    828
                ],
                "angle": 0,
                "content": "Provide overall assessment for the caption (rated from 1 to 5):"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    484,
                    828,
                    1894,
                    904
                ],
                "angle": 0,
                "content": "- Overall Score: How well the caption comprehensively covers the information in the image, provides detailed descriptions of each object, and maintains accuracy in the conveyed information."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    484,
                    940,
                    1272,
                    980
                ],
                "angle": 0,
                "content": "Output your overall score for this caption as a single number."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    484,
                    1016,
                    912,
                    1052
                ],
                "angle": 0,
                "content": "Your task is provided as follows:"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    484,
                    1056,
                    729,
                    1095
                ],
                "angle": 0,
                "content": "Text Caption: \\(\\left[\\{\\}\\right]\\)"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    484,
                    1095,
                    706,
                    1128
                ],
                "angle": 0,
                "content": "Your evaluation:"
            },
            {
                "block_id": 9,
                "type": "image_caption",
                "bbox": [
                    818,
                    1250,
                    1718,
                    1303
                ],
                "angle": 0,
                "content": "Figure 17: Prompt for Qwen2.5-VL-3B as a Judge."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    502,
                    1778,
                    1310,
                    1834
                ],
                "angle": 0,
                "content": "Prompt Qwen2.5-VL-72B to generate QA"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    499,
                    1897,
                    1930,
                    1973
                ],
                "angle": 0,
                "content": "Your task is to generate five multiple-choice questions and their answers about the object based on the provided image."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    502,
                    1976,
                    1838,
                    2049
                ],
                "angle": 0,
                "content": "The questions should be challenge and focus on the image content, you answer should strictly follow the following format:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    502,
                    2052,
                    1677,
                    2088
                ],
                "angle": 0,
                "content": "1. **Which method achieves the highest accuracy (Acc) on the FF++ (HQ) dataset?**"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    525,
                    2092,
                    739,
                    2125
                ],
                "angle": 0,
                "content": "-A)Method \"a\""
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    527,
                    2128,
                    739,
                    2164
                ],
                "angle": 0,
                "content": "-B)Method\"b\""
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    527,
                    2164,
                    736,
                    2197
                ],
                "angle": 0,
                "content": "- C) Method \"c\""
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    527,
                    2204,
                    655,
                    2237
                ],
                "angle": 0,
                "content": "- D) Ours"
            },
            {
                "block_id": 18,
                "type": "list",
                "bbox": [
                    525,
                    2092,
                    739,
                    2237
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    502,
                    2277,
                    793,
                    2313
                ],
                "angle": 0,
                "content": "**Answer:** D) Ours"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    502,
                    2352,
                    1343,
                    2392
                ],
                "angle": 0,
                "content": "2. **What is the primary color of the kayak in the image?**"
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    525,
                    2399,
                    652,
                    2432
                ],
                "angle": 0,
                "content": "-A) Blue"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    527,
                    2435,
                    647,
                    2468
                ],
                "angle": 0,
                "content": "-B)Red"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    527,
                    2471,
                    665,
                    2504
                ],
                "angle": 0,
                "content": "- C) Black"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    527,
                    2508,
                    673,
                    2541
                ],
                "angle": 0,
                "content": "- D) White"
            },
            {
                "block_id": 25,
                "type": "list",
                "bbox": [
                    525,
                    2399,
                    673,
                    2541
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    502,
                    2580,
                    780,
                    2616
                ],
                "angle": 0,
                "content": "**Answer:** B) Red"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    502,
                    2626,
                    576,
                    2649
                ],
                "angle": 0,
                "content": "#"
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    499,
                    2656,
                    1853,
                    2696
                ],
                "angle": 0,
                "content": "You should strictly follow the above format and should not generate irrelevant sentences. All the quesiton"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    502,
                    2696,
                    1017,
                    2735
                ],
                "angle": 0,
                "content": "should be answered based on the image."
            },
            {
                "block_id": 30,
                "type": "image_caption",
                "bbox": [
                    803,
                    2781,
                    1736,
                    2834
                ],
                "angle": 0,
                "content": "Figure 18: Prompt Qwen2.5-VL-72B to generate QA"
            },
            {
                "block_id": 31,
                "type": "page_number",
                "bbox": [
                    1249,
                    3128,
                    1297,
                    3168
                ],
                "angle": 0,
                "content": "26"
            }
        ]
    }
]