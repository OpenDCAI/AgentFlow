#!/usr/bin/env python
"""
MMLongBench Benchmark æµ‹è¯•è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
1. é¦–å…ˆè½¬æ¢æ•°æ®: python load_mmlongbench.py --input your_data.json --output mmlongbench_demo.jsonl
2. ç„¶åè¿è¡Œæ­¤è„šæœ¬: python test_mmlongbench_benchmark.py
"""

import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from benchmark import create_benchmark


def test_benchmark_loading():
    """æµ‹è¯• benchmark åŠ è½½"""
    print("=" * 60)
    print("æµ‹è¯• 1: Benchmark åŠ è½½")
    print("=" * 60)
    
    # ä½¿ç”¨æµ‹è¯•æ•°æ®
    data_path = os.path.join(os.path.dirname(__file__), "test_mmlongbench_output.jsonl")
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆè¿è¡Œ: python load_mmlongbench.py --input test_mmlongbench.json --output test_mmlongbench_output.jsonl")
        return None
    
    benchmark = create_benchmark(
        data_path=data_path,
        name="MMLongBench Test",
        description="MMLongBench æµ‹è¯•æ•°æ®é›†"
    )
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(benchmark.items)} æ¡æ•°æ®")
    print(f"âœ“ Benchmark åç§°: {benchmark.name}")
    
    # æ˜¾ç¤ºç¬¬ä¸€æ¡æ•°æ®
    if benchmark.items:
        item = benchmark.items[0]
        print(f"\nç¬¬ä¸€æ¡æ•°æ®:")
        print(f"  ID: {item.id}")
        print(f"  é—®é¢˜: {item.question[:60]}...")
        print(f"  ç­”æ¡ˆ: {item.answer}")
        if item.metadata:
            print(f"  å…ƒæ•°æ®å­—æ®µ: {list(item.metadata.keys())}")
            print(f"  æ–‡æ¡£ç±»å‹: {item.metadata.get('doc_type')}")
            print(f"  ç­”æ¡ˆæ ¼å¼: {item.metadata.get('answer_format')}")
    
    return benchmark


def test_evaluation(benchmark):
    """æµ‹è¯•è¯„ä¼°åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: è¯„ä¼°åŠŸèƒ½")
    print("=" * 60)
    
    if not benchmark:
        print("âŒ Benchmark æœªåŠ è½½ï¼Œè·³è¿‡è¯„ä¼°æµ‹è¯•")
        return
    
    # åˆ›å»ºä¸€äº›é¢„æµ‹ï¼ˆè¿™é‡Œä½¿ç”¨æ­£ç¡®ç­”æ¡ˆä½œä¸ºç¤ºä¾‹ï¼‰
    predictions = {}
    for item in benchmark.items:
        predictions[item.id] = item.answer  # ä½¿ç”¨æ­£ç¡®ç­”æ¡ˆï¼Œåº”è¯¥å¾—åˆ°æ»¡åˆ†
    
    # æµ‹è¯•ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡
    metrics = ["exact_match", "f1_score", "similarity"]
    
    for metric in metrics:
        print(f"\nä½¿ç”¨æŒ‡æ ‡: {metric}")
        results = benchmark.evaluate(predictions, metric=metric)
        
        scores = [r.score for r in results]
        avg_score = sum(scores) / len(scores) if scores else 0
        perfect_matches = sum(1 for s in scores if s == 1.0)
        
        print(f"  å¹³å‡åˆ†æ•°: {avg_score:.3f}")
        print(f"  å®Œç¾åŒ¹é…: {perfect_matches}/{len(results)}")
        
        # æ˜¾ç¤ºæ¯ä¸ªç»“æœçš„åˆ†æ•°
        for result in results:
            status = "âœ“" if result.score == 1.0 else "âœ—"
            print(f"  {status} {result.item_id}: {result.score:.3f}")


def test_wrong_predictions(benchmark):
    """æµ‹è¯•é”™è¯¯é¢„æµ‹çš„è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: é”™è¯¯é¢„æµ‹è¯„ä¼°")
    print("=" * 60)
    
    if not benchmark:
        print("âŒ Benchmark æœªåŠ è½½ï¼Œè·³è¿‡é”™è¯¯é¢„æµ‹æµ‹è¯•")
        return
    
    # åˆ›å»ºé”™è¯¯çš„é¢„æµ‹
    wrong_predictions = {}
    for item in benchmark.items:
        # æ•…æ„ç»™å‡ºé”™è¯¯ç­”æ¡ˆ
        wrong_predictions[item.id] = "é”™è¯¯çš„ç­”æ¡ˆ"
    
    # ä½¿ç”¨ exact_match è¯„ä¼°
    results = benchmark.evaluate(wrong_predictions, metric="exact_match")
    
    scores = [r.score for r in results]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    print(f"é”™è¯¯é¢„æµ‹çš„å¹³å‡åˆ†æ•°: {avg_score:.3f} (åº”è¯¥æ¥è¿‘ 0)")
    print(f"å®Œç¾åŒ¹é…: {sum(1 for s in scores if s == 1.0)}/{len(results)} (åº”è¯¥ä¸º 0)")


def test_summary(benchmark):
    """æµ‹è¯•æ‘˜è¦åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: æ‘˜è¦åŠŸèƒ½")
    print("=" * 60)
    
    if not benchmark:
        print("âŒ Benchmark æœªåŠ è½½ï¼Œè·³è¿‡æ‘˜è¦æµ‹è¯•")
        return
    
    # å…ˆè¿›è¡Œä¸€æ¬¡è¯„ä¼°
    predictions = {item.id: item.answer for item in benchmark.items}
    benchmark.evaluate(predictions, metric="exact_match")
    
    # è·å–æ‘˜è¦
    summary = benchmark.get_summary()
    
    print("Benchmark æ‘˜è¦:")
    for key, value in summary.items():
        print(f"  {key}: {value}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ MMLongBench Benchmark æµ‹è¯•\n")
    
    # æµ‹è¯• 1: åŠ è½½
    benchmark = test_benchmark_loading()
    
    # æµ‹è¯• 2: è¯„ä¼°
    test_evaluation(benchmark)
    
    # æµ‹è¯• 3: é”™è¯¯é¢„æµ‹
    test_wrong_predictions(benchmark)
    
    # æµ‹è¯• 4: æ‘˜è¦
    test_summary(benchmark)
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    print("\nä¸‹ä¸€æ­¥:")
    print("1. è½¬æ¢ä½ çš„ MMLongBench æ•°æ®:")
    print("   python load_mmlongbench.py --input your_data.json --output mmlongbench_demo.jsonl")
    print("\n2. è¿è¡Œå®Œæ•´çš„ benchmark æµ‹è¯•:")
    print("   python ../run.py --mode doc --data mmlongbench_demo.jsonl \\")
    print("       --ocr-model-path /path/to/model --ocr-backend-type transformers")


if __name__ == "__main__":
    main()

