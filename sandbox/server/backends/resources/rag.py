# sandbox/server/backends/resources/rag.py
"""
RAG åç«¯ - æ–‡æ¡£æ£€ç´¢æœåŠ¡

æä¾›é«˜æ€§èƒ½æ–‡æ¡£æ£€ç´¢åŠŸèƒ½ï¼š
- QueryBatcher: æ‰¹é‡è¯·æ±‚æ”¶é›†å™¨ï¼Œè‡ªåŠ¨èšåˆå¤šä¸ªæŸ¥è¯¢è¯·æ±‚
- DenseE5RAGIndex: Faiss + E5 å¯†é›†æ£€ç´¢
- å¼‚æ­¥éé˜»å¡æ¥å£

é…ç½®å‚æ•°:
- model_name: E5 æ¨¡å‹è·¯å¾„æˆ–åç§°
- index_path: Faiss ç´¢å¼•æ–‡ä»¶è·¯å¾„
- corpus_path: è¯­æ–™åº“ JSONL æ–‡ä»¶è·¯å¾„
- device: è®¾å¤‡é…ç½®ï¼ˆå¦‚ "cuda:0" æˆ– "cuda:0/cuda:1"ï¼‰
- batcher_trigger_batch_size: è§¦å‘æ‰¹å¤„ç†çš„æœ€å°è¯·æ±‚æ•°ï¼ˆé»˜è®¤ 16ï¼‰
- batcher_max_batch_size: å•æ‰¹æ¬¡æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ 32ï¼‰
- batcher_max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ 0.05ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
```python
from sandbox.server import HTTPServiceServer
from sandbox.server.backends.resources import RAGBackend

server = HTTPServiceServer()
server.load_backend(RAGBackend())
server.run()
```

å®¢æˆ·ç«¯è°ƒç”¨:
```python
async with Sandbox() as sandbox:
    # æ˜¾å¼é¢„çƒ­ï¼ˆå¯é€‰ï¼Œå‡å°‘é¦–æ¬¡è°ƒç”¨å»¶è¿Ÿï¼‰
    await sandbox.warmup(["rag"])
    
    # å•æ¡æŸ¥è¯¢
    result = await sandbox.execute("rag:search", {"query": "Python tutorial", "top_k": 10})
    
    # æ‰¹é‡æŸ¥è¯¢
    result = await sandbox.execute("rag:batch_search", {
        "queries": ["Python", "Java", "Go"],
        "top_k": 5
    })
```
"""

import logging
import asyncio
import time
import os
import json
import struct
from typing import Dict, Any, List, Optional, Union, cast, Tuple
from dataclasses import dataclass, field
from collections import deque

import numpy as np
import torch
from tqdm import tqdm

from sandbox.server.backends.base import Backend, BackendConfig
from sandbox.server.core import tool
from sandbox.server.backends.error_codes import ErrorCode, get_error_message
from sandbox.server.backends.response_builder import (
    build_success_response,
    build_error_response,
    ResponseTimer,
)

logger = logging.getLogger("RAGBackend")
index_logger = logging.getLogger("RAGIndex")

# ============================================================================
# RAG Index Core
# ============================================================================

_FAISS_AVAILABLE = True
try:
    import faiss
except ImportError:
    faiss = None  # type: ignore
    _FAISS_AVAILABLE = False


def is_faiss_available() -> bool:
    """æ£€æŸ¥ Faiss æ˜¯å¦å¯ç”¨"""
    return _FAISS_AVAILABLE


class DiskBasedChunks:
    """
    Lazy loading list-like object for large JSONL files.
    Uses an offset file to jump to specific lines without reading the whole file.
    """
    def __init__(self, jsonl_path: str, offset_path: Optional[str] = None):
        self.jsonl_path = jsonl_path
        if offset_path is None:
            # å°è¯•å¤šç§å¯èƒ½çš„ offset æ–‡ä»¶å‘½åæ–¹å¼
            candidates = []
            if jsonl_path.endswith(".jsonl"):
                # æ–¹å¼1: wiki_dump.offsets (å»æ‰ .jsonl ååŠ  .offsets)
                candidates.append(jsonl_path[:-6] + ".offsets")
                # æ–¹å¼2: wiki_dump.jsonl.offset (åœ¨ .jsonl ååŠ  .offset)
                candidates.append(jsonl_path + ".offset")
                # æ–¹å¼3: wiki_dump.jsonl.offsets (åœ¨ .jsonl ååŠ  .offsets)
                candidates.append(jsonl_path + ".offsets")
            else:
                candidates.append(jsonl_path + ".offsets")
                candidates.append(jsonl_path + ".offset")

            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå­˜åœ¨çš„ offset æ–‡ä»¶
            offset_path = None
            for candidate in candidates:
                if os.path.exists(candidate):
                    offset_path = candidate
                    break

            if offset_path is None:
                raise FileNotFoundError(
                    f"Offset file not found. Tried: {', '.join(candidates)}. "
                    "Please run convert_index.py first."
                )
        self.offset_path = offset_path

        if not os.path.exists(self.jsonl_path):
            raise FileNotFoundError(f"JSONL file not found: {self.jsonl_path}")

        if not os.path.exists(self.offset_path):
            raise FileNotFoundError(
                f"Offset file not found: {self.offset_path}. "
                "Please run convert_index.py first."
            )

        self._offsets = self._load_offsets()
        self._file = open(self.jsonl_path, "rb")

    def _load_offsets(self) -> List[int]:
        """
        åŠ è½½ offset æ–‡ä»¶ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. äºŒè¿›åˆ¶æ ¼å¼ï¼šæ¯ä¸ª offset æ˜¯ 8 å­—èŠ‚çš„ uint64_t (å°ç«¯åº)
        2. æ–‡æœ¬æ ¼å¼ï¼šæ¯è¡Œæ˜¯ "offset:length" æ ¼å¼ï¼Œåªå– offset éƒ¨åˆ†
        """
        with open(self.offset_path, "rb") as f:
            data = f.read()

        # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼šå¦‚æœå‰ 100 å­—èŠ‚éƒ½æ˜¯å¯æ‰“å°çš„ ASCII å­—ç¬¦ï¼Œåˆ™è®¤ä¸ºæ˜¯æ–‡æœ¬æ ¼å¼
        sample = data[:min(100, len(data))]
        is_text = all(32 <= b <= 126 or b in (9, 10, 13) for b in sample)

        if is_text:
            # æ–‡æœ¬æ ¼å¼ï¼šæ¯è¡Œæ˜¯ "offset:length" æˆ–åªæœ‰ "offset"
            offsets = []
            for line in data.decode("utf-8").strip().split("\n"):
                line = line.strip()
                if not line:
                    continue
                # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š "offset:length" æˆ– "offset"
                if ":" in line:
                    offset_str = line.split(":", 1)[0]
                else:
                    offset_str = line
                try:
                    offsets.append(int(offset_str))
                except ValueError:
                    index_logger.warning(f"è·³è¿‡æ— æ•ˆçš„ offset è¡Œ: {line}")
                    continue
            index_logger.info(f"ä»æ–‡æœ¬æ ¼å¼ offset æ–‡ä»¶åŠ è½½äº† {len(offsets):,} ä¸ªåç§»é‡")
            return offsets
        else:
            # äºŒè¿›åˆ¶æ ¼å¼ï¼šæ¯ä¸ª offset æ˜¯ 8 å­—èŠ‚çš„ uint64_t
            count = len(data) // 8
            if len(data) % 8 != 0:
                raise ValueError(
                    f"äºŒè¿›åˆ¶ offset æ–‡ä»¶å¤§å° ({len(data)} å­—èŠ‚) ä¸æ˜¯ 8 çš„å€æ•°ï¼Œ"
                    f"å¯èƒ½æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–å·²æŸå"
                )
            offsets = list(struct.unpack(f"<{count}Q", data))
            index_logger.info(f"ä»äºŒè¿›åˆ¶æ ¼å¼ offset æ–‡ä»¶åŠ è½½äº† {len(offsets):,} ä¸ªåç§»é‡")
            return offsets

    def __len__(self):
        return len(self._offsets)

    def __getitem__(self, idx):
        if isinstance(idx, slice):
            start, stop, step = idx.indices(len(self))
            return [self[i] for i in range(start, stop, step)]

        if idx < 0:
            idx += len(self)

        if idx < 0 or idx >= len(self._offsets):
            raise IndexError("DiskBasedChunks index out of range")

        offset = self._offsets[idx]
        self._file.seek(offset)
        line = self._file.readline()
        return json.loads(line.decode("utf-8"))

    def __del__(self):
        if hasattr(self, "_file") and self._file:
            self._file.close()


class DecExEncoder:
    """
    E5/BGE ç¼–ç å™¨ï¼Œæ”¯æŒ Mean Pooling + L2 å½’ä¸€åŒ–
    """
    def __init__(self, model_name: str, model_path: str, device: str = "cuda"):
        self.model_name = model_name
        self.device = device

        try:
            from transformers import AutoTokenizer, AutoModel
        except ImportError:
            raise ImportError("DecExEncoder éœ€è¦ transformers: pip install transformers")

        index_logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
        self.model.eval()
        self.model.to(self.device)
        index_logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")

    def encode(self, query_list: List[str], max_length: int = 512) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬åˆ—è¡¨ä¸ºå‘é‡
        """
        if isinstance(query_list, str):
            query_list = [query_list]

        # E5 ç‰¹æœ‰çš„ query prefix
        if "e5" in self.model_name.lower():
            query_list = [f"query: {q}" for q in query_list]

        inputs = self.tokenizer(
            query_list,
            max_length=max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            output = self.model(**inputs)
            attention_mask = inputs["attention_mask"]
            last_hidden = output.last_hidden_state.masked_fill(
                ~attention_mask[..., None].bool(), 0.0
            )
            embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)

        return embeddings.cpu().numpy().astype(np.float32)


def parse_device_config(device: str) -> Tuple[str, str]:
    """
    è§£æè®¾å¤‡é…ç½®å­—ç¬¦ä¸²
    """
    device = device.strip()

    if "/" in device:
        # æ ¼å¼: "encoder_device/index_device"
        parts = device.split("/", 1)
        encoder_dev = _normalize_device(parts[0].strip())
        index_dev = _normalize_device(parts[1].strip())
    else:
        # å•è®¾å¤‡æ ¼å¼: encoder ä½¿ç”¨è¯¥è®¾å¤‡ï¼Œindex é»˜è®¤ cpu
        encoder_dev = _normalize_device(device)
        index_dev = "cpu"

    return encoder_dev, index_dev


def _normalize_device(device: str) -> str:
    """
    æ ‡å‡†åŒ–è®¾å¤‡å­—ç¬¦ä¸²
    """
    if device == "cuda":
        return "cuda:0"
    return device


class DenseE5RAGIndex:
    """
    å¯†é›†æ£€ç´¢ç´¢å¼•ï¼šFaiss + E5 Encoder
    """
    corpus: Union[List[Dict[str, Any]], DiskBasedChunks]

    def __init__(
        self,
        index_path: str,
        model_name: str,
        corpus_path: str,
        device: str = "cuda:0",
        **kwargs
    ):
        # è§£æè®¾å¤‡é…ç½®
        self.encoder_device, self.index_device = parse_device_config(device)
        self.device_config = device  # ä¿å­˜åŸå§‹é…ç½®

        self.model_name = model_name
        self.corpus_path = corpus_path
        self.index_path = index_path

        if not _FAISS_AVAILABLE:
            raise ImportError("éœ€è¦ faiss: pip install faiss-gpu æˆ– faiss-cpu")

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ GPU ç´¢å¼•
        use_gpu_index = "cuda" in self.index_device

        # 1. åŠ è½½ Faiss ç´¢å¼•
        self.index = self._load_faiss_index(index_path, self.index_device, use_gpu_index)

        # 2. åŠ è½½ Encoder
        self.encoder = DecExEncoder(model_name, model_path=model_name, device=self.encoder_device)

        # 3. åŠ è½½è¯­æ–™åº“
        self.corpus = self._load_corpus(corpus_path)

        index_logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œç´¢å¼•å‘é‡æ•°: {self.index.ntotal:,}")
        index_logger.info(f"Encoder è®¾å¤‡: {self.encoder_device}, Index è®¾å¤‡: {self.index_device}")

    def _load_faiss_index(self, index_path: str, index_device: str, use_gpu_index: bool):
        """åŠ è½½ Faiss ç´¢å¼•"""
        index_logger.info(f"åŠ è½½ Faiss ç´¢å¼•: {index_path}")

        # å°è¯• mmap åŠ è½½
        try:
            io_flags = faiss.IO_FLAG_MMAP | faiss.IO_FLAG_READ_ONLY  # type: ignore
            index = faiss.read_index(index_path, io_flags)  # type: ignore
            index_logger.info("æˆåŠŸå¯ç”¨ Memory Mapping (mmap)")
        except Exception as e:
            index_logger.warning(f"mmap åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šåŠ è½½: {e}")
            index = faiss.read_index(index_path)  # type: ignore

        # GPU åŠ è½½ï¼ˆå¯é€‰ï¼‰
        if use_gpu_index and "cuda" in index_device:
            # ä»è®¾å¤‡å­—ç¬¦ä¸²æå– GPU IDï¼ˆå¦‚ "cuda:1" -> 1ï¼‰
            gpu_id = self._parse_gpu_id(index_device)
            index_size_gb = os.path.getsize(index_path) / (1024**3)

            # æ£€æµ‹ GPU æ˜¾å­˜å¹¶å†³å®šæ˜¯å¦åŠ è½½
            can_load, reason = self._check_gpu_memory(gpu_id, index_size_gb)

            if can_load and hasattr(faiss, "StandardGpuResources"):
                try:
                    res = faiss.StandardGpuResources()  # type: ignore
                    index = faiss.index_cpu_to_gpu(res, gpu_id, index)  # type: ignore
                    index_logger.info(f"Faiss ç´¢å¼•å·²è¿ç§»åˆ° GPU:{gpu_id}")
                except Exception as e:
                    index_logger.warning(f"GPU è¿ç§»å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ CPU: {e}")
            else:
                index_logger.info(reason)

        return index

    @staticmethod
    def _check_gpu_memory(gpu_id: int, index_size_gb: float, max_ratio: float = 0.8) -> Tuple[bool, str]:
        """
        æ£€æŸ¥ GPU æ˜¾å­˜æ˜¯å¦è¶³å¤ŸåŠ è½½ç´¢å¼•
        """
        try:
            if not torch.cuda.is_available():
                return False, "CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU"

            if gpu_id >= torch.cuda.device_count():
                return False, f"GPU:{gpu_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ CPU"

            # è·å– GPU æ˜¾å­˜ä¿¡æ¯
            props = torch.cuda.get_device_properties(gpu_id)
            total_memory_gb = props.total_memory / (1024**3)

            # è·å–å½“å‰å¯ç”¨æ˜¾å­˜
            torch.cuda.set_device(gpu_id)
            free_memory = torch.cuda.memory_reserved(gpu_id) - torch.cuda.memory_allocated(gpu_id)
            # æ›´å‡†ç¡®çš„æ–¹å¼ï¼šä½¿ç”¨ nvidia-smi è·å–çš„ç©ºé—²æ˜¾å­˜
            try:
                free_memory_gb = (torch.cuda.get_device_properties(gpu_id).total_memory -
                                  torch.cuda.memory_allocated(gpu_id)) / (1024**3)
            except Exception:
                free_memory_gb = total_memory_gb * 0.9  # å‡è®¾ 90% å¯ç”¨

            # è®¡ç®—æ˜¯å¦å¯ä»¥åŠ è½½
            max_allowed_gb = total_memory_gb * max_ratio

            if index_size_gb <= max_allowed_gb:
                index_logger.info(
                    f"GPU:{gpu_id} æ˜¾å­˜: {total_memory_gb:.1f}GB, "
                    f"ç´¢å¼•: {index_size_gb:.1f}GB ({index_size_gb/total_memory_gb*100:.0f}%)"
                )
                return True, ""
            else:
                return False, (
                    f"ç´¢å¼• ({index_size_gb:.1f}GB) è¶…è¿‡ GPU:{gpu_id} æ˜¾å­˜é™åˆ¶ "
                    f"({max_allowed_gb:.1f}GB = {total_memory_gb:.1f}GB Ã— {max_ratio:.0%})ï¼Œä½¿ç”¨ CPU"
                )

        except Exception as e:
            return False, f"GPU æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨ CPU"

    @staticmethod
    def _parse_gpu_id(device: str) -> int:
        """
        ä»è®¾å¤‡å­—ç¬¦ä¸²è§£æ GPU ID
        """
        if ":" in device:
            try:
                return int(device.split(":")[1])
            except (ValueError, IndexError):
                return 0
        return 0

    def _load_corpus(self, corpus_path: str) -> Union[List[Dict[str, Any]], DiskBasedChunks]:
        """åŠ è½½è¯­æ–™åº“"""
        index_logger.info(f"åŠ è½½è¯­æ–™åº“: {corpus_path}")

        # å°è¯•å¤šç§å¯èƒ½çš„ offset æ–‡ä»¶å‘½åæ–¹å¼
        offset_candidates = []
        if corpus_path.endswith(".jsonl"):
            # æ–¹å¼1: wiki_dump.offsets (å»æ‰ .jsonl ååŠ  .offsets)
            offset_candidates.append(corpus_path[:-6] + ".offsets")
            # æ–¹å¼2: wiki_dump.jsonl.offset (åœ¨ .jsonl ååŠ  .offset)
            offset_candidates.append(corpus_path + ".offset")
            # æ–¹å¼3: wiki_dump.jsonl.offsets (åœ¨ .jsonl ååŠ  .offsets)
            offset_candidates.append(corpus_path + ".offsets")
        else:
            offset_candidates.append(corpus_path + ".offsets")
            offset_candidates.append(corpus_path + ".offset")

        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå­˜åœ¨çš„ offset æ–‡ä»¶
        offset_path = None
        for candidate in offset_candidates:
            if os.path.exists(candidate):
                offset_path = candidate
                break

        if offset_path:
            index_logger.info(f"æ‰¾åˆ° offset æ–‡ä»¶: {offset_path}")
            index_logger.info("ä½¿ç”¨ç£ç›˜æ‡’åŠ è½½æ¨¡å¼")
            corpus = DiskBasedChunks(corpus_path, offset_path)
            index_logger.info(f"è¯­æ–™åº“: {len(corpus):,} æ¡ (æ‡’åŠ è½½)")
            return corpus
        else:
            index_logger.info(f"æœªæ‰¾åˆ° offset æ–‡ä»¶ï¼ˆå°è¯•äº†: {', '.join(offset_candidates)}ï¼‰")
            index_logger.info("å…¨é‡åŠ è½½åˆ°å†…å­˜")
            corpus: List[Dict[str, Any]] = []
            with open(corpus_path, "r", encoding="utf-8") as f:
                for line in tqdm(f, desc="åŠ è½½è¯­æ–™"):
                    if line.strip():
                        corpus.append(json.loads(line))
            index_logger.info(f"è¯­æ–™åº“: {len(corpus):,} æ¡")
            return corpus

    def _format_doc(self, doc: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å•ä¸ªæ–‡æ¡£"""
        text = doc.get("contents") or doc.get("text", "")
        title = doc.get("title", "")
        return f"[{title}]\n{text}" if title else text

    def search(self, query_embs: np.ndarray, top_k: int = 5) -> tuple:
        """
        åŸå§‹å‘é‡æ£€ç´¢ï¼ˆä¾› Batcher è°ƒç”¨ï¼‰
        """
        return self.index.search(query_embs, k=top_k)

    def encode(self, queries: List[str]) -> np.ndarray:
        """
        ç¼–ç æŸ¥è¯¢æ–‡æœ¬ï¼ˆä¾› Batcher è°ƒç”¨ï¼‰
        """
        return self.encoder.encode(queries)

    def format_results(self, indices: np.ndarray) -> List[str]:
        """
        æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼ˆä¾› Batcher è°ƒç”¨ï¼‰
        """
        all_results = []
        for query_indices in indices:
            query_results = []
            for idx in query_indices:
                if idx == -1 or idx >= len(self.corpus):
                    continue
                doc = cast(Dict[str, Any], self.corpus[idx])
                query_results.append(self._format_doc(doc))

            if not query_results:
                all_results.append("[æŸ¥è¯¢ç»“æœ] æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            else:
                all_results.append("\n---\n".join(query_results))

        return all_results

    def query(self, query: str, top_k: int = 5) -> str:
        """å•æ¡æŸ¥è¯¢"""
        try:
            query_emb = self.encoder.encode([query])
            _, idxs = self.index.search(query_emb, k=top_k)
            return self.format_results(idxs)[0]
        except Exception as e:
            return f"[Error] {str(e)}"

    def batch_query(self, queries: List[str], top_k: int = 5) -> List[str]:
        """æ‰¹é‡æŸ¥è¯¢"""
        if not queries:
            return []

        try:
            query_embs = self.encoder.encode(queries)
            _, idxs = self.index.search(query_embs, k=top_k)
            return self.format_results(idxs)
        except Exception as e:
            return [f"[Error] {str(e)}" for _ in queries]

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "index_path": self.index_path,
            "model_name": self.model_name,
            "corpus_path": self.corpus_path,
            "corpus_size": len(self.corpus),
            "index_size": self.index.ntotal,
            "device_config": self.device_config,
            "encoder_device": self.encoder_device,
            "index_device": self.index_device
        }

    def release(self):
        """
        é‡Šæ”¾ GPU èµ„æº
        """
        import gc

        index_logger.info("å¼€å§‹æ¸…ç† GPU èµ„æº...")

        # 1. é‡Šæ”¾ Encoder æ¨¡å‹
        if hasattr(self, "encoder") and self.encoder is not None:
            if hasattr(self.encoder, "model") and self.encoder.model is not None:
                # å°†æ¨¡å‹ç§»åˆ° CPU å¹¶åˆ é™¤
                try:
                    self.encoder.model.cpu()
                    del self.encoder.model
                    self.encoder.model = None  # type: ignore
                except Exception as e:
                    index_logger.warning(f"é‡Šæ”¾ encoder.model å¤±è´¥: {e}")
            try:
                del self.encoder
            except Exception:
                pass
            self.encoder = None  # type: ignore

        # 2. é‡Šæ”¾ Faiss GPU ç´¢å¼•
        if hasattr(self, "index") and self.index is not None:
            try:
                del self.index
            except Exception as e:
                index_logger.warning(f"é‡Šæ”¾ index å¤±è´¥: {e}")
            self.index = None  # type: ignore

        # 3. é‡Šæ”¾è¯­æ–™åº“
        if hasattr(self, "corpus") and self.corpus is not None:
            try:
                del self.corpus
            except Exception:
                pass
            self.corpus = []  # type: ignore

        # 4. å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # 5. æ¸…ç† CUDA ç¼“å­˜
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                # åŒæ­¥æ‰€æœ‰ CUDA æµ
                for i in range(torch.cuda.device_count()):
                    try:
                        with torch.cuda.device(i):
                            torch.cuda.synchronize()
                    except Exception:
                        pass
        except Exception as e:
            index_logger.warning(f"æ¸…ç† CUDA ç¼“å­˜å¤±è´¥: {e}")

        index_logger.info("GPU èµ„æºæ¸…ç†å®Œæˆ")


# ============================================================================
# Query Batcher - æ‰¹é‡è¯·æ±‚æ”¶é›†å™¨
# ============================================================================

@dataclass
class PendingQuery:
    """å¾…å¤„ç†çš„æŸ¥è¯¢è¯·æ±‚"""
    query: str
    top_k: int
    future: asyncio.Future
    timestamp: float = field(default_factory=time.time)


class QueryBatcher:
    """
    æŸ¥è¯¢æ‰¹å¤„ç†å™¨
    
    æ”¶é›†ä¸€æ®µæ—¶é—´çª—å£å†…çš„æŸ¥è¯¢è¯·æ±‚ï¼Œç„¶åæ‰¹é‡æ‰§è¡Œ Faiss æ£€ç´¢ã€‚
    
    å·¥ä½œåŸç†:
    1. è¯·æ±‚åˆ°è¾¾æ—¶åŠ å…¥é˜Ÿåˆ—ï¼Œè¿”å› Future
    2. åå°ä»»åŠ¡å®šæœŸæ£€æŸ¥é˜Ÿåˆ—
    3. è¾¾åˆ°è§¦å‘å¤§å°æˆ–è¶…æ—¶æ—¶ï¼Œæ‰¹é‡æ‰§è¡Œæ£€ç´¢
    4. å°†ç»“æœåˆ†å‘åˆ°å„ä¸ª Future
    
    Args:
        rag_index: RAG ç´¢å¼•å®ä¾‹
        trigger_batch_size: è§¦å‘æ‰¹å¤„ç†çš„é˜ˆå€¼ï¼ˆé˜Ÿåˆ—è¾¾åˆ°æ­¤å¤§å°æ—¶è§¦å‘ï¼‰
        max_batch_size: å•æ¬¡æ‰¹å¤„ç†çš„æœ€å¤§æ•°é‡
        max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    """
    
    def __init__(
        self,
        rag_index: "DenseE5RAGIndex",
        trigger_batch_size: int = 16,
        max_batch_size: int = 32,
        max_wait_time: float = 0.05,  # 50ms
        check_interval: float = 0.01   # 10ms
    ):
        self.rag_index = rag_index
        self.trigger_batch_size = trigger_batch_size
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.check_interval = check_interval

        self._queue: deque[PendingQuery] = deque()
        # ä¸åœ¨ __init__ ä¸­åˆ›å»º Lockï¼Œå› ä¸ºå®ƒä¼šç»‘å®šåˆ°å½“å‰äº‹ä»¶å¾ªç¯
        # åœ¨ start() ä¸­åˆ›å»ºï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„äº‹ä»¶å¾ªç¯
        self._lock: Optional[asyncio.Lock] = None
        self._task: Optional[asyncio.Task] = None
        self._running = False

        # ç»Ÿè®¡ä¿¡æ¯
        self._total_queries = 0
        self._total_batches = 0
        self._total_batch_size = 0
    
    async def start(self):
        """å¯åŠ¨åå°æ‰¹å¤„ç†ä»»åŠ¡"""
        if self._running:
            logger.warning("[Batcher] Already running, skipping start")
            return

        # å¼ºåˆ¶é‡æ–°åˆ›å»º Lockï¼Œç¡®ä¿ä½¿ç”¨å½“å‰äº‹ä»¶å¾ªç¯
        # å³ä½¿ä¹‹å‰æœ‰æ®‹ç•™çš„ Lockï¼Œä¹Ÿä¼šè¢«è¦†ç›–
        self._lock = asyncio.Lock()
        logger.info(f"[Batcher] Created lock in event loop: {id(asyncio.get_event_loop())}")

        logger.info(f"[Batcher] Starting... _running before={self._running}")
        self._running = True
        logger.info(f"[Batcher] _running set to True")

        self._task = asyncio.create_task(self._batch_worker())
        logger.info(
            f"[Batcher] å¯åŠ¨ï¼Œtrigger={self.trigger_batch_size}, "
            f"max={self.max_batch_size}, wait={self.max_wait_time}s"
        )
        logger.info(f"[Batcher] Background task created: {self._task}")
        logger.info(f"[Batcher] Event loop: {id(asyncio.get_event_loop())}")

        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿ä»»åŠ¡å¼€å§‹æ‰§è¡Œ
        await asyncio.sleep(0.001)
        logger.info(f"[Batcher] After sleep, task status: done={self._task.done()}, cancelled={self._task.cancelled()}")

        # å¦‚æœä»»åŠ¡ç«‹å³å®Œæˆï¼Œè¯´æ˜æœ‰é—®é¢˜
        if self._task.done():
            logger.error("[Batcher] Worker task completed immediately! This indicates a problem.")
            try:
                # å°è¯•è·å–å¼‚å¸¸
                exc = self._task.exception()
                logger.error(f"[Batcher] Worker task exception: {exc}")
            except Exception as e:
                logger.error(f"[Batcher] Could not get task exception: {e}")
    
    async def stop(self):
        """åœæ­¢åå°ä»»åŠ¡"""
        logger.info(f"[Batcher] Stopping... _running={self._running}, task={self._task}")
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                logger.info("[Batcher] Task cancelled successfully")
            except Exception as e:
                logger.error(f"[Batcher] Error during task cancellation: {e}")
            self._task = None

        # æ¸…ç†é˜Ÿåˆ—ä¸­å‰©ä½™çš„è¯·æ±‚
        if self._lock:
            async with self._lock:
                for pending in self._queue:
                    if not pending.future.done():
                        pending.future.set_exception(
                            RuntimeError("Batcher stopped")
                        )
                self._queue.clear()
        else:
            # å¦‚æœæ²¡æœ‰ Lockï¼Œç›´æ¥æ¸…ç†é˜Ÿåˆ—
            for pending in self._queue:
                if not pending.future.done():
                    pending.future.set_exception(
                        RuntimeError("Batcher stopped")
                    )
            self._queue.clear()

        # æ¸…ç† Lock å¼•ç”¨ï¼Œä¸‹æ¬¡ start æ—¶ä¼šé‡æ–°åˆ›å»º
        self._lock = None

        logger.info("[Batcher] å·²åœæ­¢")
    
    async def submit(self, query: str, top_k: int = 5) -> str:
        """
        æäº¤æŸ¥è¯¢è¯·æ±‚

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°

        Returns:
            æ£€ç´¢ç»“æœæ–‡æœ¬
        """
        logger.info(f"[Batcher] Submit query: '{query[:30]}...', top_k={top_k}")
        logger.info(f"[Batcher] Worker status: running={self._running}, task={self._task}, task_done={self._task.done() if self._task else 'N/A'}")

        loop = asyncio.get_event_loop()
        future: asyncio.Future[str] = loop.create_future()

        pending = PendingQuery(
            query=query,
            top_k=top_k,
            future=future
        )

        async with self._lock:
            self._queue.append(pending)
            self._total_queries += 1
            logger.info(f"[Batcher] Query added to queue (queue_size={len(self._queue)})")

        logger.info(f"[Batcher] Waiting for result...")
        result = await future
        logger.info(f"[Batcher] Result received (len={len(result)} chars)")
        return result
    
    async def _batch_worker(self):
        """åå°æ‰¹å¤„ç†å·¥ä½œå¾ªç¯"""
        logger.info(f"[Batcher] Worker started in event loop: {id(asyncio.get_event_loop())}")
        while self._running:
            try:
                await asyncio.sleep(self.check_interval)
                await self._try_process_batch()
            except asyncio.CancelledError:
                logger.info("[Batcher] Worker cancelled")
                break
            except Exception as e:
                logger.error(f"[Batcher] Worker error: {e}", exc_info=True)

        logger.warning(f"[Batcher] Worker EXITED! _running={self._running}")
        logger.warning(f"[Batcher] Worker exit stack trace:", exc_info=True)
    
    async def _try_process_batch(self):
        """å°è¯•å¤„ç†ä¸€æ‰¹è¯·æ±‚"""
        async with self._lock:
            if not self._queue:
                return

            now = time.time()
            oldest = self._queue[0].timestamp
            queue_age = now - oldest

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æ‰¹å¤„ç†
            # è§¦å‘æ¡ä»¶ï¼šé˜Ÿåˆ—è¾¾åˆ°è§¦å‘é˜ˆå€¼ æˆ– ç­‰å¾…è¶…æ—¶
            should_process = (
                len(self._queue) >= self.trigger_batch_size or
                queue_age >= self.max_wait_time
            )

            if not should_process:
                return

            logger.info(f"[Batcher] Triggering batch processing (queue_size={len(self._queue)}, age={queue_age:.3f}s)")
            # æ”¶é›†ä¸€æ‰¹è¯·æ±‚ï¼ˆæŒ‰ top_k åˆ†ç»„ï¼‰
            batch = self._collect_batch()

        if batch:
            await self._process_batch(batch)
    
    def _collect_batch(self) -> List[PendingQuery]:
        """ä»é˜Ÿåˆ—æ”¶é›†ä¸€æ‰¹è¯·æ±‚"""
        batch: List[PendingQuery] = []
        
        while self._queue and len(batch) < self.max_batch_size:
            batch.append(self._queue.popleft())
        
        return batch
    
    async def _process_batch(self, batch: List[PendingQuery]):
        """
        å¤„ç†ä¸€æ‰¹è¯·æ±‚

        ä½¿ç”¨ Faiss æ‰¹é‡æ£€ç´¢æ¥å£
        """
        if not batch:
            return

        self._total_batches += 1
        self._total_batch_size += len(batch)

        start_time = time.time()
        logger.info(f"[Batcher] Processing batch of {len(batch)} queries...")

        try:
            # 1. æ”¶é›†æ‰€æœ‰æŸ¥è¯¢æ–‡æœ¬
            queries = [p.query for p in batch]

            # æ‰¾å‡ºæœ€å¤§çš„ top_kï¼ˆæ‰¹é‡æ£€ç´¢ä½¿ç”¨ç»Ÿä¸€çš„ kï¼‰
            max_top_k = max(p.top_k for p in batch)
            logger.info(f"[Batcher] Batch search: {len(queries)} queries, max_top_k={max_top_k}")

            # 2. åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„æ‰¹é‡æ£€ç´¢
            loop = asyncio.get_event_loop()
            logger.info(f"[Batcher] Submitting to thread pool executor...")
            results = await loop.run_in_executor(
                None,
                self._sync_batch_search,
                queries,
                max_top_k
            )
            logger.info(f"[Batcher] Thread pool executor returned results")

            elapsed = time.time() - start_time
            avg_latency = elapsed / len(batch) * 1000

            logger.info(
                f"[Batcher] æ‰¹å¤„ç†å®Œæˆ: {len(batch)} æ¡, "
                f"è€—æ—¶ {elapsed*1000:.1f}ms, å¹³å‡ {avg_latency:.1f}ms/æ¡"
            )

            # 3. åˆ†å‘ç»“æœï¼ˆæ ¹æ®å„è‡ªçš„ top_k æˆªæ–­ï¼‰
            for pending, result in zip(batch, results):
                if not pending.future.done():
                    # å¦‚æœè¯·æ±‚çš„ top_k å°äº max_top_kï¼Œéœ€è¦æˆªæ–­ç»“æœ
                    if pending.top_k < max_top_k:
                        truncated = self._truncate_result(result, pending.top_k)
                        pending.future.set_result(truncated)
                    else:
                        pending.future.set_result(result)

        except Exception as e:
            logger.error(f"[Batcher] Batch processing error: {e}")
            # æ‰€æœ‰è¯·æ±‚è¿”å›é”™è¯¯
            for pending in batch:
                if not pending.future.done():
                    pending.future.set_exception(e)
    
    def _sync_batch_search(self, queries: List[str], top_k: int) -> List[str]:
        """
        åŒæ­¥æ‰¹é‡æ£€ç´¢ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
        """
        logger.info(f"[Batcher] _sync_batch_search START: {len(queries)} queries, top_k={top_k}")
        result = self.rag_index.batch_query(queries, top_k=top_k)
        logger.info(f"[Batcher] _sync_batch_search DONE: returned {len(result)} results")
        return result
    
    @staticmethod
    def _truncate_result(result: str, top_k: int) -> str:
        """
        æˆªæ–­ç»“æœåˆ°æŒ‡å®šçš„ top_k æ•°é‡
        
        ç»“æœæ ¼å¼: å¤šä¸ªæ–‡æ¡£ç”¨ "---" åˆ†éš”
        """
        if not result or top_k <= 0:
            return result
        
        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²æ–‡æ¡£
        docs = result.split("\n---\n")
        
        if len(docs) <= top_k:
            return result
        
        # æˆªæ–­å¹¶é‡æ–°æ‹¼æ¥
        return "\n---\n".join(docs[:top_k])
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        avg_batch_size = (
            self._total_batch_size / self._total_batches 
            if self._total_batches > 0 else 0
        )
        return {
            "total_queries": self._total_queries,
            "total_batches": self._total_batches,
            "avg_batch_size": round(avg_batch_size, 2),
            "queue_size": len(self._queue),
            "running": self._running,
            "trigger_batch_size": self.trigger_batch_size,
            "max_batch_size": self.max_batch_size,
            "max_wait_time": self.max_wait_time
        }


# ============================================================================
# RAG Backend
# ============================================================================

class RAGBackend(Backend):
    """
    RAG æ£€ç´¢åç«¯
    
    ç‰¹æ€§:
    - ä½¿ç”¨ QueryBatcher æ‰¹é‡å¤„ç†è¯·æ±‚
    - Faiss æ‰¹é‡æ£€ç´¢æé«˜ååé‡
    - æ”¯æŒ mmap åŠ è½½å¤§ç´¢å¼•
    - ç£ç›˜æ‡’åŠ è½½è¯­æ–™åº“
    
    å·¥å…·åˆ—è¡¨:
    - rag:search - æ£€ç´¢æ–‡æ¡£
    - rag:stats - è·å–ç»Ÿè®¡ä¿¡æ¯
    
    é…ç½®é¡¹:
    - model_name: E5 æ¨¡å‹åç§°
    - index_path: Faiss ç´¢å¼•è·¯å¾„
    - corpus_path: è¯­æ–™åº“ JSONL è·¯å¾„
    - device: è®¾å¤‡é…ç½®ï¼Œæ”¯æŒæ ¼å¼ï¼š
        - "cuda:0"        â†’ Encoder ç”¨ cuda:0, Index ç”¨ cpu
        - "cuda:0/cuda:1" â†’ Encoder ç”¨ cuda:0, Index ç”¨ cuda:1
        - "cpu"           â†’ å…¨éƒ¨ç”¨ cpu
    - default_top_k: é»˜è®¤è¿”å›ç»“æœæ•°
    - batcher_trigger_batch_size: è§¦å‘æ‰¹å¤„ç†çš„é˜ˆå€¼
    - batcher_max_batch_size: å•æ¬¡æ‰¹å¤„ç†çš„æœ€å¤§æ•°é‡
    - batcher_max_wait_time: æ‰¹å¤„ç†æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    """
    
    name = "rag"
    description = "RAG Retrieval Backend - æ–‡æ¡£æ£€ç´¢æœåŠ¡"
    version = "3.0.0"
    
    def __init__(self, config: Optional[BackendConfig] = None):
        if config is None:
            raise ValueError(
                "RAG Backend éœ€è¦é…ç½®ä¿¡æ¯ã€‚è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æä¾›ä»¥ä¸‹å¿…éœ€å‚æ•°:\n"
                "  - index_path: Faiss ç´¢å¼•æ–‡ä»¶è·¯å¾„\n"
                "  - corpus_path: è¯­æ–™åº“ JSONL æ–‡ä»¶è·¯å¾„\n"
                "  - model_name: E5 æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ intfloat/e5-base-v2ï¼‰\n"
                "  - device: è®¾å¤‡é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ cuda:0ï¼‰"
            )
        super().__init__(config)
        
        self._rag_index: Optional["DenseE5RAGIndex"] = None
        self._batcher: Optional[QueryBatcher] = None
        self._initialized = False
    
    async def warmup(self):
        """
        é¢„çƒ­ï¼šåŠ è½½æ¨¡å‹å’Œç´¢å¼•ï¼Œå¯åŠ¨ Batcher
        """
        if not is_faiss_available():
            raise ImportError("RAG Backend éœ€è¦ faiss: pip install faiss-gpu")
        
        cfg = self.config.default_config
        
        logger.info("=" * 60)
        logger.info("ğŸ”¥ RAG Backend é¢„çƒ­å¼€å§‹")
        logger.info("=" * 60)
        
        # 1. è·å–é…ç½®ï¼ˆå¿…éœ€å‚æ•°ï¼‰
        index_path = cfg.get("index_path")
        if not index_path:
            raise ValueError(
                "é…ç½®ç¼ºå¤±: 'index_path' æ˜¯å¿…éœ€å‚æ•°ã€‚\n"
                "è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® RAG ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚:\n"
                '  "index_path": "/path/to/faiss.index"\n'
                "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: RAG_INDEX_PATH"
            )

        corpus_path = cfg.get("corpus_path")
        if not corpus_path:
            raise ValueError(
                "é…ç½®ç¼ºå¤±: 'corpus_path' æ˜¯å¿…éœ€å‚æ•°ã€‚\n"
                "è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®è¯­æ–™åº“æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚:\n"
                '  "corpus_path": "/path/to/corpus.jsonl"\n'
                "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: RAG_CORPUS_PATH"
            )

        # å¯é€‰å‚æ•°ï¼ˆæœ‰é»˜è®¤å€¼ï¼‰
        model_name = cfg.get("model_name", "intfloat/e5-base-v2")
        device = cfg.get("device", "cuda:0")
        
        logger.info(f"   è®¾å¤‡é…ç½®: {device}")
        
        # 2. åŠ è½½ RAG ç´¢å¼•ï¼ˆåŒæ­¥æ“ä½œï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
        loop = asyncio.get_event_loop()
        self._rag_index = await loop.run_in_executor(
            None,
            lambda: DenseE5RAGIndex(
                index_path=index_path,
                model_name=model_name,
                corpus_path=corpus_path,
                device=device
            )
        )
        
        # ç¡®ä¿ç´¢å¼•åŠ è½½æˆåŠŸ
        assert self._rag_index is not None, "RAG Index åŠ è½½å¤±è´¥"
        
        # 3. åˆ›å»ºå¹¶å¯åŠ¨ Batcher
        trigger_size = cfg.get("batcher_trigger_batch_size", 16)
        max_size = cfg.get("batcher_max_batch_size", 32)
        max_wait = cfg.get("batcher_max_wait_time", 0.05)
        
        self._batcher = QueryBatcher(
            rag_index=self._rag_index,
            trigger_batch_size=trigger_size,
            max_batch_size=max_size,
            max_wait_time=max_wait
        )
        await self._batcher.start()
        
        self._initialized = True
        
        # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„è®¾å¤‡
        actual_encoder_device = self._rag_index.encoder_device
        actual_index_device = self._rag_index.index_device
        
        logger.info("=" * 60)
        logger.info("âœ… RAG Backend é¢„çƒ­å®Œæˆ")
        logger.info(f"   - æ¨¡å‹: {model_name}")
        logger.info(f"   - Encoder è®¾å¤‡: {actual_encoder_device}")
        logger.info(f"   - Index è®¾å¤‡: {actual_index_device}")
        logger.info(f"   - ç´¢å¼•: {self._rag_index.index.ntotal:,} å‘é‡")
        logger.info(f"   - è¯­æ–™: {len(self._rag_index.corpus):,} æ–‡æ¡£")
        logger.info(f"   - Batcher: trigger={trigger_size}, max={max_size}")
        logger.info("=" * 60)
    
    async def shutdown(self):
        """å…³é—­ï¼šåœæ­¢ Batcherï¼Œé‡Šæ”¾èµ„æº"""
        logger.info("ğŸ›‘ RAG Backend å…³é—­ä¸­...")
        
        if self._batcher:
            await self._batcher.stop()
            self._batcher = None
        
        if self._rag_index:
            # æ˜¾å¼é‡Šæ”¾ GPU èµ„æº
            try:
                self._rag_index.release()
            except Exception as e:
                logger.warning(f"RAG Index release warning: {e}")
            self._rag_index = None
        
        # é¢å¤–çš„åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # æ¸…ç† CUDA ç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass
        
        self._initialized = False
        logger.info("âœ… RAG Backend å·²å…³é—­")
    
    # ========================================================================
    # å·¥å…·æ–¹æ³•
    # ========================================================================
    
    @tool("rag:search")
    async def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        session_id: Optional[str] = None,
        trace_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ£€ç´¢æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            session_id: ä¼šè¯ ID (å¯é€‰)
            trace_id: è¿½è¸ª ID (å¯é€‰)

        Returns:
            æ ‡å‡†åŒ–å“åº”: {code, message, data, meta}
        """
        with ResponseTimer() as timer:
            try:
                if not self._initialized or not self._batcher:
                    return build_error_response(
                        code=ErrorCode.BACKEND_NOT_INITIALIZED,
                        message="RAG Backend æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ warmup()",
                        tool="rag:search",
                        data={
                            "query": query,
                            "details": "backend not initialized or batcher unavailable"
                        },
                        execution_time_ms=timer.get_elapsed_ms(),
                        resource_type="rag",
                        session_id=session_id,
                        trace_id=trace_id
                    )

                actual_top_k = top_k or self.config.default_config.get("default_top_k", 5)

                logger.info(f"ğŸ” [rag:search] START: query='{query[:50]}...', top_k={actual_top_k}")

                try:
                    # é€šè¿‡ Batcher æäº¤è¯·æ±‚
                    logger.info(f"   â†³ Submitting to batcher...")
                    context = await self._batcher.submit(query, top_k=actual_top_k)
                    logger.info(f"   â†³ Batcher returned result (len={len(context)} chars)")

                    return build_success_response(
                        data={
                            "query": query,
                            "context": context
                        },
                        tool="rag:search",
                        execution_time_ms=timer.get_elapsed_ms(),
                        resource_type="rag",
                        session_id=session_id,
                        trace_id=trace_id
                    )
                except Exception as e:
                    logger.error(f"[rag:search] é”™è¯¯: {e}")
                    return build_error_response(
                        code=ErrorCode.UNEXPECTED_ERROR,
                        message=str(e),
                        tool="rag:search",
                        data={
                            "query": query,
                            "details": str(e)
                        },
                        execution_time_ms=timer.get_elapsed_ms(),
                        resource_type="rag",
                        session_id=session_id,
                        trace_id=trace_id
                    )
            except Exception as e:
                return build_error_response(
                    code=ErrorCode.UNEXPECTED_ERROR,
                    message=f"[RAG] Error: {str(e)}",
                    tool="rag:search",
                    data={"details": str(e)},
                    execution_time_ms=timer.get_elapsed_ms(),
                    resource_type="rag",
                    session_id=session_id,
                    trace_id=trace_id
                )
    @tool("rag:batch_search")
    async def batch_search(
        self,
        queries: List[str],
        top_k: Optional[int] = None,
        session_id: Optional[str] = None,
        trace_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡æ£€ç´¢æ–‡æ¡£

        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°
            session_id: ä¼šè¯ ID (å¯é€‰)
            trace_id: è¿½è¸ª ID (å¯é€‰)

        Returns:
            æ ‡å‡†åŒ–å“åº”: {code, message, data, meta}
        """
        with ResponseTimer() as timer:
            try:
                if not self._initialized or not self._batcher:
                    return build_error_response(
                        code=ErrorCode.BACKEND_NOT_INITIALIZED,
                        message="RAG Backend æœªåˆå§‹åŒ–",
                        tool="rag:batch_search",
                        data={
                            "queries": queries,
                            "details": "backend not initialized or batcher unavailable"
                        },
                        execution_time_ms=timer.get_elapsed_ms(),
                        resource_type="rag",
                        session_id=session_id,
                        trace_id=trace_id
                    )

                actual_top_k = top_k or self.config.default_config.get("default_top_k", 5)

                logger.debug(f"ğŸ” [rag:batch_search] {len(queries)} æ¡æŸ¥è¯¢")

                try:
                    # å¹¶å‘æäº¤æ‰€æœ‰è¯·æ±‚
                    tasks = [
                        self._batcher.submit(q, top_k=actual_top_k)
                        for q in queries
                    ]
                    results = await asyncio.gather(*tasks, return_exceptions=True)

                    # å¤„ç†ç»“æœ
                    contexts = []
                    errors = []
                    for i, result in enumerate(results):
                        if isinstance(result, Exception):
                            contexts.append(f"[Error] {str(result)}")
                            errors.append({"index": i, "error": str(result)})
                        else:
                            contexts.append(result)

                    if len(errors) == 0:
                        # All successful
                        return build_success_response(
                            data={
                                "count": len(queries),
                                "contexts": contexts
                            },
                            tool="rag:batch_search",
                            execution_time_ms=timer.get_elapsed_ms(),
                            resource_type="rag",
                            session_id=session_id,
                            trace_id=trace_id
                        )
                    elif len(errors) == len(queries):
                        # All failed
                        return build_error_response(
                            code=ErrorCode.ALL_REQUESTS_FAILED,
                            message="All queries failed",
                            tool="rag:batch_search",
                            data={
                                "count": len(queries),
                                "contexts": contexts,
                                "errors": errors,
                                "details": "all batch queries failed"
                            },
                            execution_time_ms=timer.get_elapsed_ms(),
                            resource_type="rag",
                            session_id=session_id,
                            trace_id=trace_id
                        )
                    else:
                        # Partial failure
                        return build_error_response(
                            code=ErrorCode.PARTIAL_FAILURE,
                            message=f"{len(errors)} out of {len(queries)} queries failed",
                            tool="rag:batch_search",
                            data={
                                "count": len(queries),
                                "contexts": contexts,
                                "errors": errors,
                                "details": "partial batch query failure"
                            },
                            execution_time_ms=timer.get_elapsed_ms(),
                            resource_type="rag",
                            session_id=session_id,
                            trace_id=trace_id
                        )
                except Exception as e:
                    logger.error(f"[rag:batch_search] é”™è¯¯: {e}")
                    return build_error_response(
                        code=ErrorCode.UNEXPECTED_ERROR,
                        message=str(e),
                        tool="rag:batch_search",
                        data={
                            "queries": queries,
                            "details": str(e)
                        },
                        execution_time_ms=timer.get_elapsed_ms(),
                        resource_type="rag",
                        session_id=session_id,
                        trace_id=trace_id
                    )
            except Exception as e:
                return build_error_response(
                    code=ErrorCode.UNEXPECTED_ERROR,
                    message=f"[RAG] Error: {str(e)}",
                    tool="rag:batch_search",
                    data={"details": str(e)},
                    execution_time_ms=timer.get_elapsed_ms(),
                    resource_type="rag",
                    session_id=session_id,
                    trace_id=trace_id
                )
    @tool("rag:stats")
    async def get_stats(
        self,
        session_id: Optional[str] = None,
        trace_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯

        Args:
            session_id: ä¼šè¯ ID (å¯é€‰)
            trace_id: è¿½è¸ª ID (å¯é€‰)

        Returns:
            æ ‡å‡†åŒ–å“åº”: {code, message, data, meta}
        """
        with ResponseTimer() as timer:
            try:
                stats = {
                    "initialized": self._initialized,
                    "backend_version": self.version
                }

                if self._rag_index:
                    stats["index"] = self._rag_index.get_stats()

                if self._batcher:
                    stats["batcher"] = self._batcher.get_stats()

                return build_success_response(
                    data=stats,
                    tool="rag:stats",
                    execution_time_ms=timer.get_elapsed_ms(),
                    resource_type="rag",
                    session_id=session_id,
                    trace_id=trace_id
                )

            except Exception as e:
                return build_error_response(
                    code=ErrorCode.UNEXPECTED_ERROR,
                    message=f"[RAG] Error: {str(e)}",
                    tool="rag:stats",
                    data={"details": str(e)},
                    execution_time_ms=timer.get_elapsed_ms(),
                    resource_type="rag",
                    session_id=session_id,
                    trace_id=trace_id
                )

# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def create_rag_backend(
    model_name: str = "intfloat/e5-base-v2",
    index_path: str = "./data/index/faiss.index",
    corpus_path: str = "./data/corpus.jsonl",
    device: str = "cuda:0",
    default_top_k: int = 5,
    batcher_trigger_batch_size: int = 16,
    batcher_max_batch_size: int = 32,
    batcher_max_wait_time: float = 0.05,
    **kwargs
) -> RAGBackend:
    """
    åˆ›å»º RAG åç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_name: E5 æ¨¡å‹åç§°
        index_path: Faiss ç´¢å¼•è·¯å¾„
        corpus_path: è¯­æ–™åº“è·¯å¾„
        device: è®¾å¤‡é…ç½®ï¼Œæ”¯æŒæ ¼å¼ï¼š
        - "cuda:0"        â†’ Encoder ç”¨ cuda:0, Index ç”¨ cpu
        - "cuda:0/cuda:1" â†’ Encoder ç”¨ cuda:0, Index ç”¨ cuda:1
        - "cpu"           â†’ å…¨éƒ¨ç”¨ cpu
        default_top_k: é»˜è®¤è¿”å›ç»“æœæ•°
        batcher_trigger_batch_size: è§¦å‘æ‰¹å¤„ç†çš„é˜ˆå€¼
        batcher_max_batch_size: å•æ¬¡æ‰¹å¤„ç†çš„æœ€å¤§æ•°é‡
        batcher_max_wait_time: æ‰¹å¤„ç†æœ€å¤§ç­‰å¾…æ—¶é—´
        
    Returns:
        RAGBackend å®ä¾‹
    """
    config = BackendConfig(
        enabled=True,
        default_config={
        "model_name": model_name,
        "index_path": index_path,
        "corpus_path": corpus_path,
        "device": device,
        "default_top_k": default_top_k,
        "batcher_trigger_batch_size": batcher_trigger_batch_size,
        "batcher_max_batch_size": batcher_max_batch_size,
        "batcher_max_wait_time": batcher_max_wait_time,
        **kwargs
        }
    )
    return RAGBackend(config)
