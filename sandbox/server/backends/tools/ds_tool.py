# sandbox/server/backends/tools/ds_tool.py
"""
Data Science API å·¥å…·

æä¾› read_csv, run_python, inspect_data ç­‰å·¥å…·ï¼Œç”¨äºæ•°æ®ç§‘å­¦ä»»åŠ¡ã€‚
ç§»æ¤è‡ª DS_synthesis/src/tools/ds_tools.py
"""

import io
import os
import sys
import traceback
import contextlib
import json
import time
import math
import re
import random
import datetime
import collections
import itertools
import functools
import warnings
import base64
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# --- ç¬¬ä¸‰æ–¹åº“ ---
try:
    import pandas as pd
    import numpy as np
    import scipy
    import statsmodels.api as sm
    import sklearn
    import patsy
    # --- ç»˜å›¾åº“è®¾ç½® ---
    import matplotlib
    matplotlib.use('Agg') 
    import matplotlib.pyplot as plt
    import seaborn as sns
    DS_LIBS_AVAILABLE = True
except ImportError:
    DS_LIBS_AVAILABLE = False
    # å®šä¹‰ä¸€äº›å ä½ç¬¦ä»¥é˜²æ­¢ IDE æŠ¥é”™ï¼Œè¿è¡Œæ—¶ä¼šæ£€æŸ¥ DS_LIBS_AVAILABLE
    pd = None
    np = None
    plt = None
    sns = None


from . import register_api_tool
from ..error_codes import ErrorCode
from .base_tool import BaseApiTool, ToolBusinessError


# ==========================================
# æ ¸å¿ƒå·¥å…·å‡½æ•° (ä» DS_synthesis ç§»æ¤)
# ==========================================

def _resolve_csv_path(base_dir: str, csv_file: str) -> Path:
    """
    å°† csv æ–‡ä»¶åè§£æä¸º base_dir ä¸‹çš„å®‰å…¨è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„é€ƒé€¸ã€‚
    """
    # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œä¸”åœ¨ base_dir å†…ï¼Œåˆ™ç›´æ¥è¿”å›
    if os.path.isabs(csv_file):
        candidate = Path(csv_file).resolve()
    else:
        root = Path(base_dir).expanduser().resolve()
        candidate = (root / csv_file).expanduser().resolve()
        
    root = Path(base_dir).expanduser().resolve()
    if not str(candidate).startswith(str(root)):
        raise ValueError(f"Security Error: File path '{csv_file}' escapes the seed directory.")
    
    return candidate


def summarize_csvs_in_folder(folder_path: str, max_csv: int = 100) -> str:
    """
    Scans a folder for all CSV files and returns a natural language summary for each.
    """
    if not DS_LIBS_AVAILABLE:
        return "Error: Data Science libraries (pandas, etc.) are not installed."

    # 1. Validate Folder Path
    if not os.path.exists(folder_path):
        return f"Error: The directory '{folder_path}' does not exist."
    
    # 2. Get list of CSV files
    all_files = os.listdir(folder_path)
    csv_files = [f for f in all_files if f.lower().endswith('.csv')]
    csv_files.sort()
    
    if not csv_files:
        return f"No CSV files found in directory: {folder_path}"

    results = []

    # 3. Loop through each CSV file
    for file_name in csv_files[:max_csv]:
        file_path = os.path.join(folder_path, file_name)
        
        try:
            df = pd.read_csv(file_path)
            num_rows, num_cols = df.shape
            columns = df.columns.tolist()
            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
            cat_cols = df.select_dtypes(exclude=['number']).columns.tolist()
            missing_count = df.isnull().sum().sum()
            duplicate_count = df.duplicated().sum()
            
            desc = f"This dataset contains {num_rows} rows and {num_cols} columns. "
            if len(columns) > 5:
                col_str = ", ".join(columns[:5]) + f", and {len(columns)-5} others"
            else:
                col_str = ", ".join(columns)
            desc += f"Key attributes include: [{col_str}]. "
            desc += f"It has {len(numeric_cols)} numeric fields "
            if numeric_cols: desc += f"(e.g., {numeric_cols[0]}) "
            desc += f"and {len(cat_cols)} categorical fields. "
            
            if missing_count == 0 and duplicate_count == 0:
                desc += "Data quality is high (no missing values or duplicates)."
            else:
                issues = []
                if missing_count > 0: issues.append(f"{missing_count} missing values")
                if duplicate_count > 0: issues.append(f"{duplicate_count} duplicates")
                desc += f"Data requires cleaning: found {', '.join(issues)}."

            results.append(f"{file_name}:\n{desc}")
        except Exception as e:
            results.append(f"{file_name}:\nError processing file - {str(e)}")

    summary = "\n\n".join(results)
    if len(csv_files) > max_csv:
        summary += f"\n\n... truncated: showing first {max_csv} of {len(csv_files)} CSV files ..."
    return summary


def read_csv_impl(
    csv_file: str,
    base_dir: str,
    max_rows: int = 50,
) -> Dict[str, Any]:
    """
    è¯»å– seed ç›®å½•ä¸­çš„ CSV æ–‡ä»¶å¹¶è¿”å›æ–‡æœ¬å†…å®¹ã€‚
    """
    if not DS_LIBS_AVAILABLE:
        return {"text": "Error: Data Science libraries (pandas, etc.) are not installed.", "images": []}

    if not base_dir:
        return {"text": "base_dir ä¸ºç©ºï¼Œç¯å¢ƒæœªæ­£ç¡®æ³¨å…¥ CSV æ ¹ç›®å½•ã€‚", "images": []}

    try:
        path = _resolve_csv_path(base_dir, csv_file)
        if not path.exists():
            return {"text": f"æ–‡ä»¶ä¸å­˜åœ¨: {path}", "images": []}
        if not path.is_file():
            return {"text": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {path}", "images": []}

        df = pd.read_csv(path)
    except Exception as exc:
        return {"text": f"è¯»å–å¤±è´¥: {exc}", "images": []}

    preview = df.head(max_rows)
    body = preview.to_csv(index=False)
    note = ""
    if len(df) > max_rows:
        note = f"\n... å·²æˆªæ–­ï¼Œä»…å±•ç¤ºå‰ {max_rows} è¡Œï¼Œå…± {len(df)} è¡Œ ..."

    return {"text": f"{path.name}\n{body}{note}", "images": []}


# ==========================================
# ç™½åå•å’Œ Import æ§åˆ¶
# ==========================================
ALLOWED_MODULES = {
    "pandas", "numpy", "scipy", "statsmodels", "patsy",
    "sklearn", "joblib", "xgboost", "lightgbm", "imblearn",
    "sys", "os", "io", "pathlib", "json", "csv", "pickle", "glob", "shutil",
    "datetime", "time", "dateutil", "calendar", "pytz",
    "collections", "itertools", "functools", "operator", "heapq", "bisect", "copy", "enum",
    "re", "string", "textwrap", "difflib", "unicodedata",
    "math", "random", "statistics", "decimal", "fractions",
    "warnings", "logging", "pprint", "uuid", "hashlib", "typing", "dataclasses",
    "matplotlib", "seaborn", "plotly", "altair",
    "urllib", "html", "xml" 
}

def _safe_import(name, globals=None, locals=None, fromlist=(), level=0):
    root_module = name.split(".")[0]
    if root_module not in ALLOWED_MODULES:
        raise ImportError(f"Security: Import of module '{root_module}' is blocked.")
    return __import__(name, globals, locals, fromlist, level)

@contextlib.contextmanager
def _patch_pandas_io(base_dir: str):
    if not DS_LIBS_AVAILABLE:
        yield
        return

    original_read_csv = pd.read_csv
    
    def patched_read_csv(filepath_or_buffer, *args, **kwargs):
        if isinstance(filepath_or_buffer, str):
            if not filepath_or_buffer.startswith(('http:', 'https:', 'ftp:', 's3:')):
                try:
                    resolved_path = _resolve_csv_path(base_dir, filepath_or_buffer)
                    filepath_or_buffer = str(resolved_path)
                except Exception:
                    pass
        return original_read_csv(filepath_or_buffer, *args, **kwargs)

    pd.read_csv = patched_read_csv
    try:
        yield
    finally:
        pd.read_csv = original_read_csv


def run_python_impl(
    code: str,
    base_dir: str,
    return_vars: Optional[List[str]] = None,
) -> Dict[str, Any]:
    
    if not DS_LIBS_AVAILABLE:
        return {"text": "Error: Data Science libraries (pandas, etc.) are not installed.", "images": []}

    base_dir = base_dir or os.getcwd()

    # --- åŠ©æ‰‹å‡½æ•° ---
    def load_csv(name: str, **kwargs):
        path = _resolve_csv_path(base_dir, name)
        return pd.read_csv(path, **kwargs)

    def save_file(data: Any, name: str, **kwargs):
        path = Path(base_dir) / name
        if not str(path.resolve()).startswith(str(Path(base_dir).resolve())):
             raise ValueError("Security Error: Cannot save file outside base_dir.")
        
        if isinstance(data, pd.DataFrame):
            data.to_csv(path, **kwargs)
        elif hasattr(data, 'savefig'): 
            data.savefig(path, **kwargs)
        else:
            import joblib
            joblib.dump(data, path, **kwargs)
        return f"File saved to: {name}"

    # --- Safe Builtins ---
    safe_builtins = {
        "__import__": _safe_import,
        "abs": abs, "min": min, "max": max, "sum": sum, "round": round, "pow": pow, "divmod": divmod,
        "all": all, "any": any,
        "len": len, "range": range, "enumerate": enumerate, "zip": zip, 
        "map": map, "filter": filter, "iter": iter, "next": next,
        "slice": slice, "reversed": reversed, "sorted": sorted,
        "list": list, "dict": dict, "set": set, "tuple": tuple, "frozenset": frozenset,
        "str": str, "int": int, "float": float, "bool": bool, "complex": complex, "bytes": bytes,
        "type": type, "isinstance": isinstance, "issubclass": issubclass, "callable": callable,
        "hash": hash, "id": id, "object": object,
        "getattr": getattr, "setattr": setattr, "hasattr": hasattr, "delattr": delattr,
        "vars": vars, "dir": dir,
        "print": print, "repr": repr, "ascii": ascii, "format": format,
        "chr": chr, "ord": ord, "bin": bin, "hex": hex, "oct": oct,
        "Exception": Exception, "ValueError": ValueError, "TypeError": TypeError, "KeyError": KeyError, 
        "IndexError": IndexError, "NameError": NameError, "AttributeError": AttributeError, 
        "ImportError": ImportError, "RuntimeError": RuntimeError, "ZeroDivisionError": ZeroDivisionError
    }

    globals_ns = {
        "__builtins__": safe_builtins,
        "pd": pd,
        "np": np,
        "scipy": scipy,
        "sm": sm,
        "sklearn": sklearn,
        "sys": sys,
        "os": os,
        "json": json,
        "time": time,
        "datetime": datetime,
        "re": re,
        "math": math,
        "random": random,
        "collections": collections,
        "itertools": itertools,
        "warnings": warnings,
        "plt": plt,
        "sns": sns,
        "Path": Path,
        "load_csv": load_csv,
        "save_file": save_file,
    }
    locals_ns = {}

    plt.close('all') 
    warnings.filterwarnings("ignore")

    stdout_capture = io.StringIO()
    error_message = None
    images = []

    try:
        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stdout_capture):
            # ä½¿ç”¨ patch è‡ªåŠ¨ä¿®æ­£ pd.read_csv çš„ç›¸å¯¹è·¯å¾„
            with _patch_pandas_io(base_dir):
                exec(code, globals_ns, locals_ns)
        
        # å›¾ç‰‡æ•è·
        if plt.get_fignums():
            fig = plt.gcf()
            img_buf = io.BytesIO()
            fig.savefig(img_buf, format='png', bbox_inches='tight')
            img_buf.seek(0)
            img_b64 = base64.b64encode(img_buf.read()).decode('utf-8')
            images.append({
                "type": "image/png",
                "base64": img_b64,
                "description": "Generated Plot"
            })
            
    except Exception:
        error_message = traceback.format_exc()

    finally:
        plt.close('all')

    stdout = stdout_capture.getvalue().strip()
    MAX_CHARS = 5000
    if len(stdout) > MAX_CHARS:
        stdout = stdout[:MAX_CHARS] + f"\n\n[System Warning]: Output truncated! (Total {len(stdout)} chars)."

    parts = []
    
    if error_message:
        parts.append(f"âŒ Execution Error:\n{error_message}")
    
    if stdout:
        parts.append(f"ğŸ“„ Standard Output:\n{stdout}")

    if return_vars and not error_message:
        captures = []
        for name in return_vars:
            if name in locals_ns:
                val = locals_ns[name]
                if isinstance(val, pd.DataFrame):
                    preview = val.head(5).to_markdown(index=False)
                    captures.append(f"{name} (shape={val.shape}):\n{preview}")
                else:
                    captures.append(f"{name} = {str(val)}")
        if captures:
            parts.append("ğŸ“¦ Variables Inspection:\n" + "\n".join(captures))
            
    if not parts and not images:
        parts.append("âœ… Execution successful (No output printed).")

    return {
        "text": "\n\n".join(parts),
        "images": images 
    }


# ==========================================
# å·¥å…·ç±»å®šä¹‰
# ==========================================

class InspectDataTool(BaseApiTool):
    """Inspect Data Tool"""
    def __init__(self):
        super().__init__(tool_name="ds:inspect_data", resource_type="ds")

    async def execute(self, **kwargs) -> Any:
        seed_path = kwargs.get("seed_path")
        if not seed_path:
             # å¦‚æœ kwargs ä¸­æ²¡æœ‰ seed_pathï¼Œå°è¯•ä» config ä¸­è·å–ï¼ˆè™½ç„¶å¯¹äº ds æ¥è¯´ï¼Œseed_path é€šå¸¸æ˜¯åŠ¨æ€çš„ï¼‰
             # è¿™é‡Œæˆ‘ä»¬æŠ›å‡ºé”™è¯¯ï¼Œè¦æ±‚å¿…é¡»æä¾› seed_path
             raise ToolBusinessError("seed_path must be provided in kwargs", ErrorCode.EXECUTION_ERROR)

        max_csv = self.get_config("ds_max_csv")
        if max_csv is None:
            max_csv = 100
        max_csv = int(max_csv)

        max_chars = self.get_config("ds_summary_max_chars")
        if max_chars is None:
            max_chars = 12000
        max_chars = int(max_chars)

        summary = summarize_csvs_in_folder(seed_path, max_csv=max_csv)
        if len(summary) > max_chars:
            summary = summary[:max_chars] + "\n\n... [Summary truncated to comply with maximum length limit] ..."
        return {"result": summary}

class ReadCsvTool(BaseApiTool):
    """Read CSV Tool"""
    def __init__(self):
        super().__init__(tool_name="ds:read_csv", resource_type="ds")

    async def execute(self, csv_file: str, max_rows: int = 50, **kwargs) -> Any:
        seed_path = kwargs.get("seed_path")
        if not seed_path:
             raise ToolBusinessError("seed_path must be provided in kwargs", ErrorCode.EXECUTION_ERROR)
        
        result = read_csv_impl(csv_file, seed_path, max_rows)
        return {"result": result["text"]} # ç›®å‰åªè¿”å›æ–‡æœ¬ï¼Œå¿½ç•¥ images (read_csv ä¹Ÿä¸äº§ç”Ÿå›¾ç‰‡)

class RunPythonTool(BaseApiTool):
    """Run Python Tool"""
    def __init__(self):
        super().__init__(tool_name="ds:run_python", resource_type="ds")

    async def execute(self, code: str, return_vars: Optional[List[str]] = None, **kwargs) -> Any:
        seed_path = kwargs.get("seed_path")
        if not seed_path:
             raise ToolBusinessError("seed_path must be provided in kwargs", ErrorCode.EXECUTION_ERROR)
        
        result = run_python_impl(code, seed_path, return_vars)
        
        # æ ¼å¼åŒ–è¾“å‡ºï¼ŒåŒ…å«å›¾ç‰‡
        output = result["text"]
        if result["images"]:
            output += "\n\n[Generated Images]:\n"
            # è¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥åœ¨æ–‡æœ¬ä¸­æ˜¾ç¤ºå›¾ç‰‡ï¼Œä½†åœ¨ sandbox åè®®ä¸­ï¼Œé€šå¸¸ä¼šæŠŠ images å•ç‹¬è¿”å›
            # ç›®å‰ BaseApiTool çš„ execute è¿”å› Anyï¼Œé€šå¸¸æ˜¯ dictã€‚
            # æˆ‘ä»¬å¯ä»¥è¿”å› {"result": text, "images": [...] } 
            # ä½†æ ‡å‡†çš„ response_builder å¯èƒ½ä¼šæŠŠ dict åºåˆ—åŒ–ã€‚
            # è¿™é‡Œç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªè¿”å›æ–‡æœ¬æè¿°ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥è®© BaseApiTool æ”¯æŒè¿”å›å¤šåª’ä½“ã€‚
            # æŸ¥çœ‹ base_tool.py çš„å®ç°ï¼Œå®ƒåªæ˜¯è¿”å› execute çš„ç»“æœã€‚
            # å¦‚æœæˆ‘ä»¬è¿”å› dictï¼Œå®ƒä¼šè¢« JSON åºåˆ—åŒ–ã€‚
            # ä¸ºäº†è®©å‰ç«¯ï¼ˆå¦‚æœæœ‰ï¼‰èƒ½å±•ç¤ºå›¾ç‰‡ï¼Œæˆ‘ä»¬éœ€è¦éµå¾ªæŸç§åè®®ã€‚
            # è¿™é‡Œæš‚æ—¶åªè¿”å› "Generated X images"ï¼Œå®é™…å›¾ç‰‡æ•°æ®åœ¨ result['images'] ä¸­
            output += f"Generated {len(result['images'])} images (base64 data available)."
            
        return {
            "result": output,
            "images": result["images"] # ä¼ é€’åŸå§‹å›¾ç‰‡æ•°æ®
        }


# ==========================================
# æ³¨å†Œå·¥å…·
# ==========================================

inspect_data = register_api_tool(
    name="ds:inspect_data",
    config_key="ds",
    description="Inspect data directory"
)(InspectDataTool())

read_csv = register_api_tool(
    name="ds:read_csv",
    config_key="ds",
    description="Read CSV file"
)(ReadCsvTool())

run_python = register_api_tool(
    name="ds:run_python",
    config_key="ds",
    description="Run Python code"
)(RunPythonTool())
