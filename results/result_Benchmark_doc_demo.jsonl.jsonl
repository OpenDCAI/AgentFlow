{"task_id": "001", "question": "In document 'test_pdf', What paradigm does CapRL apply to address the limitations of Supervised Fine-Tuning (SFT) in image captioning?", "answer": "CapRL applies the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to address the limitations of Supervised Fine-Tuning (SFT) in image captioning.\n\nWhile SFT relies on expensive, human-annotated data and tends to make models memorize specific ground-truth captions—limiting their generality and creativity—CapRL introduces a decoupled two-stage RL framework. In CapRL, an LVLM generates a caption, and then a separate, vision-free LLM answers Multiple-Choice Questions based only on that caption. The objective reward for the RL training is derived from the accuracy of the LLM's answers, thus redefining caption quality through its utility (how well it enables answering image-related questions).\n\nThis RLVR paradigm allows CapRL to move beyond the memorization and data scalability limits of SFT, encouraging models to generate broader, more accurate, and diverse image descriptions.", "messages": [{"role": "developer", "content": "You are a helpful assistant. You need to use tools to solve the problem.\n\n## Available Tools\n\n- doc_check_tool: Checks if a document (by base name, e.g., report_1) or all PDFs have been converted to structured JSON (OCR) format. THIS MUST BE THE FIRST TOOL CALLED FOR ANY NEW DOCUMENT QUERY.\n- doc_ocr_tool: Converts a raw PDF file into a structured JSON file. ONLY use this immediately AFTER doc_check_tool returns that a file is NOT processed.\n- search_keyword_tool: Searches for a keyword in the structured JSON content of processed documents. ONLY call this tool AFTER confirming the file is processed via doc_check_tool (and running doc_ocr_tool if necessary).\n- get_page_ocr_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and returns the full, concatenated OCR text content of that entire page.\n- get_page_image_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and extracts the full image of that entire page, saving it as a PNG file in the temp directory. Returns the path to the saved image.\n- crop_image_tool: Crops a specific figure, table, or chart from the PDF using the location data (bbox and angle) obtained from the search_keyword_tool. Returns the local file path to the high-resolution, upright PNG image, saved in the temp directory.\n- image_description_tool: Uses a multimodal model (VLM) to analyze a local image file (PNG/JPG) and generate a detailed text description. Use after crop_image_tool or get_page_image_tool to interpret visual content.\n\n## Tool Usage Strategy\n\n**For Multi-Step Analysis:**\n1. Break complex problems into logical steps\n2. Use ONE tool at a time to gather information\n3. Verify findings through different approaches when possible\n"}, {"role": "user", "content": "In document 'test_pdf', What paradigm does CapRL apply to address the limitations of Supervised Fine-Tuning (SFT) in image captioning?"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_52uA9irqSw5clV7JLwG7dgbl", "function": {"arguments": "{\"file_name_or_all\":\"test_pdf\"}", "name": "doc_check_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_52uA9irqSw5clV7JLwG7dgbl", "name": "doc_check_tool", "content": "{\"status\": true, \"files_to_process\": [], \"message\": \"Found 0 files needing OCR processing.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_mqn74ikEUqwZUl2dh5YWoyQT", "function": {"arguments": "{\"keyword\":\"CapRL\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_mqn74ikEUqwZUl2dh5YWoyQT", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"CapRL\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 1, \"type\": \"title\", \"bbox\": [438, 326, 2116, 485], \"angle\": 0, \"content\": \"CAPRL: STIMULATING DENSE IMAGE CAPTION CAPABILITIES VIA REINFORCEMENT LEARNING\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 6, \"type\": \"text\", \"bbox\": [469, 798, 1440, 844], \"angle\": 0, \"content\": \"Model & Data: CapRL HuggingFace Collection\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 7, \"type\": \"text\", \"bbox\": [469, 848, 1147, 894], \"angle\": 0, \"content\": \"Code: CapRL Github Repository\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 1, \"block_id\": 1, \"type\": \"image\", \"bbox\": [443, 333, 2101, 1518], \"angle\": 0, \"content\": null, \"caption\": \"Figure 1: (a) Existing Reward Models: Current LVLM-as-a-judge/reward models suffer from limitations like rewarding morbidity or brevity, leading to low-quality captions and reward hacking. (b) Our CapRL: CapRL uses a decoupled two-stage VQA approach to provide subjective rewards for captions. (c) CapRL's Advantage: CapRL outperforms previous subjective reward methods, as shown by training curves and higher performance in the Prism (Qiao et al. 2024) evaluation setting. problem, where a single image can be accurately described by a wide variety of captions. Since SFT models are trained to match a single ground-truth description for each image, they tend to memorize specific answers rather than learning the underlying concepts. As a result, the SFT models become less general and struggle to generate the diverse range of valid captions possible for a single image.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 1, \"type\": \"image\", \"bbox\": [446, 336, 2085, 1029], \"angle\": 0, \"content\": null, \"caption\": \"Figure 2: Illustration of the captioning capability improvement CapRL brings to Qwen2.5-VL-3B.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 3, \"type\": \"text\", \"bbox\": [433, 1098, 2108, 1653], \"angle\": 0, \"content\": \"is detailed and accurate, a text-based LLM that can't directly \\\"see\\\" the image can still answer Visual Question Answering (VQA) questions about the image. For example, for the question \\\"What color is the frisbee?\\\", the LLM finds the phrase \\\"red frisbee\\\" in the caption and correctly answers \\\"red.\\\" Driven by this motivation, we present an effective decoupled two-stage pipeline, dubbed as Captioning Reinforcement Learning (CapRL), as shown in Fig. 1(b). Specifically, the reward of our CapRL framework is determined by how well a caption generated by an LVLM enables a separate non-visual LLM to answer Multiple-Choice Questions (MCQs) about the source image. The LLM's resulting accuracy serves as the objective reward for the RLVR training. To ensure the high-quality MCQs data that present enough knowledge required for VQA has been examined, we also developed a specific QA curation pipeline. The images are sampled from various sources, including natural images, charts, and documents. The questions and answers are filtered to ensure the questions can only be answered by analyzing the image content itself.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 4, \"type\": \"text\", \"bbox\": [436, 1673, 2111, 2138], \"angle\": 0, \"content\": \"We conduct a comprehensive evaluation of the significant benefits brought by CapRL. From a qualitative perspective, as shown in Fig. 2 applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From a quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al. 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of \\\\(8.4\\\\%\\\\). These results demonstrate that our CapRL framework, by leveraging objective reward design as a reliable optimization signal, effectively drives the model to produce dense and accurate captions.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 2432, 2103, 2620], \"angle\": 0, \"content\": \"2) We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task. We also created a specific QA curation pipeline to ensure the quality of the questions and answers used for the second stage.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 8, \"type\": \"text\", \"bbox\": [436, 2640, 2103, 2781], \"angle\": 0, \"content\": \"3) We carry out extensive experiments to verify the effectiveness of CapRL. Notably, both in the LVLM Pretraining setting for modality alignment and the Prism setting for caption informativeness evaluation, CapRL consistently exhibits superior performance compared to the baselines.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 1, \"type\": \"image\", \"bbox\": [443, 339, 2101, 1072], \"angle\": 0, \"content\": null, \"caption\": \"Figure 3: Overview of CapRL. Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process. Captions generated by the LVLM, paired with curated MCQs (b), are used to query an LLM, whose resulting accuracy becomes the objective reward for the LVLM (a). Our CapRL offers a scalable framework for applying RLVR to the open-ended image captioning task.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 3, \"type\": \"text\", \"bbox\": [436, 1313, 2103, 1732], \"angle\": 0, \"content\": \"captions, researchers design advanced captioning pipelines: BLIP-LAION (Li et al. 2022) generates short synthetic captions, LaCLIP (Fan et al. 2023) uses ChatGPT to rewrite them, and CapsFusion (Yu et al. 2024) consolidates and refine information with fine-tuned models. Besides, there are many research projects which use GPT-4V and human-in-the-loop pipelines to produce richer, fine-grained annotations such as ShareGPT4V (Chen et al. 2024b) and ALLaVA (Chen et al. 2024a). Recent studies (Li et al. 2024b Sun et al. 2024) have explored multi-expert approaches to compensate for LVLM limitations. In summary, some works rely on complex pipelines with multiple models, training-free but costly at inference, while others require lots of expensive labeled data for SFT. In contrast, our CapRL achieves strong performance with remarkable data efficiency through RLVR.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 6, \"type\": \"text\", \"bbox\": [436, 2385, 2103, 2626], \"angle\": 0, \"content\": \"An overview of our CapRL is shown in Fig.3 The CapRL framework consists of a novel, decoupled two-stage process. In the first stage, an LVLM generates a caption for an input image. In the second stage, this caption, along with a series of MCQs, is provided as input to an LLM. In the following, we will describe how to apply RLVR on the image captioning task via our CapRL in Section 3.1 Then we use the model trained with CapRL to construct the CapRL-5M dataset in Section 3.2\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 7, \"type\": \"title\", \"bbox\": [441, 2682, 685, 2729], \"angle\": 0, \"content\": \"3.1 CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 4, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 458, 2108, 877], \"angle\": 0, \"content\": \"To design a reliable verifiable reward module, we leverage a perception-reasoning decoupled VQA task as a proxy to evaluate the quality of captions. The overall process of our proposed method CapRL, is illustrated in Fig. During the GRPO training process, an image and an instruction are first provided as input to the policy model to sample a set of candidate captions. Each caption is then paired with corresponding questions and fed to a Large Language Model (LLM). We assign each caption a reward score based on the accuracy of answers generated by the LLM. Subsequently, we calculate the mean and variance of rewards across the group to derive the advantage for each caption. To ensure training stability, and consistent with the original GRPO framework, we incorporate a KL-divergence penalty. The policy model is then updated via policy gradient optimization.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 4, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 2682, 2108, 3055], \"angle\": 0, \"content\": \"QA Curation. To train CapRL effectively, a high-quality VQA dataset \\\\((q, a)\\\\) with question \\\\(q\\\\) and answer \\\\(a\\\\) is required to provide reliable reward signals. We construct this VQA dataset using a structured three-stage curation pipeline. (1) Image Collection. We begin by sourcing diverse images from the web and existing open-source datasets, including natural scenes, charts, and documents, to maximize variety. (2) QA Generation. For each image, we then use Qwen2.5-VL-72B (Bai et al. 2025) to automatically generate multiple question-answer pairs. (3) QA Filtering. Finally, we implement a stringent QA filtering process to ensure the quality of the generated QA pairs. The QA filtering stage is to verify that all questions are strictly visually-grounded and answerable exclusively\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 990, 2103, 1092], \"angle\": 0, \"content\": \"After filtering, we retain approximately 75k images along with their corresponding QA pairs to train the CapRL captioning model. Please refer to Appendix C and E for the curation details.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1138, 938, 1184], \"angle\": 0, \"content\": \"3.2 CAPRL-5M DATASET\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 1227, 2103, 1326], \"angle\": 0, \"content\": \"By employing our carefully designed CapRL training scheme, we obtained CapRL-3B, and further used this powerful captioner to annotate 5M images, ultimately forming CapRL-5M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 9, \"type\": \"text\", \"bbox\": [436, 1824, 2106, 2059], \"angle\": 0, \"content\": \"Caption Model selection. In typical multimodal pretraining scenarios, the pretraining dataset often requires a massive number of image-text pairs, making annotation costs substantial. Considering practical applications, we decide to train a highly lightweight yet powerful captioner to keep annotation costs more acceptable. Specifically, we initialize the policy model with Qwen2.5-VL-3B and employ our CapRL framework, resulting in the powerful CapRL-3B model as the captioner.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 12, \"type\": \"text\", \"bbox\": [436, 2313, 2103, 2412], \"angle\": 0, \"content\": \"To thoroughly evaluate the quality of the CapRL-5M dataset, we conduct comprehensive comparisons with widely used caption datasets from the open-source community.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 13, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2854], \"angle\": 0, \"content\": \"Implementation Details. In our setup, the language model is initialized with a pretrained LLM, the visual encoder with a pretrained ViT, and the MLP projector randomly, following a standard multimodal pretraining scheme. We conduct experiments under three settings: Qwen2.5-3B + Qwen2.5-ViT, Qwen2.5-7B + Qwen2.5-ViT, and InternLM2.5-7B + CLIP-ViT-L. Training follows the ShareGPT4V paradigm in three stages: Initial Alignment with BLIP-558K dataset (Li et al. 2022); Further Pretraining with diverse high-quality image-caption datasets; and SFT with Open-LLaVA-NeXT-1M (Chen & Xing 2024). For comparison, we adopt strong baselines including Vanilla, which skips Further Pretraining, ShareGPT4V-1M, DenseFusion-1M, and CapRL-1M (randomly sampled from CapRL-5M). Detailed training details are provided in Appendix F\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 14, \"type\": \"text\", \"bbox\": [436, 2864, 2108, 3055], \"angle\": 0, \"content\": \"Main Results. As shown in Table 1 when using CapRL-1M as the further pretraining dataset, performance on the vast majority of benchmarks surpasses both ShareGPT4V-1M and DenseFusion-1M. Specifically, under the Qwen2.5-3B + Qwen2.5-ViT setting, it exceeds DenseFusion-1M by \\\\(6.8\\\\%\\\\) on InfoVQA, and outperforms by \\\\(2.7\\\\%\\\\) and \\\\(3.6\\\\%\\\\) on DocVQA and ChartVQA. These remarkable\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 2, \"type\": \"table\", \"bbox\": [453, 478, 2088, 1240], \"angle\": 0, \"content\": \"<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-1M</td><td>56.2</td><td>87.3</td><td>78.0</td><td>55.1</td><td>45.5</td><td>62.0</td><td>30.3</td><td>70.5</td><td>47.0</td><td>50.0</td><td>72.9</td><td>61.6</td><td>59.7</td></tr><tr><td>CapRL-5M</td><td>61.5</td><td>90.0</td><td>80.5</td><td>57.6</td><td>48.1</td><td>63.2</td><td>30.9</td><td>73.1</td><td>50.4</td><td>52.6</td><td>74.7</td><td>62.6</td><td>62.0</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-7B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>47.6</td><td>83.7</td><td>77.1</td><td>55.9</td><td>47.4</td><td>60.4</td><td>29.4</td><td>72.1</td><td>48.1</td><td>47.1</td><td>72.4</td><td>62.7</td><td>58.7</td></tr><tr><td>ShareGPT4V-1M</td><td>49.8</td><td>85.1</td><td>75.7</td><td>56.8</td><td>46.6</td><td>60.9</td><td>31.8</td><td>71.9</td><td>48.4</td><td>45.9</td><td>72.2</td><td>62.7</td><td>59.0</td></tr><tr><td>DenseFusion-1M</td><td>53.5</td><td>87.8</td><td>76.7</td><td>58.6</td><td>46.3</td><td>61.0</td><td>31.1</td><td>72.6</td><td>48.6</td><td>49.7</td><td>72.5</td><td>63.1</td><td>60.2</td></tr><tr><td>CapRL-1M</td><td>59.9</td><td>89.5</td><td>80.6</td><td>58.9</td><td>50.4</td><td>63.1</td><td>32.2</td><td>72.1</td><td>51.3</td><td>50.5</td><td>75.3</td><td>63.2</td><td>62.2</td></tr><tr><td>CapRL-5M</td><td>63.4</td><td>91.4</td><td>81.5</td><td>61.4</td><td>50.8</td><td>63.2</td><td>34.9</td><td>72.7</td><td>52.6</td><td>52.6</td><td>76.9</td><td>63.8</td><td>63.8</td></tr><tr><td colspan=\\\"14\\\">InternLM2.5-7B + CLIP-ViT-L</td></tr><tr><td>Vanilla</td><td>37.4</td><td>73.2</td><td>68.7</td><td>56.9</td><td>44.2</td><td>58.2</td><td>30.7</td><td>70.7</td><td>47.0</td><td>43.1</td><td>71.8</td><td>64.9</td><td>55.6</td></tr><tr><td>ShareGPT4V-1M</td><td>38.9</td><td>73.8</td><td>69.8</td><td>56.3</td><td>44.8</td><td>59.9</td><td>33.2</td><td>72.6</td><td>46.2</td><td>43.3</td><td>72.7</td><td>65.0</td><td>56.4</td></tr><tr><td>DenseFusion-1M</td><td>39.3</td><td>76.4</td><td>70.8</td><td>59.7</td><td>44.5</td><td>60.3</td><td>34.1</td><td>72.2</td><td>47.9</td><td>44.0</td><td>73.7</td><td>65.5</td><td>57.4</td></tr><tr><td>CapRL-1M</td><td>43.3</td><td>80.0</td><td>75.8</td><td>58.0</td><td>49.6</td><td>62.8</td><td>34.1</td><td>73.4</td><td>50.2</td><td>46.6</td><td>76.0</td><td>65.8</td><td>59.6</td></tr><tr><td>CapRL-5M</td><td>47.0</td><td>83.5</td><td>77.7</td><td>59.7</td><td>50.4</td><td>63.5</td><td>38.9</td><td>73.7</td><td>53.3</td><td>54.3</td><td>77.6</td><td>66.3</td><td>62.2</td></tr></table>\", \"caption\": \"Table 2: Ablation on image sources. We annotate the images in ShareGPT4V-1M and DenseFusion-1M using CapRL-3B, and use them respectively as pretraining datasets for comparison.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 4, \"type\": \"table\", \"bbox\": [479, 1356, 2057, 1719], \"angle\": 0, \"content\": \"<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>CapRL-ShareGPT4V-1M</td><td>52.1</td><td>85.9</td><td>75.2</td><td>56.3</td><td>45.6</td><td>60.0</td><td>30.9</td><td>70.9</td><td>46.7</td><td>47.5</td><td>71.4</td><td>61.7</td><td>58.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-DenseFusion-1M</td><td>55.0</td><td>87.8</td><td>77.5</td><td>56.2</td><td>44.7</td><td>62.8</td><td>32.0</td><td>71.0</td><td>46.6</td><td>49.9</td><td>72.7</td><td>62.3</td><td>59.9</td></tr></table>\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 5, \"type\": \"text\", \"bbox\": [433, 1739, 2103, 2065], \"angle\": 0, \"content\": \"results indicate that CapRL-3B is effective for domains such as documents, charts, and infographics, which demand fine-grained perception and structured description. The captions in CapRL-1M are highly detailed and accurate for such image types, enabling LVLMs to achieve better modality alignment and a deeper understanding of the corresponding visual features. In addition, on natural image benchmarks such as MMStar and MMBench, CapRL-1M surpasses ShareGPT4V-1M by \\\\(1.6\\\\%\\\\) and \\\\(1.8\\\\%\\\\), suggesting that training with CapRL-1M enables multimodal models to acquire richer world knowledge for interpreting objects and their attributes in natural images.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 6, \"type\": \"text\", \"bbox\": [433, 2085, 2103, 2319], \"angle\": 0, \"content\": \"CapRL-5M further demonstrates consistently superior performance across all 12 benchmarks. These results highlight the strong scaling properties of the CapRL-3B-annotated dataset: as the training data size expands from 1M to 5M, model performance continues to improve steadily. This phenomenon underscores the practical value of CapRL for multimodal pretraining, as it enables the construction of high-quality, scalable datasets at very low annotation cost.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 7, \"type\": \"text\", \"bbox\": [433, 2339, 2103, 2709], \"angle\": 0, \"content\": \"Ablations about Image Sources. In the previous comparisons, the images used in each dataset are not identical. To better control for this variable, we fix the set of images and instead compare the effect of caption quality of different datasets under the Qwen2.5-3B + Qwen2.5-ViT setting. As shown in Table2 we compare CapRL with ShareGPT4V-1M and DenseFusion-1M. The results demonstrate that, when using the same set of images, further pretraining with the CapRL-3B-annotated dataset enables the LVLM to outperform the baselines by more than \\\\(2\\\\%\\\\). This finding indicates that the substantial advantage of the CapRL dataset over the baselines largely stems from the superior quality of its captions, rather than from differences in image diversity.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 8, \"type\": \"text\", \"bbox\": [433, 2729, 2108, 3055], \"angle\": 0, \"content\": \"Scaling Trend Comparison of Different Datasets. We further compare the scaling trend of CapRL and DenseFusion under Qwen2.5-3B + Qwen2.5-ViT setting. Specifically, we sample different numbers of image-caption pairs from each dataset for pretraining. As shown in Figure4 the CapRL dataset consistently outperforms the corresponding DenseFusion dataset across various scales of pretraining data. Moreover, the overall trend indicates that this performance gap continues to widen as the data size increases. This phenomenon highlights the strong scaling properties of the CapRL dataset, thanks to its high-quality captions, LVLMs continue to benefit as the dataset size grows.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 4, \"type\": \"image\", \"bbox\": [1685, 336, 2085, 660], \"angle\": 0, \"content\": null, \"caption\": \"Figure 4: The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 7, \"type\": \"table\", \"bbox\": [443, 933, 2096, 1250], \"angle\": 0, \"content\": \"<table><tr><td>Caption\\nModel</td><td>GRPO\\nTrained</td><td>Chart\\nQA</td><td>ChartQA\\nPro</td><td>Info\\nVQA</td><td>MMMU\\nPro</td><td>Math\\nVerse</td><td>Char\\nXiv</td><td>We\\nMath</td><td>Math\\nVision</td><td>MMStar</td><td>SEED</td><td>MMMU</td><td>Average</td></tr><tr><td>Qwen2.5-VL-3B</td><td>X</td><td>65.6</td><td>27.1</td><td>40.2</td><td>28.6</td><td>32.8</td><td>21.8</td><td>54.4</td><td>22.6</td><td>46.4</td><td>64.1</td><td>35.1</td><td>39.9</td></tr><tr><td>Qwen2.5-VL-7B</td><td>X</td><td>74.9</td><td>35.4</td><td>56.4</td><td>30.1</td><td>36.4</td><td>24.8</td><td>57.0</td><td>23.3</td><td>50.7</td><td>67.1</td><td>37.9</td><td>44.9</td></tr><tr><td>Qwen2.5-VL-72B</td><td>X</td><td>80.2</td><td>38.0</td><td>60.8</td><td>34.1</td><td>39.9</td><td>30.7</td><td>60.2</td><td>24.5</td><td>55.0</td><td>69.3</td><td>39.4</td><td>48.3</td></tr><tr><td>UnifiedRW-as-Judge-3B</td><td>✓</td><td>54.9</td><td>25.1</td><td>33.6</td><td>28.1</td><td>34.6</td><td>20.4</td><td>58.2</td><td>24.5</td><td>45.4</td><td>61.2</td><td>36.3</td><td>38.4</td></tr><tr><td>Qwen2.5VL-as-Judge-3B</td><td>✓</td><td>71.4</td><td>34.2</td><td>49.3</td><td>29.1</td><td>33.8</td><td>22.9</td><td>54.3</td><td>24.1</td><td>47.7</td><td>64.5</td><td>36.4</td><td>42.5</td></tr><tr><td>CapRL-3B</td><td>✓</td><td>80.5</td><td>39.9</td><td>64.8</td><td>30.7</td><td>36.4</td><td>32.4</td><td>60.1</td><td>23.4</td><td>55.0</td><td>70.6</td><td>38.1</td><td>48.3</td></tr></table>\", \"caption\": \"Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-as-a-Judge as the reward. The best results are bold and the second-best results are underlined.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 9, \"type\": \"image\", \"bbox\": [1537, 1287, 2101, 1640], \"angle\": 0, \"content\": null, \"caption\": \"Figure 5: (Left) CapRL demonstrates strong generalization even when trained on images from a single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability. (Right) CapRL demonstrates promising scaling performance on QA training datasets.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 1993, 2103, 2187], \"angle\": 0, \"content\": \"In the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al. 2024), and compare our CapRL-3B against other captioning models.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 14, \"type\": \"text\", \"bbox\": [433, 2636, 2108, 3052], \"angle\": 0, \"content\": \"Comparison with Qwen2.5-VL series. As shown in Table3 CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model. In chart and infographic understanding, CapRL-3B surpasses Qwen2.5-VL-3B by \\\\(14.9\\\\%\\\\), \\\\(12.8\\\\%\\\\), and \\\\(24.6\\\\%\\\\) on ChartQA, ChartQAPro, and InfoVQA, respectively. For natural image understanding, it leads Qwen2.5-VL-3B by \\\\(9.6\\\\%\\\\) and \\\\(6.5\\\\%\\\\) on MMStar and SEED. These results demonstrate that GRPO training has substantially unlocked the potential of Qwen2.5-VL-3B, enabling it to fully leverage its inherent knowledge to organize all objects and their attributes within an image into comprehensive and detailed captions. As a result, its perception capability is pushed to the limit, reaching a level comparable to that of the 72B model.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 2, \"type\": \"table\", \"bbox\": [443, 386, 1275, 640], \"angle\": 0, \"content\": \"<table><tr><td>Caption \\nModel</td><td>ChartQA \\nPro</td><td>Info \\nVQA</td><td>MMMU</td><td>MMStar</td><td>WeMath</td><td>Avg</td></tr><tr><td>Qwen2.5-VL-3B</td><td>27.1</td><td>40.2</td><td>35.1</td><td>46.4</td><td>54.4</td><td>40.6</td></tr><tr><td>CapRL-1QA-20k</td><td>35.5</td><td>59.8</td><td>36.6</td><td>50.8</td><td>57.3</td><td>48.0</td></tr><tr><td>CapRL-2QA-20k</td><td>36.8</td><td>60.2</td><td>37.6</td><td>51.1</td><td>56.6</td><td>48.5</td></tr><tr><td>CapRL-3QA-20k</td><td>36.9</td><td>60.3</td><td>36.9</td><td>51.3</td><td>56.8</td><td>48.5</td></tr></table>\", \"caption\": \"Table 5: Ablations about Sampling Rounds N.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 699, 2108, 1349], \"angle\": 0, \"content\": \"Comparison with LVLM-as-a-Judge reward. In our comparison with other reward design methods, we observe that when using UnifiedReward-2.0-qwen-3b as the judge to evaluate caption quality, the model's captioning ability actually deteriorates during GRPO training. We attribute this to the severe bias present in UnifiedReward-2.0-qwen-3b: during its training, it was exposed to lots of captions from text-to-image datasets, which are typically short and only describe the main objects. As a result, the UnifiedReward model tends to favor shorter captions. As shown in Fig.1 the average caption length during training continuously decreases and eventually collapses to producing only “:description”. Conversely, when using Qwen2.5-VL-3B as the judge, the bias is in the opposite direction: it prefers overly verbose captions. This makes the policy model prone to exploiting the bias by generating long passages of content irrelevant to the image, thereby satisfying the judge model's preference. As shown in Table3 the captioning ability under this reward shows significantly inferior to CapRL. Specific examples of such cases are illustrated in Fig.9 Fig.10 Fig.11 All these observations highlight that LVLM-as-a-Judge reward is fundamentally unreliable. This further underscores the advantage of CapRL, which converts subjective evaluations into objective assessments.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1465, 1377, 1511], \"angle\": 0, \"content\": \"4.3 COMPREHENSIVE DISCUSSION ABOUT CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 1577, 2103, 1673], \"angle\": 0, \"content\": \"In this section, we provide a comprehensive analysis and discussion of CapRL. These results further confirm CapRL's general applicability, robustness, and effectiveness.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 8, \"type\": \"text\", \"bbox\": [436, 1692, 2108, 2065], \"angle\": 0, \"content\": \"CapRL demonstrates strong generalization even when trained on images from a single domain. We further investigate the effect of different image sources in the QA dataset used for GRPO training. To this end, we classify the images into two categories using Qwen2.5-VL-3B: (1) documents, charts, or infographics, and (2) natural images. From each category, we sample 20k images for comparison. As illustrated in Fig. 5 (Left), models trained exclusively on chart-type images via GRPO exhibit substantial gains over Qwen2.5-VL-3B, not only in document and chart understanding but also in general VQA tasks. This demonstrates the strong generalization of CapRL-induced captioning improvements beyond the domains encountered during training.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 9, \"type\": \"text\", \"bbox\": [436, 2085, 2103, 2412], \"angle\": 0, \"content\": \"CapRL demonstrates promising scaling performance on training data. We conduct training on different amounts of QA data to evaluate the scaling behavior. As shown in Fig. [5] (Right), the model's performance improves steadily as the amount of QA data increases. These results indicate that our CapRL framework exhibits highly promising scaling potential. With the continued expansion of the training data, the captioning ability can be further enhanced, unlocking additional potential of Qwen2.5-VL-3B. Given its relatively small parameter size and excellent scaling properties, this approach holds strong promise for application in industrial-scale multimodal pretraining.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 10, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2801], \"angle\": 0, \"content\": \"Sparse QA supervision is sufficient for CapRL. We further examine the effect of varying the number of QA pairs per image. Specifically, we randomly sample 20k images that retain three QA pairs after filtering, obtain CapRL-3QA-20k after training. By controlling the number of QA pairs per image, we also construct CapRL-1QA-20k and CapRL-2QA-20k. The results, presented in Table4 show that even with only a single QA pair per image, Qwen2.5-VL-3B achieves a substantial improvement in captioning performance, averaging 7.4% higher than the baseline and only 0.5% lower than CapRL-2QA-20k. This highlights the remarkable efficiency of CapRL: highly sparse QA supervision is sufficient to unlock significant gains in captioning ability.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 9, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 438, 2111, 719], \"angle\": 0, \"content\": \"In this work, we introduce CapRL, a novel framework that successfully applies RLVR to the subjective task of image captioning. By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal. Our results show that CapRL effectively encourages models to generate dense and precise image descriptions, which in turn substantially promote modality alignment in LVLM pretraining. This work marks a significant step away from the restrictive, data-hungry SFT paradigm for RLVR in open-ended tasks.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 1, \"type\": \"title\", \"bbox\": [441, 339, 854, 386], \"angle\": 0, \"content\": \"A CAPRL CASES\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 452, 2108, 544], \"angle\": 0, \"content\": \"We provide further illustrative examples of CapRL-3B to highlight its surprising captioning capabilities in this section.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 3, \"type\": \"text\", \"bbox\": [436, 567, 2103, 851], \"angle\": 0, \"content\": \"Comparison with Qwen2.5-VL-3B. As illustrated in Fig. 6 CapRL-3B demonstrates remarkable capability in understanding infographics, providing information that is both comprehensive and accurate. In contrast, Qwen2.5-VL-3B, as shown in Fig. 7 makes numerous errors in identifying key information within infographics. Furthermore, Fig. 8 highlights that CapRL-3B achieves substantially higher accuracy in chart understanding compared to Qwen2.5-VL-3B. Similarly, in the case of physical image understanding in Fig. 12 CapRL-3B also demonstrates clear superiority.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 1211, 2113, 1310], \"angle\": 0, \"content\": \"More cases of CapRL-3B in understanding infographics and natural images. Fig. 13 and Fig. 14 provide additional evidence of the impressive perceptual capacity demonstrated by CapRL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1389, 1545, 1438], \"angle\": 0, \"content\": \"B MORE ANALYSIS EXPERIMENTS ABOUT CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 12, \"type\": \"text\", \"bbox\": [425, 2253, 2103, 2448], \"angle\": 0, \"content\": \"We provide all the prompts employed in our experiments in this section. Specifically, the prompt used in CapRL for guiding the LLM to answer questions conditioned on captions is illustrated in Figure 15; the prompt for utilizing the Unified Reward Model as the reward model is shown in Figure 16, and the prompt for adopting Qwen2.5-VL-3B as the reward model is presented in Figure 17\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 14, \"type\": \"text\", \"bbox\": [436, 2636, 2108, 3055], \"angle\": 0, \"content\": \"To ensure both quality and safety, our data processing pipeline consists of three main stages. First, inspired by SemDeDup (Abbas et al. 2023), we construct clusters to identify and remove images with redundant semantics. During this step, we also discard low-resolution and overly simple images, while filtering out content that involves violence, pornography, or other safety concerns. Second, to avoid benchmark leakage, we integrate the images used in commonly referenced evaluation datasets and form clusters with them. Any images from our collection that are overly similar to benchmark samples are eliminated. Third, we conduct a safety inspection through human verification. Annotators perform sample-based screening, and once the proportion of unsafe images falls to a negligible level, we stop filtering. Following this process, we obtain the final dataset, CapRL-5M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 14, \"block_id\": 1, \"type\": \"title\", \"bbox\": [466, 356, 650, 396], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 14, \"block_id\": 43, \"type\": \"image_caption\", \"bbox\": [510, 2333, 2029, 2385], \"angle\": 0, \"content\": \"Figure 6: An illustrative example of CapRL-3B applied to infographic understanding.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 1, \"type\": \"title\", \"bbox\": [487, 372, 668, 412], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 106, \"type\": \"image_caption\", \"bbox\": [510, 2742, 2027, 2795], \"angle\": 0, \"content\": \"Figure 8: Chart understanding comparison between CapRL-3B and Qwen2.5-VL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 1, \"type\": \"title\", \"bbox\": [484, 359, 668, 396], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 43, \"type\": \"image_caption\", \"bbox\": [785, 2517, 1754, 2570], \"angle\": 0, \"content\": \"Figure 9: Example of CapRL-3B chart understanding.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 45, \"type\": \"text\", \"bbox\": [436, 2818, 2108, 3055], \"angle\": 0, \"content\": \"Baselines. We selected several strong baselines for comparison. (1) Vanilla, which skips the Further Pretraining stage and only goes through the first and third stages. Additionally, we constructed two more baselines by varying the dataset used in the Further Pretraining stage: (2) ShareGPT4V-1M, and (3) DenseFusion-1M. To ensure a fair comparison by controlling the number of samples, we randomly sampled 1 million image-caption pairs from the 5M dataset to form CapRL-1M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 21, \"block_id\": 8, \"type\": \"title\", \"bbox\": [1476, 1376, 1624, 1402], \"angle\": 0, \"content\": \"After CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 21, \"block_id\": 16, \"type\": \"image_caption\", \"bbox\": [530, 2036, 2009, 2092], \"angle\": 0, \"content\": \"Figure 12: Example of CapRL enhancing the captioning ability of Qwen2.5-VL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 22, \"block_id\": 1, \"type\": \"title\", \"bbox\": [476, 653, 660, 689], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 22, \"block_id\": 101, \"type\": \"image\", \"bbox\": [1981, 1587, 2062, 1676], \"angle\": 0, \"content\": null, \"caption\": \"Figure 13: An illustrative example of CapRL applied to infographic understanding.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 23, \"block_id\": 1, \"type\": \"title\", \"bbox\": [469, 683, 650, 722], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 23, \"block_id\": 27, \"type\": \"image_caption\", \"bbox\": [510, 2669, 2029, 2722], \"angle\": 0, \"content\": \"Figure 14: An illustrative example of CapRL applied to natural image understanding.\", \"caption\": \"\"}], \"message\": \"Found 62 matches for keyword 'CapRL'.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_YN6Pz9XXmWySMo89YQDAA06I", "function": {"arguments": "{\"keyword\":\"Supervised Fine-Tuning\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_YN6Pz9XXmWySMo89YQDAA06I", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"Supervised Fine-Tuning\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 2821, 2111, 3055], \"angle\": 0, \"content\": \"Given the importance of image captioning, there is a strong need for captioning models that can provide dense and accurate descriptions. Most modern captioning models Chen et al. (2024b); Rotstein et al. (2024); Vasu et al. (2025) are trained based on LVLMs using Supervised Fine-Tuning (SFT). While effective, SFT requires large datasets annotated by humans or proprietary models, which are expensive and not scalable. Furthermore, image captioning is an inherently open-ended\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 6, \"type\": \"text\", \"bbox\": [436, 2224, 2103, 2415], \"angle\": 0, \"content\": \"1) We contribute the first study of applying Reinforcement Learning with Verifiable Rewards for the open-ended and subjective image captioning task. Unlike traditional Supervised Fine-Tuning, which can lead to models memorizing a limited set of annotated captions, our method allows the model to explore and generate a broader range of creative and general descriptions.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 4, \"type\": \"text\", \"bbox\": [436, 1785, 2108, 2207], \"angle\": 0, \"content\": \"Reinforcement Learning with Verifiable Rewards (RLVR). RLVR (Lambert et al. 2024) represents a promising paradigm for training Large Language Models (LLMs) to tasks that have an objective, easily verifiable reward signal. For example, in mathematical problem-solving, the reward can be a binary signal of correctness (Shao et al. 2024), and for code generation, it can be whether the code passes unit tests (Team et al. 2025). Compared to the traditional Supervised Fine-Tuning (SFT), RLVR offers a more robust and scalable approach. While SFT trains models to imitate a set of provided ground-truth answers, often leading to models that memorize specific phrasings (Chu et al. 2025), RLVR encourages the model to explore and discover optimal solutions. This is particularly beneficial for problems with multiple valid answers or reasoning paths.\", \"caption\": \"\"}], \"message\": \"Found 4 matches for keyword 'Supervised Fine-Tuning'.\"}"}, {"content": "CapRL applies the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to address the limitations of Supervised Fine-Tuning (SFT) in image captioning.\n\nWhile SFT relies on expensive, human-annotated data and tends to make models memorize specific ground-truth captions—limiting their generality and creativity—CapRL introduces a decoupled two-stage RL framework. In CapRL, an LVLM generates a caption, and then a separate, vision-free LLM answers Multiple-Choice Questions based only on that caption. The objective reward for the RL training is derived from the accuracy of the LLM's answers, thus redefining caption quality through its utility (how well it enables answering image-related questions).\n\nThis RLVR paradigm allows CapRL to move beyond the memorization and data scalability limits of SFT, encouraging models to generate broader, more accurate, and diverse image descriptions.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}], "success": true, "error": null}
{"task_id": "002", "question": "In document 'test_pdf', How many benchmarks show substantial gains when pretraining on the CapRL5M caption dataset annotated by CapRL-3B?", "answer": "The document states: \"Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks.\"\n\n**Answer:** 12 benchmarks show substantial gains when pretraining on the CapRL5M caption dataset annotated by CapRL-3B.", "messages": [{"role": "developer", "content": "You are a helpful assistant. You need to use tools to solve the problem.\n\n## Available Tools\n\n- doc_check_tool: Checks if a document (by base name, e.g., report_1) or all PDFs have been converted to structured JSON (OCR) format. THIS MUST BE THE FIRST TOOL CALLED FOR ANY NEW DOCUMENT QUERY.\n- doc_ocr_tool: Converts a raw PDF file into a structured JSON file. ONLY use this immediately AFTER doc_check_tool returns that a file is NOT processed.\n- search_keyword_tool: Searches for a keyword in the structured JSON content of processed documents. ONLY call this tool AFTER confirming the file is processed via doc_check_tool (and running doc_ocr_tool if necessary).\n- get_page_ocr_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and returns the full, concatenated OCR text content of that entire page.\n- get_page_image_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and extracts the full image of that entire page, saving it as a PNG file in the temp directory. Returns the path to the saved image.\n- crop_image_tool: Crops a specific figure, table, or chart from the PDF using the location data (bbox and angle) obtained from the search_keyword_tool. Returns the local file path to the high-resolution, upright PNG image, saved in the temp directory.\n- image_description_tool: Uses a multimodal model (VLM) to analyze a local image file (PNG/JPG) and generate a detailed text description. Use after crop_image_tool or get_page_image_tool to interpret visual content.\n\n## Tool Usage Strategy\n\n**For Multi-Step Analysis:**\n1. Break complex problems into logical steps\n2. Use ONE tool at a time to gather information\n3. Verify findings through different approaches when possible\n"}, {"role": "user", "content": "In document 'test_pdf', How many benchmarks show substantial gains when pretraining on the CapRL5M caption dataset annotated by CapRL-3B?"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_luZjTCR0i88IzNufKjASGLnO", "function": {"arguments": "{\"file_name_or_all\":\"test_pdf\"}", "name": "doc_check_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_luZjTCR0i88IzNufKjASGLnO", "name": "doc_check_tool", "content": "{\"status\": true, \"files_to_process\": [], \"message\": \"Found 0 files needing OCR processing.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_pJ6Iostq8FHdSCK2IuLZJkgy", "function": {"arguments": "{\"keyword\":\"CapRL5M\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_pJ6Iostq8FHdSCK2IuLZJkgy", "name": "search_keyword_tool", "content": "{\"status\": false, \"keyword\": \"CapRL5M\", \"matches\": [], \"message\": \"No matches found for keyword 'CapRL5M'.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_3jFrYTmOmgbgMFpzeIyRNvwG", "function": {"arguments": "{\"keyword\":\"caption dataset\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_3jFrYTmOmgbgMFpzeIyRNvwG", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"caption dataset\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 4, \"type\": \"text\", \"bbox\": [436, 1673, 2111, 2138], \"angle\": 0, \"content\": \"We conduct a comprehensive evaluation of the significant benefits brought by CapRL. From a qualitative perspective, as shown in Fig. 2 applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From a quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al. 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of \\\\(8.4\\\\%\\\\). These results demonstrate that our CapRL framework, by leveraging objective reward design as a reliable optimization signal, effectively drives the model to produce dense and accurate captions.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 12, \"type\": \"text\", \"bbox\": [436, 2313, 2103, 2412], \"angle\": 0, \"content\": \"To thoroughly evaluate the quality of the CapRL-5M dataset, we conduct comprehensive comparisons with widely used caption datasets from the open-source community.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 13, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2854], \"angle\": 0, \"content\": \"Implementation Details. In our setup, the language model is initialized with a pretrained LLM, the visual encoder with a pretrained ViT, and the MLP projector randomly, following a standard multimodal pretraining scheme. We conduct experiments under three settings: Qwen2.5-3B + Qwen2.5-ViT, Qwen2.5-7B + Qwen2.5-ViT, and InternLM2.5-7B + CLIP-ViT-L. Training follows the ShareGPT4V paradigm in three stages: Initial Alignment with BLIP-558K dataset (Li et al. 2022); Further Pretraining with diverse high-quality image-caption datasets; and SFT with Open-LLaVA-NeXT-1M (Chen & Xing 2024). For comparison, we adopt strong baselines including Vanilla, which skips Further Pretraining, ShareGPT4V-1M, DenseFusion-1M, and CapRL-1M (randomly sampled from CapRL-5M). Detailed training details are provided in Appendix F\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 107, \"type\": \"text\", \"bbox\": [436, 2864, 2103, 3052], \"angle\": 0, \"content\": \"learning rate of 1e-3 and a batch size of 256. (2) In the Further Pretraining stage, we unfreeze all parameters including the LLM, MLP, and ViT. This stage facilitates further alignment with various high-quality image-caption datasets, enabling the LLM to better understand visual features. We set the learning rate to 4e-5 and the batch size to 256. (3) In the SFT stage, we again unfreeze all\", \"caption\": \"\"}], \"message\": \"Found 5 matches for keyword 'caption dataset'.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_2MNkNQKirEBHn3S9oxOEqujS", "function": {"arguments": "{\"file_name\":\"1111test_pdf\",\"page_id\":0}", "name": "get_page_ocr_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_2MNkNQKirEBHn3S9oxOEqujS", "name": "get_page_ocr_tool", "content": "{\"status\": true, \"file_name\": \"1111test_pdf\", \"page_id\": 0, \"ocr_content\": \"Preprint\\n\\nCAPRL: STIMULATING DENSE IMAGE CAPTION CAPABILITIES VIA REINFORCEMENT LEARNING\\n\\nLong Xing\\\\(^{1,2}\\\\), Xiaoyi Dong\\\\(^{2,3}\\\\), Yuhang Zang\\\\(^{2}\\\\), Yuhang Cao\\\\(^{2}\\\\), Jianze Liang\\\\(^{2}\\\\), Qidong Huang\\\\(^{5}\\\\), Jiaqi Wang\\\\(^{2,4}\\\\), Feng Wu\\\\(^{1}\\\\), Dahua Lin\\\\(^{2,3}\\\\).\\n\\n<sup>1</sup>University of Science and Technology of China, <sup>2</sup>Shanghai AI Laboratory,\\n\\n<sup>3</sup>The Chinese University of Hong Kong, <sup>4</sup>Shanghai Innovation Institute, <sup>5</sup>Alibaba Cloud\\n\\nxing_long@mail.ustc.edu.cn\\n\\nModel & Data: CapRL HuggingFace Collection\\n\\nCode: CapRL Github Repository\\n\\nABSTRACT\\n\\nImage captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\\n\\n1 INTRODUCTION\\n\\nThe image captioning task (Karpathy & Fei-Fei 2015, Vinyals et al. 2015), which generates a natural language description for a given image, bridges the gap between the visual and linguistic worlds. The captioning capability is fundamental to various applications, including vision-language models like CLIP (Radford et al. 2021), which learn a shared embedding space for images and text. Furthermore, captions are often a core component in the pre-training stage of Large Vision-Language Models (LVLMs) (Liu et al. 2023b), where the model learns to align visual information with linguistic descriptions on a massive scale before being fine-tuned for other downstream tasks.\\n\\nGiven the importance of image captioning, there is a strong need for captioning models that can provide dense and accurate descriptions. Most modern captioning models Chen et al. (2024b); Rotstein et al. (2024); Vasu et al. (2025) are trained based on LVLMs using Supervised Fine-Tuning (SFT). While effective, SFT requires large datasets annotated by humans or proprietary models, which are expensive and not scalable. Furthermore, image captioning is an inherently open-ended\\n\\narXiv:2509.22647v1 [cs.CV] 26 Sep 2025\\n\\n1\", \"message\": \"Successfully retrieved OCR content for page 0 of file '1111test_pdf'.\"}"}, {"content": "The document states: \"Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks.\"\n\n**Answer:** 12 benchmarks show substantial gains when pretraining on the CapRL5M caption dataset annotated by CapRL-3B.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}], "success": true, "error": null}
{"task_id": "003", "question": "In document 'test_pdf', Does CapRL use a vision-free LLM to evaluate caption quality in its pipeline?", "answer": "Yes, CapRL uses a vision-free LLM (Large Language Model without visual input capabilities) to evaluate caption quality in its pipeline. Specifically, CapRL employs a decoupled two-stage process: first, an LVLM (Large Vision-Language Model) generates captions for images; then, these captions are passed to a separate vision-free LLM, which answers multiple-choice questions about the image using only the caption. The accuracy of the vision-free LLM on these questions is used as the objective reward to assess and improve caption quality.\n\nThis design is explicitly stated in the paper:\n- \"CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption.\" (page 1)\n- \"The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task.\" (page 3)\n- \"By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal.\" (page 9)\n\nThus, CapRL’s pipeline explicitly uses a vision-free LLM as the judge of caption quality.", "messages": [{"role": "developer", "content": "You are a helpful assistant. You need to use tools to solve the problem.\n\n## Available Tools\n\n- doc_check_tool: Checks if a document (by base name, e.g., report_1) or all PDFs have been converted to structured JSON (OCR) format. THIS MUST BE THE FIRST TOOL CALLED FOR ANY NEW DOCUMENT QUERY.\n- doc_ocr_tool: Converts a raw PDF file into a structured JSON file. ONLY use this immediately AFTER doc_check_tool returns that a file is NOT processed.\n- search_keyword_tool: Searches for a keyword in the structured JSON content of processed documents. ONLY call this tool AFTER confirming the file is processed via doc_check_tool (and running doc_ocr_tool if necessary).\n- get_page_ocr_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and returns the full, concatenated OCR text content of that entire page.\n- get_page_image_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and extracts the full image of that entire page, saving it as a PNG file in the temp directory. Returns the path to the saved image.\n- crop_image_tool: Crops a specific figure, table, or chart from the PDF using the location data (bbox and angle) obtained from the search_keyword_tool. Returns the local file path to the high-resolution, upright PNG image, saved in the temp directory.\n- image_description_tool: Uses a multimodal model (VLM) to analyze a local image file (PNG/JPG) and generate a detailed text description. Use after crop_image_tool or get_page_image_tool to interpret visual content.\n\n## Tool Usage Strategy\n\n**For Multi-Step Analysis:**\n1. Break complex problems into logical steps\n2. Use ONE tool at a time to gather information\n3. Verify findings through different approaches when possible\n"}, {"role": "user", "content": "In document 'test_pdf', Does CapRL use a vision-free LLM to evaluate caption quality in its pipeline?"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_TGKDpNVQbFk5NTOpFvn0DJjE", "function": {"arguments": "{\"file_name_or_all\":\"test_pdf\"}", "name": "doc_check_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_TGKDpNVQbFk5NTOpFvn0DJjE", "name": "doc_check_tool", "content": "{\"status\": true, \"files_to_process\": [], \"message\": \"Found 0 files needing OCR processing.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_QJUQJqB34gddFhV0NerQu8O7", "function": {"arguments": "{\"keyword\":\"CapRL\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_QJUQJqB34gddFhV0NerQu8O7", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"CapRL\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 1, \"type\": \"title\", \"bbox\": [438, 326, 2116, 485], \"angle\": 0, \"content\": \"CAPRL: STIMULATING DENSE IMAGE CAPTION CAPABILITIES VIA REINFORCEMENT LEARNING\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 6, \"type\": \"text\", \"bbox\": [469, 798, 1440, 844], \"angle\": 0, \"content\": \"Model & Data: CapRL HuggingFace Collection\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 7, \"type\": \"text\", \"bbox\": [469, 848, 1147, 894], \"angle\": 0, \"content\": \"Code: CapRL Github Repository\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 1, \"block_id\": 1, \"type\": \"image\", \"bbox\": [443, 333, 2101, 1518], \"angle\": 0, \"content\": null, \"caption\": \"Figure 1: (a) Existing Reward Models: Current LVLM-as-a-judge/reward models suffer from limitations like rewarding morbidity or brevity, leading to low-quality captions and reward hacking. (b) Our CapRL: CapRL uses a decoupled two-stage VQA approach to provide subjective rewards for captions. (c) CapRL's Advantage: CapRL outperforms previous subjective reward methods, as shown by training curves and higher performance in the Prism (Qiao et al. 2024) evaluation setting. problem, where a single image can be accurately described by a wide variety of captions. Since SFT models are trained to match a single ground-truth description for each image, they tend to memorize specific answers rather than learning the underlying concepts. As a result, the SFT models become less general and struggle to generate the diverse range of valid captions possible for a single image.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 1, \"type\": \"image\", \"bbox\": [446, 336, 2085, 1029], \"angle\": 0, \"content\": null, \"caption\": \"Figure 2: Illustration of the captioning capability improvement CapRL brings to Qwen2.5-VL-3B.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 3, \"type\": \"text\", \"bbox\": [433, 1098, 2108, 1653], \"angle\": 0, \"content\": \"is detailed and accurate, a text-based LLM that can't directly \\\"see\\\" the image can still answer Visual Question Answering (VQA) questions about the image. For example, for the question \\\"What color is the frisbee?\\\", the LLM finds the phrase \\\"red frisbee\\\" in the caption and correctly answers \\\"red.\\\" Driven by this motivation, we present an effective decoupled two-stage pipeline, dubbed as Captioning Reinforcement Learning (CapRL), as shown in Fig. 1(b). Specifically, the reward of our CapRL framework is determined by how well a caption generated by an LVLM enables a separate non-visual LLM to answer Multiple-Choice Questions (MCQs) about the source image. The LLM's resulting accuracy serves as the objective reward for the RLVR training. To ensure the high-quality MCQs data that present enough knowledge required for VQA has been examined, we also developed a specific QA curation pipeline. The images are sampled from various sources, including natural images, charts, and documents. The questions and answers are filtered to ensure the questions can only be answered by analyzing the image content itself.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 4, \"type\": \"text\", \"bbox\": [436, 1673, 2111, 2138], \"angle\": 0, \"content\": \"We conduct a comprehensive evaluation of the significant benefits brought by CapRL. From a qualitative perspective, as shown in Fig. 2 applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From a quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al. 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of \\\\(8.4\\\\%\\\\). These results demonstrate that our CapRL framework, by leveraging objective reward design as a reliable optimization signal, effectively drives the model to produce dense and accurate captions.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 2432, 2103, 2620], \"angle\": 0, \"content\": \"2) We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task. We also created a specific QA curation pipeline to ensure the quality of the questions and answers used for the second stage.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 8, \"type\": \"text\", \"bbox\": [436, 2640, 2103, 2781], \"angle\": 0, \"content\": \"3) We carry out extensive experiments to verify the effectiveness of CapRL. Notably, both in the LVLM Pretraining setting for modality alignment and the Prism setting for caption informativeness evaluation, CapRL consistently exhibits superior performance compared to the baselines.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 1, \"type\": \"image\", \"bbox\": [443, 339, 2101, 1072], \"angle\": 0, \"content\": null, \"caption\": \"Figure 3: Overview of CapRL. Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process. Captions generated by the LVLM, paired with curated MCQs (b), are used to query an LLM, whose resulting accuracy becomes the objective reward for the LVLM (a). Our CapRL offers a scalable framework for applying RLVR to the open-ended image captioning task.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 3, \"type\": \"text\", \"bbox\": [436, 1313, 2103, 1732], \"angle\": 0, \"content\": \"captions, researchers design advanced captioning pipelines: BLIP-LAION (Li et al. 2022) generates short synthetic captions, LaCLIP (Fan et al. 2023) uses ChatGPT to rewrite them, and CapsFusion (Yu et al. 2024) consolidates and refine information with fine-tuned models. Besides, there are many research projects which use GPT-4V and human-in-the-loop pipelines to produce richer, fine-grained annotations such as ShareGPT4V (Chen et al. 2024b) and ALLaVA (Chen et al. 2024a). Recent studies (Li et al. 2024b Sun et al. 2024) have explored multi-expert approaches to compensate for LVLM limitations. In summary, some works rely on complex pipelines with multiple models, training-free but costly at inference, while others require lots of expensive labeled data for SFT. In contrast, our CapRL achieves strong performance with remarkable data efficiency through RLVR.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 6, \"type\": \"text\", \"bbox\": [436, 2385, 2103, 2626], \"angle\": 0, \"content\": \"An overview of our CapRL is shown in Fig.3 The CapRL framework consists of a novel, decoupled two-stage process. In the first stage, an LVLM generates a caption for an input image. In the second stage, this caption, along with a series of MCQs, is provided as input to an LLM. In the following, we will describe how to apply RLVR on the image captioning task via our CapRL in Section 3.1 Then we use the model trained with CapRL to construct the CapRL-5M dataset in Section 3.2\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 7, \"type\": \"title\", \"bbox\": [441, 2682, 685, 2729], \"angle\": 0, \"content\": \"3.1 CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 4, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 458, 2108, 877], \"angle\": 0, \"content\": \"To design a reliable verifiable reward module, we leverage a perception-reasoning decoupled VQA task as a proxy to evaluate the quality of captions. The overall process of our proposed method CapRL, is illustrated in Fig. During the GRPO training process, an image and an instruction are first provided as input to the policy model to sample a set of candidate captions. Each caption is then paired with corresponding questions and fed to a Large Language Model (LLM). We assign each caption a reward score based on the accuracy of answers generated by the LLM. Subsequently, we calculate the mean and variance of rewards across the group to derive the advantage for each caption. To ensure training stability, and consistent with the original GRPO framework, we incorporate a KL-divergence penalty. The policy model is then updated via policy gradient optimization.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 4, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 2682, 2108, 3055], \"angle\": 0, \"content\": \"QA Curation. To train CapRL effectively, a high-quality VQA dataset \\\\((q, a)\\\\) with question \\\\(q\\\\) and answer \\\\(a\\\\) is required to provide reliable reward signals. We construct this VQA dataset using a structured three-stage curation pipeline. (1) Image Collection. We begin by sourcing diverse images from the web and existing open-source datasets, including natural scenes, charts, and documents, to maximize variety. (2) QA Generation. For each image, we then use Qwen2.5-VL-72B (Bai et al. 2025) to automatically generate multiple question-answer pairs. (3) QA Filtering. Finally, we implement a stringent QA filtering process to ensure the quality of the generated QA pairs. The QA filtering stage is to verify that all questions are strictly visually-grounded and answerable exclusively\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 990, 2103, 1092], \"angle\": 0, \"content\": \"After filtering, we retain approximately 75k images along with their corresponding QA pairs to train the CapRL captioning model. Please refer to Appendix C and E for the curation details.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1138, 938, 1184], \"angle\": 0, \"content\": \"3.2 CAPRL-5M DATASET\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 1227, 2103, 1326], \"angle\": 0, \"content\": \"By employing our carefully designed CapRL training scheme, we obtained CapRL-3B, and further used this powerful captioner to annotate 5M images, ultimately forming CapRL-5M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 9, \"type\": \"text\", \"bbox\": [436, 1824, 2106, 2059], \"angle\": 0, \"content\": \"Caption Model selection. In typical multimodal pretraining scenarios, the pretraining dataset often requires a massive number of image-text pairs, making annotation costs substantial. Considering practical applications, we decide to train a highly lightweight yet powerful captioner to keep annotation costs more acceptable. Specifically, we initialize the policy model with Qwen2.5-VL-3B and employ our CapRL framework, resulting in the powerful CapRL-3B model as the captioner.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 12, \"type\": \"text\", \"bbox\": [436, 2313, 2103, 2412], \"angle\": 0, \"content\": \"To thoroughly evaluate the quality of the CapRL-5M dataset, we conduct comprehensive comparisons with widely used caption datasets from the open-source community.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 13, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2854], \"angle\": 0, \"content\": \"Implementation Details. In our setup, the language model is initialized with a pretrained LLM, the visual encoder with a pretrained ViT, and the MLP projector randomly, following a standard multimodal pretraining scheme. We conduct experiments under three settings: Qwen2.5-3B + Qwen2.5-ViT, Qwen2.5-7B + Qwen2.5-ViT, and InternLM2.5-7B + CLIP-ViT-L. Training follows the ShareGPT4V paradigm in three stages: Initial Alignment with BLIP-558K dataset (Li et al. 2022); Further Pretraining with diverse high-quality image-caption datasets; and SFT with Open-LLaVA-NeXT-1M (Chen & Xing 2024). For comparison, we adopt strong baselines including Vanilla, which skips Further Pretraining, ShareGPT4V-1M, DenseFusion-1M, and CapRL-1M (randomly sampled from CapRL-5M). Detailed training details are provided in Appendix F\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 14, \"type\": \"text\", \"bbox\": [436, 2864, 2108, 3055], \"angle\": 0, \"content\": \"Main Results. As shown in Table 1 when using CapRL-1M as the further pretraining dataset, performance on the vast majority of benchmarks surpasses both ShareGPT4V-1M and DenseFusion-1M. Specifically, under the Qwen2.5-3B + Qwen2.5-ViT setting, it exceeds DenseFusion-1M by \\\\(6.8\\\\%\\\\) on InfoVQA, and outperforms by \\\\(2.7\\\\%\\\\) and \\\\(3.6\\\\%\\\\) on DocVQA and ChartVQA. These remarkable\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 2, \"type\": \"table\", \"bbox\": [453, 478, 2088, 1240], \"angle\": 0, \"content\": \"<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-1M</td><td>56.2</td><td>87.3</td><td>78.0</td><td>55.1</td><td>45.5</td><td>62.0</td><td>30.3</td><td>70.5</td><td>47.0</td><td>50.0</td><td>72.9</td><td>61.6</td><td>59.7</td></tr><tr><td>CapRL-5M</td><td>61.5</td><td>90.0</td><td>80.5</td><td>57.6</td><td>48.1</td><td>63.2</td><td>30.9</td><td>73.1</td><td>50.4</td><td>52.6</td><td>74.7</td><td>62.6</td><td>62.0</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-7B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>47.6</td><td>83.7</td><td>77.1</td><td>55.9</td><td>47.4</td><td>60.4</td><td>29.4</td><td>72.1</td><td>48.1</td><td>47.1</td><td>72.4</td><td>62.7</td><td>58.7</td></tr><tr><td>ShareGPT4V-1M</td><td>49.8</td><td>85.1</td><td>75.7</td><td>56.8</td><td>46.6</td><td>60.9</td><td>31.8</td><td>71.9</td><td>48.4</td><td>45.9</td><td>72.2</td><td>62.7</td><td>59.0</td></tr><tr><td>DenseFusion-1M</td><td>53.5</td><td>87.8</td><td>76.7</td><td>58.6</td><td>46.3</td><td>61.0</td><td>31.1</td><td>72.6</td><td>48.6</td><td>49.7</td><td>72.5</td><td>63.1</td><td>60.2</td></tr><tr><td>CapRL-1M</td><td>59.9</td><td>89.5</td><td>80.6</td><td>58.9</td><td>50.4</td><td>63.1</td><td>32.2</td><td>72.1</td><td>51.3</td><td>50.5</td><td>75.3</td><td>63.2</td><td>62.2</td></tr><tr><td>CapRL-5M</td><td>63.4</td><td>91.4</td><td>81.5</td><td>61.4</td><td>50.8</td><td>63.2</td><td>34.9</td><td>72.7</td><td>52.6</td><td>52.6</td><td>76.9</td><td>63.8</td><td>63.8</td></tr><tr><td colspan=\\\"14\\\">InternLM2.5-7B + CLIP-ViT-L</td></tr><tr><td>Vanilla</td><td>37.4</td><td>73.2</td><td>68.7</td><td>56.9</td><td>44.2</td><td>58.2</td><td>30.7</td><td>70.7</td><td>47.0</td><td>43.1</td><td>71.8</td><td>64.9</td><td>55.6</td></tr><tr><td>ShareGPT4V-1M</td><td>38.9</td><td>73.8</td><td>69.8</td><td>56.3</td><td>44.8</td><td>59.9</td><td>33.2</td><td>72.6</td><td>46.2</td><td>43.3</td><td>72.7</td><td>65.0</td><td>56.4</td></tr><tr><td>DenseFusion-1M</td><td>39.3</td><td>76.4</td><td>70.8</td><td>59.7</td><td>44.5</td><td>60.3</td><td>34.1</td><td>72.2</td><td>47.9</td><td>44.0</td><td>73.7</td><td>65.5</td><td>57.4</td></tr><tr><td>CapRL-1M</td><td>43.3</td><td>80.0</td><td>75.8</td><td>58.0</td><td>49.6</td><td>62.8</td><td>34.1</td><td>73.4</td><td>50.2</td><td>46.6</td><td>76.0</td><td>65.8</td><td>59.6</td></tr><tr><td>CapRL-5M</td><td>47.0</td><td>83.5</td><td>77.7</td><td>59.7</td><td>50.4</td><td>63.5</td><td>38.9</td><td>73.7</td><td>53.3</td><td>54.3</td><td>77.6</td><td>66.3</td><td>62.2</td></tr></table>\", \"caption\": \"Table 2: Ablation on image sources. We annotate the images in ShareGPT4V-1M and DenseFusion-1M using CapRL-3B, and use them respectively as pretraining datasets for comparison.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 4, \"type\": \"table\", \"bbox\": [479, 1356, 2057, 1719], \"angle\": 0, \"content\": \"<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>CapRL-ShareGPT4V-1M</td><td>52.1</td><td>85.9</td><td>75.2</td><td>56.3</td><td>45.6</td><td>60.0</td><td>30.9</td><td>70.9</td><td>46.7</td><td>47.5</td><td>71.4</td><td>61.7</td><td>58.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-DenseFusion-1M</td><td>55.0</td><td>87.8</td><td>77.5</td><td>56.2</td><td>44.7</td><td>62.8</td><td>32.0</td><td>71.0</td><td>46.6</td><td>49.9</td><td>72.7</td><td>62.3</td><td>59.9</td></tr></table>\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 5, \"type\": \"text\", \"bbox\": [433, 1739, 2103, 2065], \"angle\": 0, \"content\": \"results indicate that CapRL-3B is effective for domains such as documents, charts, and infographics, which demand fine-grained perception and structured description. The captions in CapRL-1M are highly detailed and accurate for such image types, enabling LVLMs to achieve better modality alignment and a deeper understanding of the corresponding visual features. In addition, on natural image benchmarks such as MMStar and MMBench, CapRL-1M surpasses ShareGPT4V-1M by \\\\(1.6\\\\%\\\\) and \\\\(1.8\\\\%\\\\), suggesting that training with CapRL-1M enables multimodal models to acquire richer world knowledge for interpreting objects and their attributes in natural images.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 6, \"type\": \"text\", \"bbox\": [433, 2085, 2103, 2319], \"angle\": 0, \"content\": \"CapRL-5M further demonstrates consistently superior performance across all 12 benchmarks. These results highlight the strong scaling properties of the CapRL-3B-annotated dataset: as the training data size expands from 1M to 5M, model performance continues to improve steadily. This phenomenon underscores the practical value of CapRL for multimodal pretraining, as it enables the construction of high-quality, scalable datasets at very low annotation cost.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 7, \"type\": \"text\", \"bbox\": [433, 2339, 2103, 2709], \"angle\": 0, \"content\": \"Ablations about Image Sources. In the previous comparisons, the images used in each dataset are not identical. To better control for this variable, we fix the set of images and instead compare the effect of caption quality of different datasets under the Qwen2.5-3B + Qwen2.5-ViT setting. As shown in Table2 we compare CapRL with ShareGPT4V-1M and DenseFusion-1M. The results demonstrate that, when using the same set of images, further pretraining with the CapRL-3B-annotated dataset enables the LVLM to outperform the baselines by more than \\\\(2\\\\%\\\\). This finding indicates that the substantial advantage of the CapRL dataset over the baselines largely stems from the superior quality of its captions, rather than from differences in image diversity.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 8, \"type\": \"text\", \"bbox\": [433, 2729, 2108, 3055], \"angle\": 0, \"content\": \"Scaling Trend Comparison of Different Datasets. We further compare the scaling trend of CapRL and DenseFusion under Qwen2.5-3B + Qwen2.5-ViT setting. Specifically, we sample different numbers of image-caption pairs from each dataset for pretraining. As shown in Figure4 the CapRL dataset consistently outperforms the corresponding DenseFusion dataset across various scales of pretraining data. Moreover, the overall trend indicates that this performance gap continues to widen as the data size increases. This phenomenon highlights the strong scaling properties of the CapRL dataset, thanks to its high-quality captions, LVLMs continue to benefit as the dataset size grows.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 4, \"type\": \"image\", \"bbox\": [1685, 336, 2085, 660], \"angle\": 0, \"content\": null, \"caption\": \"Figure 4: The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 7, \"type\": \"table\", \"bbox\": [443, 933, 2096, 1250], \"angle\": 0, \"content\": \"<table><tr><td>Caption\\nModel</td><td>GRPO\\nTrained</td><td>Chart\\nQA</td><td>ChartQA\\nPro</td><td>Info\\nVQA</td><td>MMMU\\nPro</td><td>Math\\nVerse</td><td>Char\\nXiv</td><td>We\\nMath</td><td>Math\\nVision</td><td>MMStar</td><td>SEED</td><td>MMMU</td><td>Average</td></tr><tr><td>Qwen2.5-VL-3B</td><td>X</td><td>65.6</td><td>27.1</td><td>40.2</td><td>28.6</td><td>32.8</td><td>21.8</td><td>54.4</td><td>22.6</td><td>46.4</td><td>64.1</td><td>35.1</td><td>39.9</td></tr><tr><td>Qwen2.5-VL-7B</td><td>X</td><td>74.9</td><td>35.4</td><td>56.4</td><td>30.1</td><td>36.4</td><td>24.8</td><td>57.0</td><td>23.3</td><td>50.7</td><td>67.1</td><td>37.9</td><td>44.9</td></tr><tr><td>Qwen2.5-VL-72B</td><td>X</td><td>80.2</td><td>38.0</td><td>60.8</td><td>34.1</td><td>39.9</td><td>30.7</td><td>60.2</td><td>24.5</td><td>55.0</td><td>69.3</td><td>39.4</td><td>48.3</td></tr><tr><td>UnifiedRW-as-Judge-3B</td><td>✓</td><td>54.9</td><td>25.1</td><td>33.6</td><td>28.1</td><td>34.6</td><td>20.4</td><td>58.2</td><td>24.5</td><td>45.4</td><td>61.2</td><td>36.3</td><td>38.4</td></tr><tr><td>Qwen2.5VL-as-Judge-3B</td><td>✓</td><td>71.4</td><td>34.2</td><td>49.3</td><td>29.1</td><td>33.8</td><td>22.9</td><td>54.3</td><td>24.1</td><td>47.7</td><td>64.5</td><td>36.4</td><td>42.5</td></tr><tr><td>CapRL-3B</td><td>✓</td><td>80.5</td><td>39.9</td><td>64.8</td><td>30.7</td><td>36.4</td><td>32.4</td><td>60.1</td><td>23.4</td><td>55.0</td><td>70.6</td><td>38.1</td><td>48.3</td></tr></table>\", \"caption\": \"Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-as-a-Judge as the reward. The best results are bold and the second-best results are underlined.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 9, \"type\": \"image\", \"bbox\": [1537, 1287, 2101, 1640], \"angle\": 0, \"content\": null, \"caption\": \"Figure 5: (Left) CapRL demonstrates strong generalization even when trained on images from a single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability. (Right) CapRL demonstrates promising scaling performance on QA training datasets.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 1993, 2103, 2187], \"angle\": 0, \"content\": \"In the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al. 2024), and compare our CapRL-3B against other captioning models.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 14, \"type\": \"text\", \"bbox\": [433, 2636, 2108, 3052], \"angle\": 0, \"content\": \"Comparison with Qwen2.5-VL series. As shown in Table3 CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model. In chart and infographic understanding, CapRL-3B surpasses Qwen2.5-VL-3B by \\\\(14.9\\\\%\\\\), \\\\(12.8\\\\%\\\\), and \\\\(24.6\\\\%\\\\) on ChartQA, ChartQAPro, and InfoVQA, respectively. For natural image understanding, it leads Qwen2.5-VL-3B by \\\\(9.6\\\\%\\\\) and \\\\(6.5\\\\%\\\\) on MMStar and SEED. These results demonstrate that GRPO training has substantially unlocked the potential of Qwen2.5-VL-3B, enabling it to fully leverage its inherent knowledge to organize all objects and their attributes within an image into comprehensive and detailed captions. As a result, its perception capability is pushed to the limit, reaching a level comparable to that of the 72B model.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 2, \"type\": \"table\", \"bbox\": [443, 386, 1275, 640], \"angle\": 0, \"content\": \"<table><tr><td>Caption \\nModel</td><td>ChartQA \\nPro</td><td>Info \\nVQA</td><td>MMMU</td><td>MMStar</td><td>WeMath</td><td>Avg</td></tr><tr><td>Qwen2.5-VL-3B</td><td>27.1</td><td>40.2</td><td>35.1</td><td>46.4</td><td>54.4</td><td>40.6</td></tr><tr><td>CapRL-1QA-20k</td><td>35.5</td><td>59.8</td><td>36.6</td><td>50.8</td><td>57.3</td><td>48.0</td></tr><tr><td>CapRL-2QA-20k</td><td>36.8</td><td>60.2</td><td>37.6</td><td>51.1</td><td>56.6</td><td>48.5</td></tr><tr><td>CapRL-3QA-20k</td><td>36.9</td><td>60.3</td><td>36.9</td><td>51.3</td><td>56.8</td><td>48.5</td></tr></table>\", \"caption\": \"Table 5: Ablations about Sampling Rounds N.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 699, 2108, 1349], \"angle\": 0, \"content\": \"Comparison with LVLM-as-a-Judge reward. In our comparison with other reward design methods, we observe that when using UnifiedReward-2.0-qwen-3b as the judge to evaluate caption quality, the model's captioning ability actually deteriorates during GRPO training. We attribute this to the severe bias present in UnifiedReward-2.0-qwen-3b: during its training, it was exposed to lots of captions from text-to-image datasets, which are typically short and only describe the main objects. As a result, the UnifiedReward model tends to favor shorter captions. As shown in Fig.1 the average caption length during training continuously decreases and eventually collapses to producing only “:description”. Conversely, when using Qwen2.5-VL-3B as the judge, the bias is in the opposite direction: it prefers overly verbose captions. This makes the policy model prone to exploiting the bias by generating long passages of content irrelevant to the image, thereby satisfying the judge model's preference. As shown in Table3 the captioning ability under this reward shows significantly inferior to CapRL. Specific examples of such cases are illustrated in Fig.9 Fig.10 Fig.11 All these observations highlight that LVLM-as-a-Judge reward is fundamentally unreliable. This further underscores the advantage of CapRL, which converts subjective evaluations into objective assessments.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1465, 1377, 1511], \"angle\": 0, \"content\": \"4.3 COMPREHENSIVE DISCUSSION ABOUT CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 1577, 2103, 1673], \"angle\": 0, \"content\": \"In this section, we provide a comprehensive analysis and discussion of CapRL. These results further confirm CapRL's general applicability, robustness, and effectiveness.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 8, \"type\": \"text\", \"bbox\": [436, 1692, 2108, 2065], \"angle\": 0, \"content\": \"CapRL demonstrates strong generalization even when trained on images from a single domain. We further investigate the effect of different image sources in the QA dataset used for GRPO training. To this end, we classify the images into two categories using Qwen2.5-VL-3B: (1) documents, charts, or infographics, and (2) natural images. From each category, we sample 20k images for comparison. As illustrated in Fig. 5 (Left), models trained exclusively on chart-type images via GRPO exhibit substantial gains over Qwen2.5-VL-3B, not only in document and chart understanding but also in general VQA tasks. This demonstrates the strong generalization of CapRL-induced captioning improvements beyond the domains encountered during training.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 9, \"type\": \"text\", \"bbox\": [436, 2085, 2103, 2412], \"angle\": 0, \"content\": \"CapRL demonstrates promising scaling performance on training data. We conduct training on different amounts of QA data to evaluate the scaling behavior. As shown in Fig. [5] (Right), the model's performance improves steadily as the amount of QA data increases. These results indicate that our CapRL framework exhibits highly promising scaling potential. With the continued expansion of the training data, the captioning ability can be further enhanced, unlocking additional potential of Qwen2.5-VL-3B. Given its relatively small parameter size and excellent scaling properties, this approach holds strong promise for application in industrial-scale multimodal pretraining.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 10, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2801], \"angle\": 0, \"content\": \"Sparse QA supervision is sufficient for CapRL. We further examine the effect of varying the number of QA pairs per image. Specifically, we randomly sample 20k images that retain three QA pairs after filtering, obtain CapRL-3QA-20k after training. By controlling the number of QA pairs per image, we also construct CapRL-1QA-20k and CapRL-2QA-20k. The results, presented in Table4 show that even with only a single QA pair per image, Qwen2.5-VL-3B achieves a substantial improvement in captioning performance, averaging 7.4% higher than the baseline and only 0.5% lower than CapRL-2QA-20k. This highlights the remarkable efficiency of CapRL: highly sparse QA supervision is sufficient to unlock significant gains in captioning ability.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 9, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 438, 2111, 719], \"angle\": 0, \"content\": \"In this work, we introduce CapRL, a novel framework that successfully applies RLVR to the subjective task of image captioning. By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal. Our results show that CapRL effectively encourages models to generate dense and precise image descriptions, which in turn substantially promote modality alignment in LVLM pretraining. This work marks a significant step away from the restrictive, data-hungry SFT paradigm for RLVR in open-ended tasks.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 1, \"type\": \"title\", \"bbox\": [441, 339, 854, 386], \"angle\": 0, \"content\": \"A CAPRL CASES\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 452, 2108, 544], \"angle\": 0, \"content\": \"We provide further illustrative examples of CapRL-3B to highlight its surprising captioning capabilities in this section.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 3, \"type\": \"text\", \"bbox\": [436, 567, 2103, 851], \"angle\": 0, \"content\": \"Comparison with Qwen2.5-VL-3B. As illustrated in Fig. 6 CapRL-3B demonstrates remarkable capability in understanding infographics, providing information that is both comprehensive and accurate. In contrast, Qwen2.5-VL-3B, as shown in Fig. 7 makes numerous errors in identifying key information within infographics. Furthermore, Fig. 8 highlights that CapRL-3B achieves substantially higher accuracy in chart understanding compared to Qwen2.5-VL-3B. Similarly, in the case of physical image understanding in Fig. 12 CapRL-3B also demonstrates clear superiority.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 1211, 2113, 1310], \"angle\": 0, \"content\": \"More cases of CapRL-3B in understanding infographics and natural images. Fig. 13 and Fig. 14 provide additional evidence of the impressive perceptual capacity demonstrated by CapRL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1389, 1545, 1438], \"angle\": 0, \"content\": \"B MORE ANALYSIS EXPERIMENTS ABOUT CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 12, \"type\": \"text\", \"bbox\": [425, 2253, 2103, 2448], \"angle\": 0, \"content\": \"We provide all the prompts employed in our experiments in this section. Specifically, the prompt used in CapRL for guiding the LLM to answer questions conditioned on captions is illustrated in Figure 15; the prompt for utilizing the Unified Reward Model as the reward model is shown in Figure 16, and the prompt for adopting Qwen2.5-VL-3B as the reward model is presented in Figure 17\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 14, \"type\": \"text\", \"bbox\": [436, 2636, 2108, 3055], \"angle\": 0, \"content\": \"To ensure both quality and safety, our data processing pipeline consists of three main stages. First, inspired by SemDeDup (Abbas et al. 2023), we construct clusters to identify and remove images with redundant semantics. During this step, we also discard low-resolution and overly simple images, while filtering out content that involves violence, pornography, or other safety concerns. Second, to avoid benchmark leakage, we integrate the images used in commonly referenced evaluation datasets and form clusters with them. Any images from our collection that are overly similar to benchmark samples are eliminated. Third, we conduct a safety inspection through human verification. Annotators perform sample-based screening, and once the proportion of unsafe images falls to a negligible level, we stop filtering. Following this process, we obtain the final dataset, CapRL-5M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 14, \"block_id\": 1, \"type\": \"title\", \"bbox\": [466, 356, 650, 396], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 14, \"block_id\": 43, \"type\": \"image_caption\", \"bbox\": [510, 2333, 2029, 2385], \"angle\": 0, \"content\": \"Figure 6: An illustrative example of CapRL-3B applied to infographic understanding.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 1, \"type\": \"title\", \"bbox\": [487, 372, 668, 412], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 106, \"type\": \"image_caption\", \"bbox\": [510, 2742, 2027, 2795], \"angle\": 0, \"content\": \"Figure 8: Chart understanding comparison between CapRL-3B and Qwen2.5-VL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 1, \"type\": \"title\", \"bbox\": [484, 359, 668, 396], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 43, \"type\": \"image_caption\", \"bbox\": [785, 2517, 1754, 2570], \"angle\": 0, \"content\": \"Figure 9: Example of CapRL-3B chart understanding.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 45, \"type\": \"text\", \"bbox\": [436, 2818, 2108, 3055], \"angle\": 0, \"content\": \"Baselines. We selected several strong baselines for comparison. (1) Vanilla, which skips the Further Pretraining stage and only goes through the first and third stages. Additionally, we constructed two more baselines by varying the dataset used in the Further Pretraining stage: (2) ShareGPT4V-1M, and (3) DenseFusion-1M. To ensure a fair comparison by controlling the number of samples, we randomly sampled 1 million image-caption pairs from the 5M dataset to form CapRL-1M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 21, \"block_id\": 8, \"type\": \"title\", \"bbox\": [1476, 1376, 1624, 1402], \"angle\": 0, \"content\": \"After CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 21, \"block_id\": 16, \"type\": \"image_caption\", \"bbox\": [530, 2036, 2009, 2092], \"angle\": 0, \"content\": \"Figure 12: Example of CapRL enhancing the captioning ability of Qwen2.5-VL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 22, \"block_id\": 1, \"type\": \"title\", \"bbox\": [476, 653, 660, 689], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 22, \"block_id\": 101, \"type\": \"image\", \"bbox\": [1981, 1587, 2062, 1676], \"angle\": 0, \"content\": null, \"caption\": \"Figure 13: An illustrative example of CapRL applied to infographic understanding.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 23, \"block_id\": 1, \"type\": \"title\", \"bbox\": [469, 683, 650, 722], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 23, \"block_id\": 27, \"type\": \"image_caption\", \"bbox\": [510, 2669, 2029, 2722], \"angle\": 0, \"content\": \"Figure 14: An illustrative example of CapRL applied to natural image understanding.\", \"caption\": \"\"}], \"message\": \"Found 62 matches for keyword 'CapRL'.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_RHLM4wCo0nTkfT5A2bBnRGui", "function": {"arguments": "{\"keyword\":\"vision-free\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_RHLM4wCo0nTkfT5A2bBnRGui", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"vision-free\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 2432, 2103, 2620], \"angle\": 0, \"content\": \"2) We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task. We also created a specific QA curation pipeline to ensure the quality of the questions and answers used for the second stage.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 9, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 438, 2111, 719], \"angle\": 0, \"content\": \"In this work, we introduce CapRL, a novel framework that successfully applies RLVR to the subjective task of image captioning. By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal. Our results show that CapRL effectively encourages models to generate dense and precise image descriptions, which in turn substantially promote modality alignment in LVLM pretraining. This work marks a significant step away from the restrictive, data-hungry SFT paradigm for RLVR in open-ended tasks.\", \"caption\": \"\"}], \"message\": \"Found 3 matches for keyword 'vision-free'.\"}"}, {"content": "Yes, CapRL uses a vision-free LLM (Large Language Model without visual input capabilities) to evaluate caption quality in its pipeline. Specifically, CapRL employs a decoupled two-stage process: first, an LVLM (Large Vision-Language Model) generates captions for images; then, these captions are passed to a separate vision-free LLM, which answers multiple-choice questions about the image using only the caption. The accuracy of the vision-free LLM on these questions is used as the objective reward to assess and improve caption quality.\n\nThis design is explicitly stated in the paper:\n- \"CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption.\" (page 1)\n- \"The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task.\" (page 3)\n- \"By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal.\" (page 9)\n\nThus, CapRL’s pipeline explicitly uses a vision-free LLM as the judge of caption quality.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}], "success": true, "error": null}
{"task_id": "004", "question": "In document 'test_pdf', Does the CapRL framework adopt a single-stage or two-stage pipeline?", "answer": "The CapRL framework adopts a two-stage pipeline.\n\nEvidence from the document 'test_pdf':\n- The description says: \"CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption.\"\n- Figure 1(b) and further text detail that CapRL uses a two-stage VQA approach for subjective reward generation.\n- Section 2) clearly states: \"We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task.\"\n- Figure 3 (caption): \"Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process.\"\n\nThus, CapRL does NOT use a single-stage approach—it uses a decoupled, two-stage pipeline.", "messages": [{"role": "developer", "content": "You are a helpful assistant. You need to use tools to solve the problem.\n\n## Available Tools\n\n- doc_check_tool: Checks if a document (by base name, e.g., report_1) or all PDFs have been converted to structured JSON (OCR) format. THIS MUST BE THE FIRST TOOL CALLED FOR ANY NEW DOCUMENT QUERY.\n- doc_ocr_tool: Converts a raw PDF file into a structured JSON file. ONLY use this immediately AFTER doc_check_tool returns that a file is NOT processed.\n- search_keyword_tool: Searches for a keyword in the structured JSON content of processed documents. ONLY call this tool AFTER confirming the file is processed via doc_check_tool (and running doc_ocr_tool if necessary).\n- get_page_ocr_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and returns the full, concatenated OCR text content of that entire page.\n- get_page_image_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and extracts the full image of that entire page, saving it as a PNG file in the temp directory. Returns the path to the saved image.\n- crop_image_tool: Crops a specific figure, table, or chart from the PDF using the location data (bbox and angle) obtained from the search_keyword_tool. Returns the local file path to the high-resolution, upright PNG image, saved in the temp directory.\n- image_description_tool: Uses a multimodal model (VLM) to analyze a local image file (PNG/JPG) and generate a detailed text description. Use after crop_image_tool or get_page_image_tool to interpret visual content.\n\n## Tool Usage Strategy\n\n**For Multi-Step Analysis:**\n1. Break complex problems into logical steps\n2. Use ONE tool at a time to gather information\n3. Verify findings through different approaches when possible\n"}, {"role": "user", "content": "In document 'test_pdf', Does the CapRL framework adopt a single-stage or two-stage pipeline?"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_94OScCM5ySH4Q0HHvdDFAWCZ", "function": {"arguments": "{\"file_name_or_all\":\"test_pdf\"}", "name": "doc_check_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_94OScCM5ySH4Q0HHvdDFAWCZ", "name": "doc_check_tool", "content": "{\"status\": true, \"files_to_process\": [], \"message\": \"Found 0 files needing OCR processing.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_vwsvV8eiwAWWjOmAqTmr8f8l", "function": {"arguments": "{\"keyword\":\"CapRL\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_vwsvV8eiwAWWjOmAqTmr8f8l", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"CapRL\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 1, \"type\": \"title\", \"bbox\": [438, 326, 2116, 485], \"angle\": 0, \"content\": \"CAPRL: STIMULATING DENSE IMAGE CAPTION CAPABILITIES VIA REINFORCEMENT LEARNING\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 6, \"type\": \"text\", \"bbox\": [469, 798, 1440, 844], \"angle\": 0, \"content\": \"Model & Data: CapRL HuggingFace Collection\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 7, \"type\": \"text\", \"bbox\": [469, 848, 1147, 894], \"angle\": 0, \"content\": \"Code: CapRL Github Repository\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 1, \"block_id\": 1, \"type\": \"image\", \"bbox\": [443, 333, 2101, 1518], \"angle\": 0, \"content\": null, \"caption\": \"Figure 1: (a) Existing Reward Models: Current LVLM-as-a-judge/reward models suffer from limitations like rewarding morbidity or brevity, leading to low-quality captions and reward hacking. (b) Our CapRL: CapRL uses a decoupled two-stage VQA approach to provide subjective rewards for captions. (c) CapRL's Advantage: CapRL outperforms previous subjective reward methods, as shown by training curves and higher performance in the Prism (Qiao et al. 2024) evaluation setting. problem, where a single image can be accurately described by a wide variety of captions. Since SFT models are trained to match a single ground-truth description for each image, they tend to memorize specific answers rather than learning the underlying concepts. As a result, the SFT models become less general and struggle to generate the diverse range of valid captions possible for a single image.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 1, \"type\": \"image\", \"bbox\": [446, 336, 2085, 1029], \"angle\": 0, \"content\": null, \"caption\": \"Figure 2: Illustration of the captioning capability improvement CapRL brings to Qwen2.5-VL-3B.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 3, \"type\": \"text\", \"bbox\": [433, 1098, 2108, 1653], \"angle\": 0, \"content\": \"is detailed and accurate, a text-based LLM that can't directly \\\"see\\\" the image can still answer Visual Question Answering (VQA) questions about the image. For example, for the question \\\"What color is the frisbee?\\\", the LLM finds the phrase \\\"red frisbee\\\" in the caption and correctly answers \\\"red.\\\" Driven by this motivation, we present an effective decoupled two-stage pipeline, dubbed as Captioning Reinforcement Learning (CapRL), as shown in Fig. 1(b). Specifically, the reward of our CapRL framework is determined by how well a caption generated by an LVLM enables a separate non-visual LLM to answer Multiple-Choice Questions (MCQs) about the source image. The LLM's resulting accuracy serves as the objective reward for the RLVR training. To ensure the high-quality MCQs data that present enough knowledge required for VQA has been examined, we also developed a specific QA curation pipeline. The images are sampled from various sources, including natural images, charts, and documents. The questions and answers are filtered to ensure the questions can only be answered by analyzing the image content itself.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 4, \"type\": \"text\", \"bbox\": [436, 1673, 2111, 2138], \"angle\": 0, \"content\": \"We conduct a comprehensive evaluation of the significant benefits brought by CapRL. From a qualitative perspective, as shown in Fig. 2 applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From a quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al. 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of \\\\(8.4\\\\%\\\\). These results demonstrate that our CapRL framework, by leveraging objective reward design as a reliable optimization signal, effectively drives the model to produce dense and accurate captions.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 2432, 2103, 2620], \"angle\": 0, \"content\": \"2) We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task. We also created a specific QA curation pipeline to ensure the quality of the questions and answers used for the second stage.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 8, \"type\": \"text\", \"bbox\": [436, 2640, 2103, 2781], \"angle\": 0, \"content\": \"3) We carry out extensive experiments to verify the effectiveness of CapRL. Notably, both in the LVLM Pretraining setting for modality alignment and the Prism setting for caption informativeness evaluation, CapRL consistently exhibits superior performance compared to the baselines.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 1, \"type\": \"image\", \"bbox\": [443, 339, 2101, 1072], \"angle\": 0, \"content\": null, \"caption\": \"Figure 3: Overview of CapRL. Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process. Captions generated by the LVLM, paired with curated MCQs (b), are used to query an LLM, whose resulting accuracy becomes the objective reward for the LVLM (a). Our CapRL offers a scalable framework for applying RLVR to the open-ended image captioning task.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 3, \"type\": \"text\", \"bbox\": [436, 1313, 2103, 1732], \"angle\": 0, \"content\": \"captions, researchers design advanced captioning pipelines: BLIP-LAION (Li et al. 2022) generates short synthetic captions, LaCLIP (Fan et al. 2023) uses ChatGPT to rewrite them, and CapsFusion (Yu et al. 2024) consolidates and refine information with fine-tuned models. Besides, there are many research projects which use GPT-4V and human-in-the-loop pipelines to produce richer, fine-grained annotations such as ShareGPT4V (Chen et al. 2024b) and ALLaVA (Chen et al. 2024a). Recent studies (Li et al. 2024b Sun et al. 2024) have explored multi-expert approaches to compensate for LVLM limitations. In summary, some works rely on complex pipelines with multiple models, training-free but costly at inference, while others require lots of expensive labeled data for SFT. In contrast, our CapRL achieves strong performance with remarkable data efficiency through RLVR.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 6, \"type\": \"text\", \"bbox\": [436, 2385, 2103, 2626], \"angle\": 0, \"content\": \"An overview of our CapRL is shown in Fig.3 The CapRL framework consists of a novel, decoupled two-stage process. In the first stage, an LVLM generates a caption for an input image. In the second stage, this caption, along with a series of MCQs, is provided as input to an LLM. In the following, we will describe how to apply RLVR on the image captioning task via our CapRL in Section 3.1 Then we use the model trained with CapRL to construct the CapRL-5M dataset in Section 3.2\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 3, \"block_id\": 7, \"type\": \"title\", \"bbox\": [441, 2682, 685, 2729], \"angle\": 0, \"content\": \"3.1 CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 4, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 458, 2108, 877], \"angle\": 0, \"content\": \"To design a reliable verifiable reward module, we leverage a perception-reasoning decoupled VQA task as a proxy to evaluate the quality of captions. The overall process of our proposed method CapRL, is illustrated in Fig. During the GRPO training process, an image and an instruction are first provided as input to the policy model to sample a set of candidate captions. Each caption is then paired with corresponding questions and fed to a Large Language Model (LLM). We assign each caption a reward score based on the accuracy of answers generated by the LLM. Subsequently, we calculate the mean and variance of rewards across the group to derive the advantage for each caption. To ensure training stability, and consistent with the original GRPO framework, we incorporate a KL-divergence penalty. The policy model is then updated via policy gradient optimization.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 4, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 2682, 2108, 3055], \"angle\": 0, \"content\": \"QA Curation. To train CapRL effectively, a high-quality VQA dataset \\\\((q, a)\\\\) with question \\\\(q\\\\) and answer \\\\(a\\\\) is required to provide reliable reward signals. We construct this VQA dataset using a structured three-stage curation pipeline. (1) Image Collection. We begin by sourcing diverse images from the web and existing open-source datasets, including natural scenes, charts, and documents, to maximize variety. (2) QA Generation. For each image, we then use Qwen2.5-VL-72B (Bai et al. 2025) to automatically generate multiple question-answer pairs. (3) QA Filtering. Finally, we implement a stringent QA filtering process to ensure the quality of the generated QA pairs. The QA filtering stage is to verify that all questions are strictly visually-grounded and answerable exclusively\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 990, 2103, 1092], \"angle\": 0, \"content\": \"After filtering, we retain approximately 75k images along with their corresponding QA pairs to train the CapRL captioning model. Please refer to Appendix C and E for the curation details.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1138, 938, 1184], \"angle\": 0, \"content\": \"3.2 CAPRL-5M DATASET\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 1227, 2103, 1326], \"angle\": 0, \"content\": \"By employing our carefully designed CapRL training scheme, we obtained CapRL-3B, and further used this powerful captioner to annotate 5M images, ultimately forming CapRL-5M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 9, \"type\": \"text\", \"bbox\": [436, 1824, 2106, 2059], \"angle\": 0, \"content\": \"Caption Model selection. In typical multimodal pretraining scenarios, the pretraining dataset often requires a massive number of image-text pairs, making annotation costs substantial. Considering practical applications, we decide to train a highly lightweight yet powerful captioner to keep annotation costs more acceptable. Specifically, we initialize the policy model with Qwen2.5-VL-3B and employ our CapRL framework, resulting in the powerful CapRL-3B model as the captioner.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 12, \"type\": \"text\", \"bbox\": [436, 2313, 2103, 2412], \"angle\": 0, \"content\": \"To thoroughly evaluate the quality of the CapRL-5M dataset, we conduct comprehensive comparisons with widely used caption datasets from the open-source community.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 13, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2854], \"angle\": 0, \"content\": \"Implementation Details. In our setup, the language model is initialized with a pretrained LLM, the visual encoder with a pretrained ViT, and the MLP projector randomly, following a standard multimodal pretraining scheme. We conduct experiments under three settings: Qwen2.5-3B + Qwen2.5-ViT, Qwen2.5-7B + Qwen2.5-ViT, and InternLM2.5-7B + CLIP-ViT-L. Training follows the ShareGPT4V paradigm in three stages: Initial Alignment with BLIP-558K dataset (Li et al. 2022); Further Pretraining with diverse high-quality image-caption datasets; and SFT with Open-LLaVA-NeXT-1M (Chen & Xing 2024). For comparison, we adopt strong baselines including Vanilla, which skips Further Pretraining, ShareGPT4V-1M, DenseFusion-1M, and CapRL-1M (randomly sampled from CapRL-5M). Detailed training details are provided in Appendix F\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 5, \"block_id\": 14, \"type\": \"text\", \"bbox\": [436, 2864, 2108, 3055], \"angle\": 0, \"content\": \"Main Results. As shown in Table 1 when using CapRL-1M as the further pretraining dataset, performance on the vast majority of benchmarks surpasses both ShareGPT4V-1M and DenseFusion-1M. Specifically, under the Qwen2.5-3B + Qwen2.5-ViT setting, it exceeds DenseFusion-1M by \\\\(6.8\\\\%\\\\) on InfoVQA, and outperforms by \\\\(2.7\\\\%\\\\) and \\\\(3.6\\\\%\\\\) on DocVQA and ChartVQA. These remarkable\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 2, \"type\": \"table\", \"bbox\": [453, 478, 2088, 1240], \"angle\": 0, \"content\": \"<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-1M</td><td>56.2</td><td>87.3</td><td>78.0</td><td>55.1</td><td>45.5</td><td>62.0</td><td>30.3</td><td>70.5</td><td>47.0</td><td>50.0</td><td>72.9</td><td>61.6</td><td>59.7</td></tr><tr><td>CapRL-5M</td><td>61.5</td><td>90.0</td><td>80.5</td><td>57.6</td><td>48.1</td><td>63.2</td><td>30.9</td><td>73.1</td><td>50.4</td><td>52.6</td><td>74.7</td><td>62.6</td><td>62.0</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-7B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>47.6</td><td>83.7</td><td>77.1</td><td>55.9</td><td>47.4</td><td>60.4</td><td>29.4</td><td>72.1</td><td>48.1</td><td>47.1</td><td>72.4</td><td>62.7</td><td>58.7</td></tr><tr><td>ShareGPT4V-1M</td><td>49.8</td><td>85.1</td><td>75.7</td><td>56.8</td><td>46.6</td><td>60.9</td><td>31.8</td><td>71.9</td><td>48.4</td><td>45.9</td><td>72.2</td><td>62.7</td><td>59.0</td></tr><tr><td>DenseFusion-1M</td><td>53.5</td><td>87.8</td><td>76.7</td><td>58.6</td><td>46.3</td><td>61.0</td><td>31.1</td><td>72.6</td><td>48.6</td><td>49.7</td><td>72.5</td><td>63.1</td><td>60.2</td></tr><tr><td>CapRL-1M</td><td>59.9</td><td>89.5</td><td>80.6</td><td>58.9</td><td>50.4</td><td>63.1</td><td>32.2</td><td>72.1</td><td>51.3</td><td>50.5</td><td>75.3</td><td>63.2</td><td>62.2</td></tr><tr><td>CapRL-5M</td><td>63.4</td><td>91.4</td><td>81.5</td><td>61.4</td><td>50.8</td><td>63.2</td><td>34.9</td><td>72.7</td><td>52.6</td><td>52.6</td><td>76.9</td><td>63.8</td><td>63.8</td></tr><tr><td colspan=\\\"14\\\">InternLM2.5-7B + CLIP-ViT-L</td></tr><tr><td>Vanilla</td><td>37.4</td><td>73.2</td><td>68.7</td><td>56.9</td><td>44.2</td><td>58.2</td><td>30.7</td><td>70.7</td><td>47.0</td><td>43.1</td><td>71.8</td><td>64.9</td><td>55.6</td></tr><tr><td>ShareGPT4V-1M</td><td>38.9</td><td>73.8</td><td>69.8</td><td>56.3</td><td>44.8</td><td>59.9</td><td>33.2</td><td>72.6</td><td>46.2</td><td>43.3</td><td>72.7</td><td>65.0</td><td>56.4</td></tr><tr><td>DenseFusion-1M</td><td>39.3</td><td>76.4</td><td>70.8</td><td>59.7</td><td>44.5</td><td>60.3</td><td>34.1</td><td>72.2</td><td>47.9</td><td>44.0</td><td>73.7</td><td>65.5</td><td>57.4</td></tr><tr><td>CapRL-1M</td><td>43.3</td><td>80.0</td><td>75.8</td><td>58.0</td><td>49.6</td><td>62.8</td><td>34.1</td><td>73.4</td><td>50.2</td><td>46.6</td><td>76.0</td><td>65.8</td><td>59.6</td></tr><tr><td>CapRL-5M</td><td>47.0</td><td>83.5</td><td>77.7</td><td>59.7</td><td>50.4</td><td>63.5</td><td>38.9</td><td>73.7</td><td>53.3</td><td>54.3</td><td>77.6</td><td>66.3</td><td>62.2</td></tr></table>\", \"caption\": \"Table 2: Ablation on image sources. We annotate the images in ShareGPT4V-1M and DenseFusion-1M using CapRL-3B, and use them respectively as pretraining datasets for comparison.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 4, \"type\": \"table\", \"bbox\": [479, 1356, 2057, 1719], \"angle\": 0, \"content\": \"<table><tr><td>Pretraining Dataset</td><td>Info VQA</td><td>Doc VQA</td><td>Chart QA</td><td>Real WorldQA</td><td>Math Vista</td><td>SEED2 Plus</td><td>MME RW</td><td>MMB</td><td>MMStar</td><td>MMVet</td><td>AI2D</td><td>GQA</td><td>Average</td></tr><tr><td colspan=\\\"14\\\">Qwen2.5-3B + Qwen2.5-ViT</td></tr><tr><td>Vanilla</td><td>43.9</td><td>81.0</td><td>72.7</td><td>55.1</td><td>41.6</td><td>56.6</td><td>30.5</td><td>68.6</td><td>44.7</td><td>41.0</td><td>68.3</td><td>61.5</td><td>55.5</td></tr><tr><td>ShareGPT4V-1M</td><td>46.1</td><td>82.4</td><td>74.2</td><td>55.0</td><td>44.7</td><td>60.5</td><td>29.8</td><td>68.9</td><td>45.2</td><td>42.4</td><td>70.1</td><td>61.4</td><td>56.7</td></tr><tr><td>CapRL-ShareGPT4V-1M</td><td>52.1</td><td>85.9</td><td>75.2</td><td>56.3</td><td>45.6</td><td>60.0</td><td>30.9</td><td>70.9</td><td>46.7</td><td>47.5</td><td>71.4</td><td>61.7</td><td>58.7</td></tr><tr><td>DenseFusion-1M</td><td>49.4</td><td>84.6</td><td>74.4</td><td>54.1</td><td>44.6</td><td>59.1</td><td>30.7</td><td>69.0</td><td>45.6</td><td>40.2</td><td>70.4</td><td>62.5</td><td>57.1</td></tr><tr><td>CapRL-DenseFusion-1M</td><td>55.0</td><td>87.8</td><td>77.5</td><td>56.2</td><td>44.7</td><td>62.8</td><td>32.0</td><td>71.0</td><td>46.6</td><td>49.9</td><td>72.7</td><td>62.3</td><td>59.9</td></tr></table>\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 5, \"type\": \"text\", \"bbox\": [433, 1739, 2103, 2065], \"angle\": 0, \"content\": \"results indicate that CapRL-3B is effective for domains such as documents, charts, and infographics, which demand fine-grained perception and structured description. The captions in CapRL-1M are highly detailed and accurate for such image types, enabling LVLMs to achieve better modality alignment and a deeper understanding of the corresponding visual features. In addition, on natural image benchmarks such as MMStar and MMBench, CapRL-1M surpasses ShareGPT4V-1M by \\\\(1.6\\\\%\\\\) and \\\\(1.8\\\\%\\\\), suggesting that training with CapRL-1M enables multimodal models to acquire richer world knowledge for interpreting objects and their attributes in natural images.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 6, \"type\": \"text\", \"bbox\": [433, 2085, 2103, 2319], \"angle\": 0, \"content\": \"CapRL-5M further demonstrates consistently superior performance across all 12 benchmarks. These results highlight the strong scaling properties of the CapRL-3B-annotated dataset: as the training data size expands from 1M to 5M, model performance continues to improve steadily. This phenomenon underscores the practical value of CapRL for multimodal pretraining, as it enables the construction of high-quality, scalable datasets at very low annotation cost.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 7, \"type\": \"text\", \"bbox\": [433, 2339, 2103, 2709], \"angle\": 0, \"content\": \"Ablations about Image Sources. In the previous comparisons, the images used in each dataset are not identical. To better control for this variable, we fix the set of images and instead compare the effect of caption quality of different datasets under the Qwen2.5-3B + Qwen2.5-ViT setting. As shown in Table2 we compare CapRL with ShareGPT4V-1M and DenseFusion-1M. The results demonstrate that, when using the same set of images, further pretraining with the CapRL-3B-annotated dataset enables the LVLM to outperform the baselines by more than \\\\(2\\\\%\\\\). This finding indicates that the substantial advantage of the CapRL dataset over the baselines largely stems from the superior quality of its captions, rather than from differences in image diversity.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 6, \"block_id\": 8, \"type\": \"text\", \"bbox\": [433, 2729, 2108, 3055], \"angle\": 0, \"content\": \"Scaling Trend Comparison of Different Datasets. We further compare the scaling trend of CapRL and DenseFusion under Qwen2.5-3B + Qwen2.5-ViT setting. Specifically, we sample different numbers of image-caption pairs from each dataset for pretraining. As shown in Figure4 the CapRL dataset consistently outperforms the corresponding DenseFusion dataset across various scales of pretraining data. Moreover, the overall trend indicates that this performance gap continues to widen as the data size increases. This phenomenon highlights the strong scaling properties of the CapRL dataset, thanks to its high-quality captions, LVLMs continue to benefit as the dataset size grows.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 4, \"type\": \"image\", \"bbox\": [1685, 336, 2085, 660], \"angle\": 0, \"content\": null, \"caption\": \"Figure 4: The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 7, \"type\": \"table\", \"bbox\": [443, 933, 2096, 1250], \"angle\": 0, \"content\": \"<table><tr><td>Caption\\nModel</td><td>GRPO\\nTrained</td><td>Chart\\nQA</td><td>ChartQA\\nPro</td><td>Info\\nVQA</td><td>MMMU\\nPro</td><td>Math\\nVerse</td><td>Char\\nXiv</td><td>We\\nMath</td><td>Math\\nVision</td><td>MMStar</td><td>SEED</td><td>MMMU</td><td>Average</td></tr><tr><td>Qwen2.5-VL-3B</td><td>X</td><td>65.6</td><td>27.1</td><td>40.2</td><td>28.6</td><td>32.8</td><td>21.8</td><td>54.4</td><td>22.6</td><td>46.4</td><td>64.1</td><td>35.1</td><td>39.9</td></tr><tr><td>Qwen2.5-VL-7B</td><td>X</td><td>74.9</td><td>35.4</td><td>56.4</td><td>30.1</td><td>36.4</td><td>24.8</td><td>57.0</td><td>23.3</td><td>50.7</td><td>67.1</td><td>37.9</td><td>44.9</td></tr><tr><td>Qwen2.5-VL-72B</td><td>X</td><td>80.2</td><td>38.0</td><td>60.8</td><td>34.1</td><td>39.9</td><td>30.7</td><td>60.2</td><td>24.5</td><td>55.0</td><td>69.3</td><td>39.4</td><td>48.3</td></tr><tr><td>UnifiedRW-as-Judge-3B</td><td>✓</td><td>54.9</td><td>25.1</td><td>33.6</td><td>28.1</td><td>34.6</td><td>20.4</td><td>58.2</td><td>24.5</td><td>45.4</td><td>61.2</td><td>36.3</td><td>38.4</td></tr><tr><td>Qwen2.5VL-as-Judge-3B</td><td>✓</td><td>71.4</td><td>34.2</td><td>49.3</td><td>29.1</td><td>33.8</td><td>22.9</td><td>54.3</td><td>24.1</td><td>47.7</td><td>64.5</td><td>36.4</td><td>42.5</td></tr><tr><td>CapRL-3B</td><td>✓</td><td>80.5</td><td>39.9</td><td>64.8</td><td>30.7</td><td>36.4</td><td>32.4</td><td>60.1</td><td>23.4</td><td>55.0</td><td>70.6</td><td>38.1</td><td>48.3</td></tr></table>\", \"caption\": \"Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-as-a-Judge as the reward. The best results are bold and the second-best results are underlined.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 9, \"type\": \"image\", \"bbox\": [1537, 1287, 2101, 1640], \"angle\": 0, \"content\": null, \"caption\": \"Figure 5: (Left) CapRL demonstrates strong generalization even when trained on images from a single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability. (Right) CapRL demonstrates promising scaling performance on QA training datasets.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 1993, 2103, 2187], \"angle\": 0, \"content\": \"In the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al. 2024), and compare our CapRL-3B against other captioning models.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 14, \"type\": \"text\", \"bbox\": [433, 2636, 2108, 3052], \"angle\": 0, \"content\": \"Comparison with Qwen2.5-VL series. As shown in Table3 CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model. In chart and infographic understanding, CapRL-3B surpasses Qwen2.5-VL-3B by \\\\(14.9\\\\%\\\\), \\\\(12.8\\\\%\\\\), and \\\\(24.6\\\\%\\\\) on ChartQA, ChartQAPro, and InfoVQA, respectively. For natural image understanding, it leads Qwen2.5-VL-3B by \\\\(9.6\\\\%\\\\) and \\\\(6.5\\\\%\\\\) on MMStar and SEED. These results demonstrate that GRPO training has substantially unlocked the potential of Qwen2.5-VL-3B, enabling it to fully leverage its inherent knowledge to organize all objects and their attributes within an image into comprehensive and detailed captions. As a result, its perception capability is pushed to the limit, reaching a level comparable to that of the 72B model.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 2, \"type\": \"table\", \"bbox\": [443, 386, 1275, 640], \"angle\": 0, \"content\": \"<table><tr><td>Caption \\nModel</td><td>ChartQA \\nPro</td><td>Info \\nVQA</td><td>MMMU</td><td>MMStar</td><td>WeMath</td><td>Avg</td></tr><tr><td>Qwen2.5-VL-3B</td><td>27.1</td><td>40.2</td><td>35.1</td><td>46.4</td><td>54.4</td><td>40.6</td></tr><tr><td>CapRL-1QA-20k</td><td>35.5</td><td>59.8</td><td>36.6</td><td>50.8</td><td>57.3</td><td>48.0</td></tr><tr><td>CapRL-2QA-20k</td><td>36.8</td><td>60.2</td><td>37.6</td><td>51.1</td><td>56.6</td><td>48.5</td></tr><tr><td>CapRL-3QA-20k</td><td>36.9</td><td>60.3</td><td>36.9</td><td>51.3</td><td>56.8</td><td>48.5</td></tr></table>\", \"caption\": \"Table 5: Ablations about Sampling Rounds N.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 699, 2108, 1349], \"angle\": 0, \"content\": \"Comparison with LVLM-as-a-Judge reward. In our comparison with other reward design methods, we observe that when using UnifiedReward-2.0-qwen-3b as the judge to evaluate caption quality, the model's captioning ability actually deteriorates during GRPO training. We attribute this to the severe bias present in UnifiedReward-2.0-qwen-3b: during its training, it was exposed to lots of captions from text-to-image datasets, which are typically short and only describe the main objects. As a result, the UnifiedReward model tends to favor shorter captions. As shown in Fig.1 the average caption length during training continuously decreases and eventually collapses to producing only “:description”. Conversely, when using Qwen2.5-VL-3B as the judge, the bias is in the opposite direction: it prefers overly verbose captions. This makes the policy model prone to exploiting the bias by generating long passages of content irrelevant to the image, thereby satisfying the judge model's preference. As shown in Table3 the captioning ability under this reward shows significantly inferior to CapRL. Specific examples of such cases are illustrated in Fig.9 Fig.10 Fig.11 All these observations highlight that LVLM-as-a-Judge reward is fundamentally unreliable. This further underscores the advantage of CapRL, which converts subjective evaluations into objective assessments.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1465, 1377, 1511], \"angle\": 0, \"content\": \"4.3 COMPREHENSIVE DISCUSSION ABOUT CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 7, \"type\": \"text\", \"bbox\": [436, 1577, 2103, 1673], \"angle\": 0, \"content\": \"In this section, we provide a comprehensive analysis and discussion of CapRL. These results further confirm CapRL's general applicability, robustness, and effectiveness.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 8, \"type\": \"text\", \"bbox\": [436, 1692, 2108, 2065], \"angle\": 0, \"content\": \"CapRL demonstrates strong generalization even when trained on images from a single domain. We further investigate the effect of different image sources in the QA dataset used for GRPO training. To this end, we classify the images into two categories using Qwen2.5-VL-3B: (1) documents, charts, or infographics, and (2) natural images. From each category, we sample 20k images for comparison. As illustrated in Fig. 5 (Left), models trained exclusively on chart-type images via GRPO exhibit substantial gains over Qwen2.5-VL-3B, not only in document and chart understanding but also in general VQA tasks. This demonstrates the strong generalization of CapRL-induced captioning improvements beyond the domains encountered during training.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 9, \"type\": \"text\", \"bbox\": [436, 2085, 2103, 2412], \"angle\": 0, \"content\": \"CapRL demonstrates promising scaling performance on training data. We conduct training on different amounts of QA data to evaluate the scaling behavior. As shown in Fig. [5] (Right), the model's performance improves steadily as the amount of QA data increases. These results indicate that our CapRL framework exhibits highly promising scaling potential. With the continued expansion of the training data, the captioning ability can be further enhanced, unlocking additional potential of Qwen2.5-VL-3B. Given its relatively small parameter size and excellent scaling properties, this approach holds strong promise for application in industrial-scale multimodal pretraining.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 8, \"block_id\": 10, \"type\": \"text\", \"bbox\": [436, 2428, 2108, 2801], \"angle\": 0, \"content\": \"Sparse QA supervision is sufficient for CapRL. We further examine the effect of varying the number of QA pairs per image. Specifically, we randomly sample 20k images that retain three QA pairs after filtering, obtain CapRL-3QA-20k after training. By controlling the number of QA pairs per image, we also construct CapRL-1QA-20k and CapRL-2QA-20k. The results, presented in Table4 show that even with only a single QA pair per image, Qwen2.5-VL-3B achieves a substantial improvement in captioning performance, averaging 7.4% higher than the baseline and only 0.5% lower than CapRL-2QA-20k. This highlights the remarkable efficiency of CapRL: highly sparse QA supervision is sufficient to unlock significant gains in captioning ability.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 9, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 438, 2111, 719], \"angle\": 0, \"content\": \"In this work, we introduce CapRL, a novel framework that successfully applies RLVR to the subjective task of image captioning. By redefining caption quality based on its utility in enabling a vision-free LLM to accurately answer questions, we create a robust, objective reward signal. Our results show that CapRL effectively encourages models to generate dense and precise image descriptions, which in turn substantially promote modality alignment in LVLM pretraining. This work marks a significant step away from the restrictive, data-hungry SFT paradigm for RLVR in open-ended tasks.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 1, \"type\": \"title\", \"bbox\": [441, 339, 854, 386], \"angle\": 0, \"content\": \"A CAPRL CASES\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 2, \"type\": \"text\", \"bbox\": [436, 452, 2108, 544], \"angle\": 0, \"content\": \"We provide further illustrative examples of CapRL-3B to highlight its surprising captioning capabilities in this section.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 3, \"type\": \"text\", \"bbox\": [436, 567, 2103, 851], \"angle\": 0, \"content\": \"Comparison with Qwen2.5-VL-3B. As illustrated in Fig. 6 CapRL-3B demonstrates remarkable capability in understanding infographics, providing information that is both comprehensive and accurate. In contrast, Qwen2.5-VL-3B, as shown in Fig. 7 makes numerous errors in identifying key information within infographics. Furthermore, Fig. 8 highlights that CapRL-3B achieves substantially higher accuracy in chart understanding compared to Qwen2.5-VL-3B. Similarly, in the case of physical image understanding in Fig. 12 CapRL-3B also demonstrates clear superiority.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 5, \"type\": \"text\", \"bbox\": [436, 1211, 2113, 1310], \"angle\": 0, \"content\": \"More cases of CapRL-3B in understanding infographics and natural images. Fig. 13 and Fig. 14 provide additional evidence of the impressive perceptual capacity demonstrated by CapRL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 6, \"type\": \"title\", \"bbox\": [438, 1389, 1545, 1438], \"angle\": 0, \"content\": \"B MORE ANALYSIS EXPERIMENTS ABOUT CAPRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 12, \"type\": \"text\", \"bbox\": [425, 2253, 2103, 2448], \"angle\": 0, \"content\": \"We provide all the prompts employed in our experiments in this section. Specifically, the prompt used in CapRL for guiding the LLM to answer questions conditioned on captions is illustrated in Figure 15; the prompt for utilizing the Unified Reward Model as the reward model is shown in Figure 16, and the prompt for adopting Qwen2.5-VL-3B as the reward model is presented in Figure 17\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 13, \"block_id\": 14, \"type\": \"text\", \"bbox\": [436, 2636, 2108, 3055], \"angle\": 0, \"content\": \"To ensure both quality and safety, our data processing pipeline consists of three main stages. First, inspired by SemDeDup (Abbas et al. 2023), we construct clusters to identify and remove images with redundant semantics. During this step, we also discard low-resolution and overly simple images, while filtering out content that involves violence, pornography, or other safety concerns. Second, to avoid benchmark leakage, we integrate the images used in commonly referenced evaluation datasets and form clusters with them. Any images from our collection that are overly similar to benchmark samples are eliminated. Third, we conduct a safety inspection through human verification. Annotators perform sample-based screening, and once the proportion of unsafe images falls to a negligible level, we stop filtering. Following this process, we obtain the final dataset, CapRL-5M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 14, \"block_id\": 1, \"type\": \"title\", \"bbox\": [466, 356, 650, 396], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 14, \"block_id\": 43, \"type\": \"image_caption\", \"bbox\": [510, 2333, 2029, 2385], \"angle\": 0, \"content\": \"Figure 6: An illustrative example of CapRL-3B applied to infographic understanding.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 1, \"type\": \"title\", \"bbox\": [487, 372, 668, 412], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 16, \"block_id\": 106, \"type\": \"image_caption\", \"bbox\": [510, 2742, 2027, 2795], \"angle\": 0, \"content\": \"Figure 8: Chart understanding comparison between CapRL-3B and Qwen2.5-VL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 1, \"type\": \"title\", \"bbox\": [484, 359, 668, 396], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 43, \"type\": \"image_caption\", \"bbox\": [785, 2517, 1754, 2570], \"angle\": 0, \"content\": \"Figure 9: Example of CapRL-3B chart understanding.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 17, \"block_id\": 45, \"type\": \"text\", \"bbox\": [436, 2818, 2108, 3055], \"angle\": 0, \"content\": \"Baselines. We selected several strong baselines for comparison. (1) Vanilla, which skips the Further Pretraining stage and only goes through the first and third stages. Additionally, we constructed two more baselines by varying the dataset used in the Further Pretraining stage: (2) ShareGPT4V-1M, and (3) DenseFusion-1M. To ensure a fair comparison by controlling the number of samples, we randomly sampled 1 million image-caption pairs from the 5M dataset to form CapRL-1M.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 21, \"block_id\": 8, \"type\": \"title\", \"bbox\": [1476, 1376, 1624, 1402], \"angle\": 0, \"content\": \"After CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 21, \"block_id\": 16, \"type\": \"image_caption\", \"bbox\": [530, 2036, 2009, 2092], \"angle\": 0, \"content\": \"Figure 12: Example of CapRL enhancing the captioning ability of Qwen2.5-VL-3B.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 22, \"block_id\": 1, \"type\": \"title\", \"bbox\": [476, 653, 660, 689], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 22, \"block_id\": 101, \"type\": \"image\", \"bbox\": [1981, 1587, 2062, 1676], \"angle\": 0, \"content\": null, \"caption\": \"Figure 13: An illustrative example of CapRL applied to infographic understanding.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 23, \"block_id\": 1, \"type\": \"title\", \"bbox\": [469, 683, 650, 722], \"angle\": 0, \"content\": \"CapRL-3B:\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 23, \"block_id\": 27, \"type\": \"image_caption\", \"bbox\": [510, 2669, 2029, 2722], \"angle\": 0, \"content\": \"Figure 14: An illustrative example of CapRL applied to natural image understanding.\", \"caption\": \"\"}], \"message\": \"Found 62 matches for keyword 'CapRL'.\"}"}, {"content": "The CapRL framework adopts a two-stage pipeline.\n\nEvidence from the document 'test_pdf':\n- The description says: \"CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption.\"\n- Figure 1(b) and further text detail that CapRL uses a two-stage VQA approach for subjective reward generation.\n- Section 2) clearly states: \"We present CapRL, a new training paradigm featuring a decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using a vision-free LLM to perform the QA task.\"\n- Figure 3 (caption): \"Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process.\"\n\nThus, CapRL does NOT use a single-stage approach—it uses a decoupled, two-stage pipeline.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}], "success": true, "error": null}
{"task_id": "005", "question": "In document 'test_pdf', In the Prism Framework evaluation, is the performance of CapRL-3B comparable to that of Qwen2.5-VL-72B?", "answer": "Based on the contents extracted from the 'test_pdf' document, the performance of CapRL-3B in the Prism Framework evaluation is indeed comparable to that of Qwen2.5-VL-72B.\n\nSpecifically, Table 3 in the document compares various models on multiple benchmarks within the Prism Framework. CapRL-3B achieves an average score of 48.3, which is identical to the average score of Qwen2.5-VL-72B, and both models show very close or identical results across individual benchmarks. The table caption and the surrounding text state that \"CapRL-3B achieves comparable performance to Qwen2.5-VL-72B,\" and this is reaffirmed in the analysis:\n\n- \"CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model.\"\n- For specific benchmarks (e.g., ChartQA, ChartQAPro, InfoVQA), CapRL-3B even outperforms the Qwen2.5-VL-3B model by a substantial margin and matches or outperforms Qwen2.5-VL-72B on several, resulting in overall comparable performance.\n\n**Conclusion:**  \nYes, in the Prism Framework evaluation, the performance of CapRL-3B is comparable to that of Qwen2.5-VL-72B.", "messages": [{"role": "developer", "content": "You are a helpful assistant. You need to use tools to solve the problem.\n\n## Available Tools\n\n- doc_check_tool: Checks if a document (by base name, e.g., report_1) or all PDFs have been converted to structured JSON (OCR) format. THIS MUST BE THE FIRST TOOL CALLED FOR ANY NEW DOCUMENT QUERY.\n- doc_ocr_tool: Converts a raw PDF file into a structured JSON file. ONLY use this immediately AFTER doc_check_tool returns that a file is NOT processed.\n- search_keyword_tool: Searches for a keyword in the structured JSON content of processed documents. ONLY call this tool AFTER confirming the file is processed via doc_check_tool (and running doc_ocr_tool if necessary).\n- get_page_ocr_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and returns the full, concatenated OCR text content of that entire page.\n- get_page_image_tool: Based on a document block (e.g., from search_keyword_tool), it identifies the page and extracts the full image of that entire page, saving it as a PNG file in the temp directory. Returns the path to the saved image.\n- crop_image_tool: Crops a specific figure, table, or chart from the PDF using the location data (bbox and angle) obtained from the search_keyword_tool. Returns the local file path to the high-resolution, upright PNG image, saved in the temp directory.\n- image_description_tool: Uses a multimodal model (VLM) to analyze a local image file (PNG/JPG) and generate a detailed text description. Use after crop_image_tool or get_page_image_tool to interpret visual content.\n\n## Tool Usage Strategy\n\n**For Multi-Step Analysis:**\n1. Break complex problems into logical steps\n2. Use ONE tool at a time to gather information\n3. Verify findings through different approaches when possible\n"}, {"role": "user", "content": "In document 'test_pdf', In the Prism Framework evaluation, is the performance of CapRL-3B comparable to that of Qwen2.5-VL-72B?"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_UXh0U0CLsOoFrHjgLwyZ9Fqk", "function": {"arguments": "{\"file_name_or_all\":\"test_pdf\"}", "name": "doc_check_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_UXh0U0CLsOoFrHjgLwyZ9Fqk", "name": "doc_check_tool", "content": "{\"status\": true, \"files_to_process\": [], \"message\": \"Found 0 files needing OCR processing.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_Nn8BFOngWiyHdzcOatA6rDGM", "function": {"arguments": "{\"keyword\":\"Prism Framework\"}", "name": "search_keyword_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_Nn8BFOngWiyHdzcOatA6rDGM", "name": "search_keyword_tool", "content": "{\"status\": true, \"keyword\": \"Prism Framework\", \"matches\": [{\"file_name\": \"1111test_pdf\", \"page_id\": 0, \"block_id\": 9, \"type\": \"text\", \"bbox\": [586, 1105, 1960, 2303], \"angle\": 0, \"content\": \"Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \\\"good\\\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of \\\\(8.4\\\\%\\\\). Results validate that our CapRL effectively trains models to produce a more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 2, \"block_id\": 4, \"type\": \"text\", \"bbox\": [436, 1673, 2111, 2138], \"angle\": 0, \"content\": \"We conduct a comprehensive evaluation of the significant benefits brought by CapRL. From a qualitative perspective, as shown in Fig. 2 applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From a quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al. 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of \\\\(8.4\\\\%\\\\). These results demonstrate that our CapRL framework, by leveraging objective reward design as a reliable optimization signal, effectively drives the model to produce dense and accurate captions.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 7, \"type\": \"table\", \"bbox\": [443, 933, 2096, 1250], \"angle\": 0, \"content\": \"<table><tr><td>Caption\\nModel</td><td>GRPO\\nTrained</td><td>Chart\\nQA</td><td>ChartQA\\nPro</td><td>Info\\nVQA</td><td>MMMU\\nPro</td><td>Math\\nVerse</td><td>Char\\nXiv</td><td>We\\nMath</td><td>Math\\nVision</td><td>MMStar</td><td>SEED</td><td>MMMU</td><td>Average</td></tr><tr><td>Qwen2.5-VL-3B</td><td>X</td><td>65.6</td><td>27.1</td><td>40.2</td><td>28.6</td><td>32.8</td><td>21.8</td><td>54.4</td><td>22.6</td><td>46.4</td><td>64.1</td><td>35.1</td><td>39.9</td></tr><tr><td>Qwen2.5-VL-7B</td><td>X</td><td>74.9</td><td>35.4</td><td>56.4</td><td>30.1</td><td>36.4</td><td>24.8</td><td>57.0</td><td>23.3</td><td>50.7</td><td>67.1</td><td>37.9</td><td>44.9</td></tr><tr><td>Qwen2.5-VL-72B</td><td>X</td><td>80.2</td><td>38.0</td><td>60.8</td><td>34.1</td><td>39.9</td><td>30.7</td><td>60.2</td><td>24.5</td><td>55.0</td><td>69.3</td><td>39.4</td><td>48.3</td></tr><tr><td>UnifiedRW-as-Judge-3B</td><td>✓</td><td>54.9</td><td>25.1</td><td>33.6</td><td>28.1</td><td>34.6</td><td>20.4</td><td>58.2</td><td>24.5</td><td>45.4</td><td>61.2</td><td>36.3</td><td>38.4</td></tr><tr><td>Qwen2.5VL-as-Judge-3B</td><td>✓</td><td>71.4</td><td>34.2</td><td>49.3</td><td>29.1</td><td>33.8</td><td>22.9</td><td>54.3</td><td>24.1</td><td>47.7</td><td>64.5</td><td>36.4</td><td>42.5</td></tr><tr><td>CapRL-3B</td><td>✓</td><td>80.5</td><td>39.9</td><td>64.8</td><td>30.7</td><td>36.4</td><td>32.4</td><td>60.1</td><td>23.4</td><td>55.0</td><td>70.6</td><td>38.1</td><td>48.3</td></tr></table>\", \"caption\": \"Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-as-a-Judge as the reward. The best results are bold and the second-best results are underlined.\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 12, \"type\": \"text\", \"bbox\": [433, 1993, 2103, 2187], \"angle\": 0, \"content\": \"In the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al. 2024), and compare our CapRL-3B against other captioning models.\", \"caption\": \"\"}, {\"file_name\": \"1111test_pdf\", \"page_id\": 7, \"block_id\": 13, \"type\": \"text\", \"bbox\": [433, 2201, 2103, 2620], \"angle\": 0, \"content\": \"Implementation details. Similar to our caption reward design, the Prism Framework decouples VQA into two stages. In Stage 1, the captioner generates captions about the input image. In Stage 2, an LLM answers questions based solely on the generated caption. We leverage the Prism framework primarily because it can evaluate caption quality in an objective and stable manner. In our setup, we fix Stage 2 with a fine-tuned Qwen2.5-3B-Instruct as the answering LLM, ensuring that benchmark performance directly reflects the quality of captions produced by the captioner. To assess the effect of different reward designs in GRPO, we include two other baseline models: one trained with UnifiedReward-2.0-qwen-3b (Wang et al. 2025), and the other with Qwen2.5-VL-3B as the judge for caption quality evaluation. The corresponding prompts are provided in Appendix C.\", \"caption\": \"\"}], \"message\": \"Found 5 matches for keyword 'Prism Framework'.\"}"}, {"content": null, "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": [{"id": "call_BNaqZvj0qSuHjEa2xMhmyyQi", "function": {"arguments": "{\"file_name\":\"1111test_pdf\",\"page_id\":7}", "name": "get_page_ocr_tool"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "call_BNaqZvj0qSuHjEa2xMhmyyQi", "name": "get_page_ocr_tool", "content": "{\"status\": true, \"file_name\": \"1111test_pdf\", \"page_id\": 7, \"ocr_content\": \"Preprint\\n\\n[IMAGE Content]\\nNone\\n\\n[IMAGE Content]\\nNone\\n\\n[IMAGE Content]\\nNone\\n\\nCaption: Figure 4: The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend.\\n\\n[IMAGE Content]\\nNone\\n\\nCaption: Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-as-a-Judge as the reward. The best results are bold and the second-best results are underlined.\\n\\n[TABLE Content]\\n<table><tr><td>Caption\\nModel</td><td>GRPO\\nTrained</td><td>Chart\\nQA</td><td>ChartQA\\nPro</td><td>Info\\nVQA</td><td>MMMU\\nPro</td><td>Math\\nVerse</td><td>Char\\nXiv</td><td>We\\nMath</td><td>Math\\nVision</td><td>MMStar</td><td>SEED</td><td>MMMU</td><td>Average</td></tr><tr><td>Qwen2.5-VL-3B</td><td>X</td><td>65.6</td><td>27.1</td><td>40.2</td><td>28.6</td><td>32.8</td><td>21.8</td><td>54.4</td><td>22.6</td><td>46.4</td><td>64.1</td><td>35.1</td><td>39.9</td></tr><tr><td>Qwen2.5-VL-7B</td><td>X</td><td>74.9</td><td>35.4</td><td>56.4</td><td>30.1</td><td>36.4</td><td>24.8</td><td>57.0</td><td>23.3</td><td>50.7</td><td>67.1</td><td>37.9</td><td>44.9</td></tr><tr><td>Qwen2.5-VL-72B</td><td>X</td><td>80.2</td><td>38.0</td><td>60.8</td><td>34.1</td><td>39.9</td><td>30.7</td><td>60.2</td><td>24.5</td><td>55.0</td><td>69.3</td><td>39.4</td><td>48.3</td></tr><tr><td>UnifiedRW-as-Judge-3B</td><td>✓</td><td>54.9</td><td>25.1</td><td>33.6</td><td>28.1</td><td>34.6</td><td>20.4</td><td>58.2</td><td>24.5</td><td>45.4</td><td>61.2</td><td>36.3</td><td>38.4</td></tr><tr><td>Qwen2.5VL-as-Judge-3B</td><td>✓</td><td>71.4</td><td>34.2</td><td>49.3</td><td>29.1</td><td>33.8</td><td>22.9</td><td>54.3</td><td>24.1</td><td>47.7</td><td>64.5</td><td>36.4</td><td>42.5</td></tr><tr><td>CapRL-3B</td><td>✓</td><td>80.5</td><td>39.9</td><td>64.8</td><td>30.7</td><td>36.4</td><td>32.4</td><td>60.1</td><td>23.4</td><td>55.0</td><td>70.6</td><td>38.1</td><td>48.3</td></tr></table>\\n\\n[IMAGE Content]\\nNone\\n\\nCaption: Figure 5: (Left) CapRL demonstrates strong generalization even when trained on images from a single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability. (Right) CapRL demonstrates promising scaling performance on QA training datasets.\\n\\n[IMAGE Content]\\nNone\\n\\n4.2 PRISM SETTING\\n\\nIn the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al. 2024), and compare our CapRL-3B against other captioning models.\\n\\nImplementation details. Similar to our caption reward design, the Prism Framework decouples VQA into two stages. In Stage 1, the captioner generates captions about the input image. In Stage 2, an LLM answers questions based solely on the generated caption. We leverage the Prism framework primarily because it can evaluate caption quality in an objective and stable manner. In our setup, we fix Stage 2 with a fine-tuned Qwen2.5-3B-Instruct as the answering LLM, ensuring that benchmark performance directly reflects the quality of captions produced by the captioner. To assess the effect of different reward designs in GRPO, we include two other baseline models: one trained with UnifiedReward-2.0-qwen-3b (Wang et al. 2025), and the other with Qwen2.5-VL-3B as the judge for caption quality evaluation. The corresponding prompts are provided in Appendix C.\\n\\nComparison with Qwen2.5-VL series. As shown in Table3 CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model. In chart and infographic understanding, CapRL-3B surpasses Qwen2.5-VL-3B by \\\\(14.9\\\\%\\\\), \\\\(12.8\\\\%\\\\), and \\\\(24.6\\\\%\\\\) on ChartQA, ChartQAPro, and InfoVQA, respectively. For natural image understanding, it leads Qwen2.5-VL-3B by \\\\(9.6\\\\%\\\\) and \\\\(6.5\\\\%\\\\) on MMStar and SEED. These results demonstrate that GRPO training has substantially unlocked the potential of Qwen2.5-VL-3B, enabling it to fully leverage its inherent knowledge to organize all objects and their attributes within an image into comprehensive and detailed captions. As a result, its perception capability is pushed to the limit, reaching a level comparable to that of the 72B model.\\n\\n8\", \"message\": \"Successfully retrieved OCR content for page 7 of file '1111test_pdf'.\"}"}, {"content": "Based on the contents extracted from the 'test_pdf' document, the performance of CapRL-3B in the Prism Framework evaluation is indeed comparable to that of Qwen2.5-VL-72B.\n\nSpecifically, Table 3 in the document compares various models on multiple benchmarks within the Prism Framework. CapRL-3B achieves an average score of 48.3, which is identical to the average score of Qwen2.5-VL-72B, and both models show very close or identical results across individual benchmarks. The table caption and the surrounding text state that \"CapRL-3B achieves comparable performance to Qwen2.5-VL-72B,\" and this is reaffirmed in the analysis:\n\n- \"CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model.\"\n- For specific benchmarks (e.g., ChartQA, ChartQAPro, InfoVQA), CapRL-3B even outperforms the Qwen2.5-VL-3B model by a substantial margin and matches or outperforms Qwen2.5-VL-72B on several, resulting in overall comparable performance.\n\n**Conclusion:**  \nYes, in the Prism Framework evaluation, the performance of CapRL-3B is comparable to that of Qwen2.5-VL-72B.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}], "success": true, "error": null}
