{
  "_comment": "DS Benchmark Configuration - For document QA tasks",
  "benchmark_name": "ds_infer",
  "model_name": "YOUR_MODEL_NAME",
  "api_key": "${OPENAI_API_KEY}",
  "base_url": "VLLM_URL",

  "max_turns": 30,

  "available_tools": ["ds_inspect_data", "ds_read_csv", "ds_run_python"],
  "sandbox_server_url": "http://127.0.0.1:18890",
  "sandbox_auto_start": true,
  "sandbox_config_path": "configs/sandbox-server/ds_config.json",
  "sandbox_timeout": 300,

  "system_prompt": [
    "You are a data analysis assistant. You must use the provided tools to answer questions grounded in the CSV files.",
    "",
    "## Available Tools",
    "1. ds_inspect_data: list CSV files and summarize each file (shape, columns, missing values)",
    "2. ds_read_csv: preview a CSV file",
    "3. ds_run_python: run Python for analysis (pandas, numpy, scipy, sklearn, statsmodels, matplotlib, seaborn)",
    "",
    "## Strategy",
    "1. Start with ds_inspect_data to understand available files",
    "2. Use ds_read_csv to confirm schema and values",
    "3. Use ds_run_python for any computation/aggregation",
    "",
    "## Output",
    "- Reply with the final answer only (no extra explanation) unless the question asks otherwise."
  ],

  "evaluate_results": true,
  "evaluation_metric": "exact_match",

  "data_path": "benchmark/ds_benchmark.jsonl",
  "output_dir": "infer_results/ds",

  "save_results": true,
  "save_trajectories": true,
  "trajectory_only": true,
  "save_summary": false
}
