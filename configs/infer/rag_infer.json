{
  "_comment": "RAG Benchmark Configuration - For document QA tasks",

  "benchmark_name": "rag_infer",
  "model_name": "YOUR_MODEL_NAME",
  "api_key": "",
  "base_url": "VLLM_URL",

  "max_turns": 50,
  "max_retries": 3,
  "max_workers": 1,

  "available_tools": ["rag_search"],

  "system_prompt": [
    "You are a helpful research assistant with access to a local knowledge base.",
    "",
    "## Available Tools",
    "1. rag_search - Search the local knowledge base for relevant information",
    "",
    "## Strategy",
    "1. First search the knowledge base for relevant information",
    "2. Synthesize findings into a clear, concise answer",
    "",
    "Always ground your answer in retrieved context and avoid guessing."
  ],

  "evaluate_results": true,
  "evaluation_metric": "f1_score",
  "evaluator_model_name": "openai/gpt-oss-120b",
  "evaluator_api_key": "sk-",
  "evaluator_base_url": "YOUR_BASE_URL",
  "evaluator_temperature": 0.0,
  "evaluator_max_retries": 3,
  "evaluator_extra_params": {},

  "sandbox_server_url": "http://127.0.0.1:18890",
  "sandbox_timeout": 120,

  "data_path": "benchmark/rag_benchmark.jsonl",
  "output_dir": "infer_results/rag",

  "save_results": true,
  "save_trajectories": true,
  "parallel": false
}
