[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "aside_text",
                "bbox": [
                    57,
                    1090,
                    151,
                    2546
                ],
                "angle": 270,
                "content": "arXiv:2303.08559v2 [cs.CL] 21 Oct 2023"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    334,
                    312,
                    2153,
                    456
                ],
                "angle": 0,
                "content": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    672,
                    498,
                    1823,
                    564
                ],
                "angle": 0,
                "content": "Yubo Ma\\(^{1}\\), Yixin Cao\\(^{2}\\), YongChing Hong\\(^{1}\\), Aixin Sun\\(^{1}\\)"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    798,
                    613,
                    1689,
                    677
                ],
                "angle": 0,
                "content": "\\(^{1}\\) S-Lab, Nanyang Technological University"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    875,
                    677,
                    1612,
                    736
                ],
                "angle": 0,
                "content": "2 Singapore Management University"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    987,
                    740,
                    1498,
                    796
                ],
                "angle": 0,
                "content": "yubo001@e.ntu.edu.sg"
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    647,
                    887,
                    846,
                    936
                ],
                "angle": 0,
                "content": "Abstract"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    352,
                    978,
                    1146,
                    2427
                ],
                "angle": 0,
                "content": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general \\(^{1}\\). Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment. Our code is available at https://github.com/mayubo2333/LLM-IE."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    285,
                    2466,
                    645,
                    2518
                ],
                "angle": 0,
                "content": "1 Introduction"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    280,
                    2553,
                    1215,
                    3069
                ],
                "angle": 0,
                "content": "Large Language Models (LLMs, Brown et al. 2020; Chowdhery et al. 2022; Touvron et al. 2023) have shown remarkable abilities on various NLP applications such as factual question answering (Yu et al., 2023; Sun et al., 2023), arithmetic reasoning (Chen et al., 2022a; Qian et al., 2023) and logical reasoning (Jung et al., 2022; Pan et al., 2023). Given the reasoning, memorization, instruction-following and few-shot adaption capabilities emerging from"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    887,
                    2188,
                    1052
                ],
                "angle": 0,
                "content": "LLMs, it prompts a compelling question: Can LLMs be used to boost performance in few-shot information extraction (IE) tasks?"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    1059,
                    2195,
                    2409
                ],
                "angle": 0,
                "content": "To answer this question, we conduct an extensive empirical study to compare the performance between LLMs using in-context learning \\(^{2}\\) (ICL) and fine-tuned Small Language Models (SLMs). We fairly evaluate SLMs-based and LLMs-based methods across nine datasets spanning four common IE tasks: (1) Named Entity Recognition, (2) Relation Extraction, (3) Event Detection and (4) Event Argument Extraction. For each dataset, we explored four to six settings to encompass typical low-resource extents, from 1-shot to 20-shot or even more. Given the potential sensitivity of LLMs' performance to the prompt context, we meticulously considered variations in instruction, demonstration number and selection strategy, prompt format, etc. Our study reveals that LLMs excel over SLMs only when annotations are extremely limited, i.e., both label types\\(^{3}\\) and the samples\\(^{4}\\) per label are extremely scarce. With more (e.g., hundreds of) samples, SLMs significantly outperform LLMs. Furthermore, LLMs incur greater inference latency and costs than fine-tuned SLMs. Hence, we claim that current LLMs are not good few-shot information extractors in general."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1260,
                    2417,
                    2200,
                    2925
                ],
                "angle": 0,
                "content": "We further investigate whether LLMs and SLMs exhibit different abilities to handle various types of samples. We categorize samples according to their difficulty measured by SLMs' confidence scores, and compare LLMs' and SLMs' results within each group. We find that LLMs are good at hard samples, though bad at easy samples. We posit that the knowledge and reasoning abilities in LLMs enable them to handle hard samples (which are sim"
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    1260,
                    2960,
                    2193,
                    3048
                ],
                "angle": 0,
                "content": "2All LLMs discussed in this paper are not fine-tuned, and results for LLMs are based on in-context learning."
            },
            {
                "block_id": 14,
                "type": "page_footnote",
                "bbox": [
                    1260,
                    3048,
                    2195,
                    3132
                ],
                "angle": 0,
                "content": "<sup>3</sup>Label types denote entity/relation/event/role types in different tasks. We use them interchangeably there-in-after."
            },
            {
                "block_id": 15,
                "type": "page_footnote",
                "bbox": [
                    1260,
                    3132,
                    2195,
                    3223
                ],
                "angle": 0,
                "content": "<sup>4</sup>Samples refer to (i) demonstrations in ICL of LLMs, or (ii) training samples for SLMs' fine-tuning."
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    1260,
                    2960,
                    2195,
                    3223
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "page_footnote",
                "bbox": [
                    282,
                    3090,
                    1213,
                    3223
                ],
                "angle": 0,
                "content": "<sup>1</sup>A more precise assertion is that current LLMs, with vanilla prompting setting and without IE-specific fine-tuning, are not good few-shot information extractors in general."
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1210,
                    694
                ],
                "angle": 0,
                "content": "ply beyond SLMs' capabilities) well. Nevertheless, LLMs demonstrate strong predisposition to false-positive predictions on negative samples. Since most negative samples are easy samples (which could be solved readily by SLMs), the performance of LLMs on easy samples sometimes collapses and are usually much worse than fine-tuned SLMs."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    290,
                    701,
                    1215,
                    1876
                ],
                "angle": 0,
                "content": "Leveraging these findings, we pursue an approach to incorporate LLMs and SLMs within a single system and combine their merits. To this end, we propose a novel filter-then-rerank framework. The basic idea is that SLMs serve as a filter and LLMs as a reranker. Specifically, SLMs initially predict and determine the difficulty of each sample. If the sample is a hard one, we further pass the top-\\(N\\) most-likely candidate labels from SLMs to LLMs for reranking. Otherwise we view the prediction from SLMs as the final decision. By providing easy/hard samples with different solution strategies, our system utilizes each model's strengths to complement each other. Also, it reranks only a small subset of samples and minimizes the extra latency and budgets for calling LLMs. With a modest cost increase, our framework yields a consistent F1 improvement, averaging \\(2.4\\%\\) higher than previous methods on various few-shot IE tasks. To the best of our knowledge, this is the first successful attempt to use LLMs to enhance few-shot IE tasks."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    282,
                    1925,
                    672,
                    1982
                ],
                "angle": 0,
                "content": "2 Related Work"
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    282,
                    2020,
                    1059,
                    2076
                ],
                "angle": 0,
                "content": "2.1 LLMs for Information Extraction"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    2097,
                    1215,
                    3223
                ],
                "angle": 0,
                "content": "Recent studies have increasingly explored Information Extraction (IE) tasks using LLMs. Drawing inspiration from instruction tuning (Wei et al., 2022a), several methods (Wadhwa et al., 2023; Wang et al., 2023a; Lu et al., 2023) transform annotated samples into instruction-answer pairs and then fine-tune LLMs, such as FlanT5 (Chung et al., 2022), on them. Nonetheless, this method necessitates a vast range of samples with diverse schemas and often yields suboptimal results in low-resource scenarios. In the context of few-shot IE tasks, prevalent strategies bifurcate into two main streams. The first approach perceives LLMs as efficient annotators (Ding et al., 2023; Josifoski et al., 2023). In these methods, they produce a plethora of pseudolabeled samples through LLMs and leverage the enhanced annotations to train SLMs. Conversely, the latter approach employs LLMs in inference using the ICL paradigm, which is the focus of our subsequent discussion."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    1262,
                    298,
                    1808,
                    347
                ],
                "angle": 0,
                "content": "2.2 Few-shot IE with ICL"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1257,
                    375,
                    2193,
                    1048
                ],
                "angle": 0,
                "content": "Regarding few-shot IE tasks, recent studies intensively compare the performance between SLMs and LLMs but yield inconsistent conclusions. Some studies favor LLMs as competent few-shot extractors (Agrawal et al., 2022; Wang et al., 2023b; Li et al., 2023; Zhang et al., 2023a; Wadhwa et al., 2023), while others dispute this claim (Jimenez Gutierrez et al., 2022; Qin et al., 2023; Wei et al., 2023; Gao et al., 2023). This discrepancy leaves the question of whether LLMs perform competitively on few-shot IE tasks unresolved, thus hindering the advances of this domain."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1260,
                    1052,
                    2195,
                    1669
                ],
                "angle": 0,
                "content": "We attribute such disagreement to the absence of an comprehensive and unified benchmark. Existing studies usually vary in tasks, datasets, and few-shot settings. Furthermore, some studies rely on overly simplistic datasets (Jimenez Gutierrez et al., 2022; Li et al., 2023) and may exaggerate the effectiveness of LLMs. Driven by these findings, our research undertakes comprehensive experiments across four IE tasks, nine datasets with various schema complexities (from coarse-grained to fine-grained) and low-resource settings."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1260,
                    1676,
                    2193,
                    2290
                ],
                "angle": 0,
                "content": "In addition to the empirical study, we develop an innovative filter-then-erank paradigm to combine the strengths of both LLMs and SLMs. It utilizes prompting strategies akin to QA4RE (Zhang et al., 2023a), transforming IE tasks into multi-choice questions. However, our method stands apart by integrating SLMs and LLMs within a single framework. This incorporation (1) enables our paradigm applicable to various IE tasks by providing candidate spans in the text and (2) achieves promising performance under low-resource IE scenarios."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1262,
                    2339,
                    1925,
                    2399
                ],
                "angle": 0,
                "content": "3 Large LMs v.s. Small LMs"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    2434,
                    2193,
                    2606
                ],
                "angle": 0,
                "content": "In this section, we compare the performance between LLMs and SLMs to evaluate whether LLMs perform competitively."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1262,
                    2645,
                    1952,
                    2697
                ],
                "angle": 0,
                "content": "3.1 Task, Dataset and Evaluation"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    2718,
                    2193,
                    3227
                ],
                "angle": 0,
                "content": "We run experiments on nine widely-used datasets across four IE tasks. (1) Named Entity Recognition (NER): CONLL03 (Tjong Kim Sang and De Meulder, 2003), OntoNotes (Weischedel et al., 2013) and FewNERD (Ding et al., 2021). (2) Relation Extraction (RE): TACRED (Zhang et al., 2017) and TACREV (Alt et al., 2020). (3) Event Detection (ED): ACE05 (Doddington et al., 2004), MAVEN (Wang et al., 2020) and ERE (Song et al.,"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    369,
                    329,
                    1220,
                    1322
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    1255,
                    329,
                    2101,
                    1319
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction, demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs. We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1589,
                    1213,
                    1873
                ],
                "angle": 0,
                "content": "2015). (4) Event Argument Extraction (EAE): ACE05, ERE and RAMS (Ebner et al., 2020). With label numbers ranging from 4 to 168, we assess LLMs' performance under different schema complexities. See their details in Appendix A.1."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    1873,
                    1213,
                    2431
                ],
                "angle": 0,
                "content": "Few-shot Set We construct few-shot datasets from the original datasets above. For training and validation set, we adopt \\( K \\)-shot sampling strategy, i.e., sampling \\( K \\) samples for each label type. See more details in Appendix A.2. For test set, we down-sample their original test sets to reduce the cost of LLMs. We randomly sample 500 sentences for RE tasks, and 250 sentences for other task. We ensure that each label has at least one corresponding sample to avoid the absence of rare labels."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2438,
                    1213,
                    2827
                ],
                "angle": 0,
                "content": "Evaluation We adopt micro-F1 score in NER, RE and ED tasks. For EAE task, we follow previous work (Wang et al., 2023b) and adopt head-F1 score, which merely considers matching of the head word rather than the whole content of a text span. We report averaged score w.r.t 5 sampled train/validation sets unless otherwise stated."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    282,
                    2869,
                    870,
                    2925
                ],
                "angle": 0,
                "content": "3.2 Small Language Models"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    2943,
                    1213,
                    3223
                ],
                "angle": 0,
                "content": "We adopt five supervised methods to evaluate the abilities of SLMs. (1) Vanilla fine-tuning for all tasks, (2) FSLS (Ma et al., 2022a) for NER and ED tasks, (3) KnowPrompt (Chen et al., 2022b) for RE task, (4) PAIE (Ma et al., 2022b) for EAE task, and"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1260,
                    1589,
                    2188,
                    1701
                ],
                "angle": 0,
                "content": "(5) UIE (Lu et al., 2022c) for all tasks. See their details in Appendix B."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1262,
                    1743,
                    1853,
                    1799
                ],
                "angle": 0,
                "content": "3.3 Large Language Models"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    1820,
                    2193,
                    2104
                ],
                "angle": 0,
                "content": "Detailed in Appendix C, we evaluate the ICL abilities of LLMs. Given labeled sentences \\( D = \\{(s_i, y_i)\\} \\) and a test sentence \\( s \\), our goal is to predict structured information \\( y \\) from \\( s \\) using a frozen LLM \\( \\mathcal{L} \\). We feed LLM with prompt \\( \\mathcal{P}_{\\mathcal{E}, I, f}(D, s) \\):"
            },
            {
                "block_id": 11,
                "type": "equation",
                "bbox": [
                    1384,
                    2206,
                    2188,
                    2269
                ],
                "angle": 0,
                "content": "\\[\n\\mathcal {P} _ {\\mathcal {E}, I, f} (D, s) = [ I; f (\\mathcal {E} (D, s)); f (s) ] \\tag {1}\n\\]"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    2318,
                    2193,
                    2936
                ],
                "angle": 0,
                "content": "We give examples of prompts on four IE tasks in Figure 1. The prompts consist of three parts: instruction \\( I \\) (color in green in Figure 1), demonstration \\( f(\\mathcal{E}(D, s)) \\) (demo; color in blue) and the question \\( f(x) \\) (color in black). Here \\( \\mathcal{E} \\) denotes demo selector and \\( \\mathcal{E}(D, s) \\subset D \\) denotes selected sentences as the demo to predict \\( s \\). Prompt format \\( f^5 \\) refers to the template which converts demo \\( \\mathcal{E}(D, s) \\) and sample \\( s \\) to input context for LLMs. Then LLM generates \\( f(y) \\) (color in red) from which we could readily parse the extraction results \\( y \\)."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1260,
                    2943,
                    2193,
                    3055
                ],
                "angle": 0,
                "content": "Models \\(\\mathcal{L}\\): We explore six LLMs from two sources. (1) OpenAI models \\(^{6}\\): we employ Chat"
            },
            {
                "block_id": 14,
                "type": "page_footnote",
                "bbox": [
                    1262,
                    3087,
                    2185,
                    3178
                ],
                "angle": 0,
                "content": "We slightly abuse the notation \\( f \\) to allow \\( s, y \\) and \\( \\{(s, y)\\} \\) as the input for simplicity."
            },
            {
                "block_id": 15,
                "type": "page_footnote",
                "bbox": [
                    1322,
                    3178,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": "6The versions of model we use are: gpt-3.5-turbo-0301,"
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    1262,
                    3087,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    387,
                    291,
                    947,
                    663
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    952,
                    291,
                    1535,
                    663
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    1535,
                    291,
                    2093,
                    663
                ],
                "angle": 0,
                "content": null,
                "caption": "(a) Named Entity Recognition (NER)"
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    387,
                    778,
                    1238,
                    1147
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1238,
                    778,
                    2093,
                    1147
                ],
                "angle": 0,
                "content": null,
                "caption": "(b) Relation Extraction (RE)"
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    387,
                    1266,
                    952,
                    1634
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    960,
                    1266,
                    1543,
                    1634
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1543,
                    1266,
                    2086,
                    1634
                ],
                "angle": 0,
                "content": null,
                "caption": "(c) Event Detection (ED)"
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    389,
                    1750,
                    965,
                    2118
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "image",
                "bbox": [
                    965,
                    1754,
                    1523,
                    2118
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    1528,
                    1754,
                    2089,
                    2118
                ],
                "angle": 0,
                "content": null,
                "caption": "(d) Event Argument Extraction (EAE)"
            },
            {
                "block_id": 15,
                "type": "image_caption",
                "bbox": [
                    280,
                    2252,
                    2188,
                    2357
                ],
                "angle": 0,
                "content": "Figure 2: Overall results of SLM-based methods (dashed lines) and LLM-based methods (solid lines) on nine datasets across four IE tasks. The black, horizontal dashed lines represent the SoTA performance on full dataset."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    280,
                    2441,
                    1213,
                    2778
                ],
                "angle": 0,
                "content": "GPT, CODEX (Chen et al., 2022a) and Instruct-GPT (Ouyang et al., 2022) for main experiments. We also evaluate GPT-4 in Appendix D.3. (2) Open-source models: we use LLaMA-13B (Touvron et al., 2023) and its instruction-tuned counterpart, Vicuna-13B (Chiang et al., 2023)."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    280,
                    2785,
                    1213,
                    2953
                ],
                "angle": 0,
                "content": "Instruction \\(I\\): The instruction (1) describes the task and (2) enumerates all possible labels for reference. we adopt instructions shown in Figure 1."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    282,
                    2957,
                    1210,
                    3013
                ],
                "angle": 0,
                "content": "Demo selector \\(\\mathcal{E}\\): The maximum input length of"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    1260,
                    2441,
                    2195,
                    2894
                ],
                "angle": 0,
                "content": "LLMs usually limits the sentence number in demos even under few-shot settings. Therefore for each test sentence \\( s \\), we demand a demo retriever \\( \\mathcal{E}(D, s) \\) which selects a small subset from \\( D \\) as the sentences in demo. Following previous methods (Liu et al., 2022; Su et al., 2022), we retrieve demos according to their sentence embedding similarity to the test samples."
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1260,
                    2943,
                    2195,
                    3227
                ],
                "angle": 0,
                "content": "Prompt format \\(f\\): We use simple textual templates to format the demos and the test sample in main experiments. For example, the template for NER is \"Sentence: [S], Entities: ([type1], [entity1]), ([type2], [entity2])...\"."
            },
            {
                "block_id": 21,
                "type": "page_footnote",
                "bbox": [
                    280,
                    3051,
                    1213,
                    3220
                ],
                "angle": 0,
                "content": "code-davinci-002, text-davinci-003 and gpt-4-0314. Due to budget constraints, we execute InstructGPT and GPT-4 only once per setting. We do not conduct EAE task on CODEX since it had been unavailable at that time."
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    339,
                    280,
                    940,
                    733
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    942,
                    277,
                    1540,
                    736
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    1540,
                    284,
                    2138,
                    736
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 3: LLMs' performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets in Appendix E.2- E.5. Left: ChatGPT's performance (F1 Score) across six instruction variants. Middle: F1 Score changes over varying numbers of demo. Right: ChatGPT's performance across three demo selection strategies. Random: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever (Rubin et al., 2022)."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    1069,
                    659,
                    1119
                ],
                "angle": 0,
                "content": "3.4 Main Results"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    1164,
                    1215,
                    1782
                ],
                "angle": 0,
                "content": "We summarize the main experimental outcomes in Figure 2, indicating that LLMs only outperform SLMs in environments with restricted labels and samples. Conversely, SLMs are generally more effective. Given (1) the practicality of fine-grained IE tasks and the manageable effort of obtaining 10-20 annotations per label and (2) the excessive time and budget demands of LLM inference, we conclude that LLMs are not as effective as supervised SLMs for few-shot IE tasks under real scenarios. We detail our findings as below."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    280,
                    1792,
                    1215,
                    2469
                ],
                "angle": 0,
                "content": "Performance w.r.t sample number. The performance dynamics of SLMs and LLMs are influenced by variations in sample size. Under extremely low-resource (1-shot or 5-shot) settings, LLMs sometimes present superior performance than SLMs. Yet, LLMs tend to reach a performance plateau with only modest increases in sample size. Conversely, SLMs demonstrate marked performance enhancement as sample sizes grow. This trend is evident in Figure 2, where the SLM trajectories (represented by dashed lines) ascend more steeply compared to the LLM ones (solid lines)."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    2480,
                    1215,
                    2988
                ],
                "angle": 0,
                "content": "Performance w.r.t label number. Compared with SLMs, LLMs tend to struggle on fine-grained datasets. For instance, LLMs perform relatively worse on MAVEN and RAMS datasets (with 168/139 labels) than on CONLL (4 labels only). Detailed quantitative results are shown in Appendix E.1, illustrating a clear negative correlation between the label number and the result disparity between LLMs and SLMs across various IE tasks."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    280,
                    2999,
                    1218,
                    3227
                ],
                "angle": 0,
                "content": "Comparisons among LLMs. We observe performance variability among LLMs. (1) Open-source models, LLaMA and Vicuna, significantly lag behind proprietary LLMs across all few-shot IE tasks."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1260,
                    1069,
                    2195,
                    1294
                ],
                "angle": 0,
                "content": "(2) Among proprietary LLMs, ChatGPT performs better on NER and EAE tasks, but poorer so on RE and ED tasks. InstructGPT and CODEX demonstrate comparable performance across these tasks."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    1297,
                    2193,
                    1634
                ],
                "angle": 0,
                "content": "LLMs show limited inference speed. We compare the inference speed of different methods and show their results in Table 1. We observe that LLMs is much slower than SLMs since they have much more parameters, longer input contexts and extra response decay (if external APIs applied)."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1262,
                    1683,
                    1979,
                    1736
                ],
                "angle": 0,
                "content": "3.5 Analysis on Prompt Sensitivity"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    1757,
                    2195,
                    2718
                ],
                "angle": 0,
                "content": "Previous work (Lu et al., 2022b) indicates that the efficacy of LLMs on specific tasks can be significantly influenced by the construction of the prompt. To ensure that LLMs' suboptimal outcomes are not erroneously ascribed to inappropriate prompt designs, we meticulously examine the impact of diverse prompt variations from four aspects, i.e., instruction format, demo number, demo selector and prompt format. We leave comprehensive details of the variants and their results to Appendix E.2-E.5, and illustrate salient findings in Figure 3. Our findings include that (1) diverse instruction strategies yield comparable results in IE task; (2) increasing the number of samples in demonstrations does not unequivocally enhance performance; and (3) The selection strategy of demonstration matters, and retrieval based on sentence embedding"
            },
            {
                "block_id": 14,
                "type": "table",
                "bbox": [
                    1314,
                    2985,
                    2133,
                    3216
                ],
                "angle": 0,
                "content": "<table><tr><td>Dataset (Task)</td><td>Roberta</td><td>T5</td><td>LLaMA</td><td>CODEX</td></tr><tr><td>FewNERD (NER)</td><td>2.8</td><td>39.4</td><td>1135.4</td><td>179.4</td></tr><tr><td>TACREV (RE)</td><td>1.4</td><td>45.6</td><td>1144.9</td><td>151.6</td></tr><tr><td>ACE05 (ED)</td><td>6.6</td><td>62.5</td><td>733.4</td><td>171.7</td></tr></table>",
                "caption": "Table 1: The inference seconds over 500 sentences (run on single V100 GPU). Here LLaMA is extremely slow since we set batch size as 1 due to memory limit."
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1215,
                    526
                ],
                "angle": 0,
                "content": "(what we used) proves sufficiently effective. Consequently, we believe that there unlikely exists a lottery prompt that substantially alters our conclusions that LLMs are not good few-shot IE solver."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    282,
                    568,
                    1156,
                    677
                ],
                "angle": 0,
                "content": "3.6 Discussion: Why LLMs Fail to Obtain Satisfactory Performance on IE Tasks?"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    701,
                    1215,
                    1715
                ],
                "angle": 0,
                "content": "Underutilized Annotations. We notice that LLMs appear to benefit less from additional annotations, i.e., more training samples and label types, than SLMs. We speculate that LLMs are constrained by ICL in two ways. (1) More samples: The number of effective samples for LLMs, those in demos, is limited by maximum input length. Moreover, we also observe LLMs' performance plateaus in some tasks before reaching this limit (see Appendix E.3). Meanwhile, SLMs can continually learn from more samples through supervised learning, widening the performance gap as annotated samples increase. (2) More labels: LLMs struggle with fine-grained datasets. It suggests a difficulty in understanding numerous labels and their subtle interactions merely from the given instruction and exemplars for LLMs. Also, the examples per label in demos decrease as label types increase."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1722,
                    1213,
                    2167
                ],
                "angle": 0,
                "content": "Unexplored Task format. As stated in Zhang et al. (2023a), IE-related tasks are scarce in the widely-used instruction tuning datasets like Wei et al. (2022a) and Wang et al. (2022). Furthermore, the highly-flexible format of NER and ED tasks impair the ICL abilities<sup>7</sup>. Therefore it is likely that instruction-tuned LLMs are not well-acquainted with such IE-related task formats."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    2220,
                    1136,
                    2273
                ],
                "angle": 0,
                "content": "4 LLMs are Good Few-shot Reranker"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    282,
                    2315,
                    960,
                    2367
                ],
                "angle": 0,
                "content": "4.1 Filter-then-erank Paradigm"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    352,
                    2466,
                    1143,
                    2525
                ],
                "angle": 0,
                "content": "Read following sentences and identify what is the entity type of \"The New Yorker\" quoted by \\(< t>\\)."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    357,
                    2525,
                    471,
                    2550
                ],
                "angle": 0,
                "content": "Sentence:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    357,
                    2553,
                    1136,
                    2620
                ],
                "angle": 0,
                "content": "In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for “<t> The New Yorker <t>”. Candidate: Choice"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    357,
                    2620,
                    587,
                    2634
                ],
                "angle": 0,
                "content": "Candidate Choices:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    357,
                    2641,
                    1069,
                    2669
                ],
                "angle": 0,
                "content": "(a)The New Yorker does not belong to any known entities."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    357,
                    2673,
                    883,
                    2697
                ],
                "angle": 0,
                "content": "(b)The New Yorker is a broadcast program."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    357,
                    2701,
                    1019,
                    2736
                ],
                "angle": 0,
                "content": "(c)The New Yorker is a kind of written art.  \n(d)The New Yorker is a media/newspaper organization."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    357,
                    2736,
                    471,
                    2771
                ],
                "angle": 0,
                "content": "(a) The way to Analysis:"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    357,
                    2774,
                    1143,
                    2939
                ],
                "angle": 0,
                "content": "The New Yorker is a well-known American magazine that has been published since 1925, and is primarily known for its long-form journalism, commentary, and satire. It has a reputation for publishing high-quality writing on a wide variety of topics, including politics, culture, and the arts. So The New Yorker is a media/newspaper organization."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    357,
                    2943,
                    602,
                    2971
                ],
                "angle": 0,
                "content": "Correct Answer: (d)"
            },
            {
                "block_id": 16,
                "type": "image_caption",
                "bbox": [
                    337,
                    3037,
                    1153,
                    3090
                ],
                "angle": 0,
                "content": "Figure 4: Multi-choice question (MCQ) prompt."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1257,
                    298,
                    2195,
                    1536
                ],
                "angle": 0,
                "content": "To mitigate LLMs' drawbacks mentioned above, we propose a filter-then-rerank paradigm to integrate both SLMs and LLMs within the same system. This paradigm uses SLMs as filters to select the top-\\(N\\) candidate labels, then LLMs rerank them to make final decisions. By using SLM-generated candidate answers, the focus of LLMs shifts from sentence-level (i.e., identifying all entities/events in the sentence) to sample-level (i.e., determining single entity/event candidate provided). Each question now corresponds to a single sample, allowing us to reframe prompts as multi-choice questions (MCQ; shown in Figure 4) problem. Under such format, each candidate label is converted to a choice by pre-defined templates. We claim filter-then-rerank paradigm is more likely to elicit the powers of LLMs and smoothly solve few-shot IE tasks because: (1) LLMs are more familiar with MCQ prompts than IE-format prompts (Zhang et al., 2023a). (2) This paradigm reduces the label scopes significantly, since \\(N\\) is usually much smaller than fine-grained label numbers."
            },
            {
                "block_id": 18,
                "type": "title",
                "bbox": [
                    1260,
                    1592,
                    1974,
                    1645
                ],
                "angle": 0,
                "content": "4.2 LLMs are Hard Sample Solver"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    1257,
                    1673,
                    2193,
                    2518
                ],
                "angle": 0,
                "content": "Our filter-then-rerank paradigm, unfortunately, presents unsatisfactory performance (and even suffers longer latency since LLMs rerank candidates per sample). Given LLMs' abilities in memorization and reasoning, however, we still believe that LLMs are potential to solve some, if not most, IE samples effectively. We hypothesize that LLMs are more proficient than SLMs on hard samples. These samples are characterized by their requisite for external knowledge acquisition or sophisticated reasoning strategies, areas where LLMs can leverage their extensive parametric knowledge bases and inherent reasoning mechanisms. In contrast, SLMs often falter with such samples, constrained by their restricted modeling capacities."
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1260,
                    2525,
                    2193,
                    2750
                ],
                "angle": 0,
                "content": "We leverage an unsupervised metric from SLMs to evaluate the difficulty of samples. Given a sample \\( x \\) in the sentence \\( s \\), we define the highest probability across all labels as the confidence score:"
            },
            {
                "block_id": 21,
                "type": "equation",
                "bbox": [
                    1446,
                    2809,
                    2188,
                    2890
                ],
                "angle": 0,
                "content": "\\[\n\\operatorname {c o n f} (x) = \\max  _ {l \\in L} P _ {S L M} (l | x; s) \\tag {2}\n\\]"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    1260,
                    2943,
                    2193,
                    3223
                ],
                "angle": 0,
                "content": "where \\( L \\) denotes the label set and \\( P_{SLM}(l|x;s) \\) the probability of a span \\( x \\) (in the sentence \\( s \\)) referring to label \\( l \\) computed by SLMs. We classify samples with low confidence scores as hard samples. Otherwise we view them as easy samples."
            },
            {
                "block_id": 23,
                "type": "page_footnote",
                "bbox": [
                    282,
                    3129,
                    1210,
                    3223
                ],
                "angle": 0,
                "content": "These two tasks require unfixed numbers of (label, span) tuple. Furthermore, the length of each span is also unfixed."
            }
        ]
    },
    {
        "page_id": 6,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    349,
                    287,
                    1146,
                    1227
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 5: Relationship between confidence scores and performance with/without LLM reranking. We adopt RoBERTa-large as filter and InstructGPT as reranker."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    1508,
                    1215,
                    1957
                ],
                "angle": 0,
                "content": "We conduct experiments to confirm our hypothesis that LLMs excel on hard samples. We group samples by confidence scores and compare two methods within each group: (a) SLM-based methods without LLM reranking, and (b) SLMs as the filter and LLMs as the reranker. Method (b) differs from (a) by adding a single LLM to rerank the top-\\(N\\) SLM predictions, using MCQ prompts."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1964,
                    1215,
                    2873
                ],
                "angle": 0,
                "content": "The results in Figure 5 substantiate our assumption. (1) LLM-based reranking (blue lines) enhances performance on hard samples (left areas in the figure). We provide a detailed analysis of specific challenging instances where LLM rerankers prove advantageous in Appendix F.1. These instances demonstrate the efficacy of LLMs in harnessing external knowledge and complex reasoning to rectify erroneous predictions initially made by SLMs (red lines). (2) Conversely, LLM-based reranking impedes performance on easy samples (right areas), resulting in a significant degradation, particularly for very easy samples (rightmost areas). In conclusion, LLMs exhibit greater proficiency in handling hard samples compared to SLMs, yet they underperform relative to SLMs on easy samples."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    2918,
                    1049,
                    2978
                ],
                "angle": 0,
                "content": "4.3 Why LLMs Fail on Easy Samples"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2999,
                    1215,
                    3227
                ],
                "angle": 0,
                "content": "We investigate why LLMs (relatively) fail on easy samples in this section. As shown in Table 2, we observe significant higher negative sample ratios for easy samples across diverse IE tasks. In other"
            },
            {
                "block_id": 7,
                "type": "table",
                "bbox": [
                    1292,
                    477,
                    2170,
                    712
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>FewNERD</td><td>TACREV</td><td>ACE05</td></tr><tr><td>Overall</td><td>5.88</td><td>3.03</td><td>38.2</td></tr><tr><td>Easy samples (τ &gt; 0.9)</td><td>9.44</td><td>3.21</td><td>44.0</td></tr><tr><td>Hard samples (τ &lt; 0.6)</td><td>1.28</td><td>2.68</td><td>1.36</td></tr></table>",
                "caption": "Table 2: Comparative ratios of negative to positive samples across various datasets and subsets. We set fixed threshold \\(\\tau\\) here for simplicity."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1257,
                    789,
                    2195,
                    1582
                ],
                "angle": 0,
                "content": "words, most negative samples are easy samples for SLMs. Here we refer negative samples to those labeled as None. We speculate that the proficiency of SLMs with negative samples stems from their ability to adeptly discern apparent patterns during the fine-tuning stages. Therefore, SLMs could predict negative samples with (relatively) high confidence and accuracy. Due to LLMs' predisposition to false-positive predictions on negative samples, however, the performance of LLMs on easy samples collapses. We attribute such false-positive predictions to (1) hallucination and (2) span boundary mismatch. We detail such two kinds of mistakes with cases in Appendix F.2."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1260,
                    1620,
                    2175,
                    1683
                ],
                "angle": 0,
                "content": "5 Adaptive Filter-then-erank Paradigm"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1257,
                    1708,
                    2193,
                    2160
                ],
                "angle": 0,
                "content": "Above findings can be summarized as: (1) SLMs generally outperform LLMs, especially with more training samples and fine-grained labels. (2) SLMs are much more time- and cost-efficient. (3) LLMs serve as powerful rerankers on hard samples that challenge SLMs. Based on them, we propose a simple, efficient, and effective adaptive reranker that combines the strengths of SLMs and LLMs."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1260,
                    2199,
                    1535,
                    2245
                ],
                "angle": 0,
                "content": "5.1 Method"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    2269,
                    2195,
                    2894
                ],
                "angle": 0,
                "content": "Our adaptive filter-then-erank approach, shown in Figure 6, uses supervised SLMs as a filter to make preliminary decisions. Samples with confidence scores exceeding threshold are viewed as easy samples otherwise hard ones. For easy samples, we retain SLM predictions as final results. For hard samples, top-\\(N\\) predictions from SLMs are reranked via LLMs using ICL. Here LLMs employ MCQ prompts (Figure 4), containing demos and a sample to be reranked. The LLMs then generate the final answer and optionally provide an explanation."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1260,
                    2929,
                    1773,
                    2985
                ],
                "angle": 0,
                "content": "5.2 Experimental Setup"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1257,
                    2999,
                    2193,
                    3227
                ],
                "angle": 0,
                "content": "We conduct experiments on FewNERD for NER task, TACREV for RE task and ACE05 for ED task. We employ top-performing SLM-based methods from Section 3 (FSLS or KnowPrompt) as the"
            }
        ]
    },
    {
        "page_id": 7,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    285,
                    291,
                    2185,
                    1073
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 6: The overall architecture of our adaptive filter-then-erank paradigm. We color easy samples in orange and hard samples in pink. For easy samples, the final predictions are exactly from the SLM-based methods. For hard samples, the top-\\(N\\) predictions from SLMs are fed into LLMs as the format of multiple-choice questions (pink box). The question is paired with demos (green box). LLMs rerank these \\(N\\) candidates and generate the final prediction."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    277,
                    1392,
                    1213,
                    2069
                ],
                "angle": 0,
                "content": "filter, and Vicuna-13B, InstructGPT or GPT-4 as the reranker. The threshold \\(\\tau\\) to determine sample difficulty is optimized on the valid set. For hard sample, the top-3 SLM predictions and None (if not included) are feed to LLMs for reranking. Each LLM prompt has 4-shot demos. See demo examples in Appendix G.1. We follow templates in Lu et al. (2022a) for TACREV and carefully design others. See these templates in Appendix G.2. We adopt chain-of-thought reasoning (Wei et al., 2022b), i.e.,prefacing the answer with an explanation, to facilitate LLMs' reranking procedure."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    2073,
                    1208,
                    2185
                ],
                "angle": 0,
                "content": "Baseline We compare our method with two kinds of baselines to validate its effectiveness."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    2188,
                    1213,
                    2704
                ],
                "angle": 0,
                "content": "(1) LLMs with ICL: We follow the prompts in Section 3.3 and conduct experiments on three LLMs. (2) Supervised SLMs: We follow previous SoTA methods shown in Section 3.4 (FSLS or Know-Prompt). We additionally combine two SLMs with ensemble or reranking approach (i.e., replace the LLM with another SLM as the reranker) to verify that improvements from our SLM-LLM integrated system are not solely due to the ensemble effects."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    282,
                    2753,
                    657,
                    2802
                ],
                "angle": 0,
                "content": "5.3 Main Results"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    277,
                    2830,
                    1215,
                    3227
                ],
                "angle": 0,
                "content": "Table 3 shows that our filter-then-erank method consistently improves performance across three datasets and nine settings. For instance, with InstructGPT, reranking provides an average F1 gain of \\(2.4\\%\\) without SLM ensemble (Lines 4 vs. 7). Based on ensemble SLMs as the filter, our method still achieves \\(2.1\\%\\) (Lines 5 vs. 8) gains on av"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1260,
                    1392,
                    2190,
                    1564
                ],
                "angle": 0,
                "content": "erage. This confirms (1) the effectiveness of the LLM reranking and (2) its gains are different and (almost) orthogonal to the SLM ensemble."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1262,
                    1610,
                    1550,
                    1662
                ],
                "angle": 0,
                "content": "5.4 Analysis"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1257,
                    1687,
                    2190,
                    2136
                ],
                "angle": 0,
                "content": "Few makes big difference Our method selectively reranks hard samples. Table 4 shows that (1) only a minor fraction \\((0.5\\% \\sim 10\\%)\\) of samples are deemed hard and are reranked by LLMs. (2) Despite their limited quantity, reranking results in a substantial performance boost on these samples \\((10\\% \\sim 25\\%\\) absolute F1 gains). This uplift on a small subset significantly enhances the overall performance."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    2139,
                    2193,
                    2532
                ],
                "angle": 0,
                "content": "GPT-4 is more aggressive From Tables 3 and 4, GPT-4 generally improves more on hard samples, yet InstructGPT surpasses GPT-4 in NER and RE tasks when evaluated overall. This discrepancy arises from GPT-4's aggressive reranking which introduces more true positives. InstructGPT, however, focuses more on reducing false positives."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    2539,
                    2193,
                    2876
                ],
                "angle": 0,
                "content": "Few makes small cost Figure 7 demonstrates that our method impressively reduces budget and latency by approximately \\(80\\% \\sim 90\\%\\) compared to direct ICL. This reduction is due to (1) fewer LLM callings (only for hard samples) and (2) shorter prompts (fewer candidate labels and demos)."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1262,
                    2922,
                    1672,
                    2978
                ],
                "angle": 0,
                "content": "5.5 Ablation Study"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1260,
                    2999,
                    2190,
                    3223
                ],
                "angle": 0,
                "content": "We investigate the effectiveness of the modules in adaptive filter-then-erank system by removing each of them in turn: (1) CoT: We exclude the explantion for each examples in demo. (2) Demo:"
            }
        ]
    },
    {
        "page_id": 8,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    300,
                    480,
                    2178,
                    1294
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\"></td><td colspan=\"3\">FewNERD (NER)</td><td colspan=\"3\">TACREV (RE)</td><td colspan=\"3\">ACE (ED)</td></tr><tr><td>5-shot</td><td>10-shot</td><td>20-shot</td><td>20-shot</td><td>50-shot</td><td>100-shot</td><td>5-shot</td><td>10-shot</td><td>20-shot</td></tr><tr><td>CODEX</td><td>53.8(0.5)</td><td>54.0(1.4)</td><td>55.9(0.5)</td><td>59.1(1.4)</td><td>60.3(2.4)</td><td>62.4(2.6)</td><td>47.1(1.2)</td><td>47.7(2.8)</td><td>47.9(0.5)</td></tr><tr><td>InstructGPT</td><td>53.6(-)</td><td>54.6(-)</td><td>57.2(-)</td><td>60.1(-)</td><td>58.3(-)</td><td>62.7(-)</td><td>52.9(-)</td><td>52.1(-)</td><td>49.3(-)</td></tr><tr><td>GPT-4</td><td>-</td><td>-</td><td>57.8(-)</td><td>-</td><td>-</td><td>59.3(-)</td><td>-</td><td>-</td><td>52.1(-)</td></tr><tr><td>Previous SoTA</td><td>59.4(1.5)</td><td>61.4(0.8)</td><td>61.9(1.2)</td><td>62.4(3.8)</td><td>68.5(1.6)</td><td>72.6(1.5)</td><td>55.1(4.6)</td><td>63.9(0.8)</td><td>65.8(2.0)</td></tr><tr><td>+ Ensemble (S)</td><td>59.6(1.7)</td><td>61.8(1.2)</td><td>62.6(1.0)</td><td>64.9(1.5)</td><td>71.9(2.2)</td><td>74.1(1.7)</td><td>56.9(4.7)</td><td>64.2(2.1)</td><td>66.5(1.7)</td></tr><tr><td>+ Rerank (S)</td><td>59.4(1.5)</td><td>61.0(1.7)</td><td>61.5(1.7)</td><td>64.2(2.3)</td><td>70.8(2.3)</td><td>74.3(2.2)</td><td>56.1(0.3)</td><td>64.0(1.0)</td><td>66.7(1.7)</td></tr><tr><td colspan=\"10\">Vicuna-13B</td></tr><tr><td>+ Rerank (L)</td><td>60.0(1.8)</td><td>61.9(2.1)</td><td>62.2(1.4)</td><td>65.2(1.4)</td><td>70.8(1.6)</td><td>73.8(1.7)</td><td>56.9(4.0)</td><td>63.5(2.7)</td><td>66.0(2.6)</td></tr><tr><td>+ Ensemble (S) + Rerank (L)</td><td>59.9(0.7)</td><td>62.1(0.7)</td><td>62.8(1.1)</td><td>66.5(0.5)</td><td>73.6(1.4)</td><td>75.0(1.5)</td><td>57.9(5.2)</td><td>64.4(1.2)</td><td>66.2(2.4)</td></tr><tr><td colspan=\"10\">InstructGPT</td></tr><tr><td>+ Rerank (L)</td><td>60.6(2.1)</td><td>62.7(0.8)</td><td>63.3(0.6)</td><td>66.8(2.6)</td><td>72.3(1.4)</td><td>75.4(1.5)</td><td>57.8(4.6)</td><td>65.3(1.7)</td><td>67.3(2.2)</td></tr><tr><td>+ Ensemble (S) + Rerank (L)</td><td>61.3(1.9)</td><td>63.2(0.9)</td><td>63.7(1.8)</td><td>68.9(1.3)</td><td>74.8(1.3)</td><td>76.8(1.2)</td><td>59.5(3.7)</td><td>65.3(1.9)</td><td>67.8(2.1)</td></tr><tr><td colspan=\"10\">GPT-4</td></tr><tr><td>+ Rerank (L)</td><td>60.8(2.3)</td><td>62.6(2.7)</td><td>63.0(1.3)</td><td>65.9(2.7)</td><td>72.3(0.3)</td><td>74.5(1.5)</td><td>59.6(2.9)</td><td>64.9(2.5)</td><td>67.1(2.5)</td></tr><tr><td>+ Ensemble (S) + Rerank (L)</td><td>61.1(2.2)</td><td>62.8(0.9)</td><td>63.6(1.2)</td><td>68.6(1.3)</td><td>73.9(1.4)</td><td>75.9(2.4)</td><td>60.9(3.9)</td><td>65.6(1.5)</td><td>67.8(1.7)</td></tr></table>",
                "caption": "Table 4: The F1-score differences before and after reranking on the reranked samples, as well as their proportion of the total samples."
            },
            {
                "block_id": 3,
                "type": "table",
                "bbox": [
                    287,
                    1561,
                    1218,
                    1827
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td colspan=\"4\">GPT-4</td><td colspan=\"4\">InstructGPT</td></tr><tr><td></td><td>before</td><td>after</td><td>△</td><td>ratio</td><td>before</td><td>after</td><td>△</td><td>ratio</td></tr><tr><td>FewNER</td><td>31.9</td><td>40.7</td><td>8.8</td><td>3.2%</td><td>31.4</td><td>28.3</td><td>-3.1</td><td>3.3%</td></tr><tr><td>TACREV</td><td>25.3</td><td>43.0</td><td>17.7</td><td>9.1%</td><td>33.8</td><td>43.4</td><td>9.6</td><td>7.1%</td></tr><tr><td>ACE05</td><td>31.1</td><td>57.9</td><td>26.8</td><td>1.6%</td><td>35.6</td><td>55.7</td><td>20.1</td><td>0.5%</td></tr></table>"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    277,
                    1929,
                    1213,
                    2266
                ],
                "angle": 0,
                "content": "We remove all examples, rendering the reranking a zero-shot problem. (3) LF (label filtering): We retain all labels as candidate choices for reranking, instead of only the top- \\(N\\) labels from the SLMs. (4) AD (adaptive): We feed all samples, not just hard ones, to the LLMs."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2273,
                    1215,
                    2613
                ],
                "angle": 0,
                "content": "We show their results in Table 5 and see that (1) Demos with explanations consistently enhance the reranking ability of LLMs across all datasets. (2) Demos without explanations also contribute to performance improvement. (3) Label filtering results in gains and notably reduces the demo length,"
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    292,
                    2701,
                    1205,
                    3065
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 7: The financial and time cost over 500 sentences. InstructGPT as the reranker."
            },
            {
                "block_id": 9,
                "type": "table",
                "bbox": [
                    1272,
                    1508,
                    2188,
                    1946
                ],
                "angle": 0,
                "content": "<table><tr><td>CoT</td><td>Demo</td><td>LF</td><td>AD</td><td>FewNERD (20-shot)</td><td>TACREV (100-shot)</td><td>ACE05 (20-shot)</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>63.6(1.2)</td><td>75.9(2.4)</td><td>67.8(1.7)</td></tr><tr><td>✗</td><td>✓</td><td>✓</td><td>✓</td><td>63.2(1.2)</td><td>75.4(2.4)</td><td>67.2(1.7)</td></tr><tr><td>✗</td><td>✗</td><td>✓</td><td>✓</td><td>63.0(1.4)</td><td>74.9(2.2)</td><td>66.6(1.5)</td></tr><tr><td>✗</td><td>✗</td><td>✗</td><td>✓</td><td>62.4(2.1)</td><td>73.8(2.5)</td><td>66.5(1.3)</td></tr><tr><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>12.5(2.7)</td><td>59.9(6.0)</td><td>5.4(1.1)</td></tr><tr><td colspan=\"4\">Previous SoTA methods</td><td>62.6(1.0)</td><td>74.1(1.7)</td><td>66.5(1.7)</td></tr></table>",
                "caption": "Table 5: Ablation study on three datasets. The filter is ensembled SLMs and the reranker is GPT-4."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    2027,
                    2193,
                    2252
                ],
                "angle": 0,
                "content": "hence cutting inference costs. (4) The performance collapses without a filter to identify sample difficulty, reiterating the need for an integrated SLM-LLM system to complement each other."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1262,
                    2290,
                    1592,
                    2343
                ],
                "angle": 0,
                "content": "6 Conclusion"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    2378,
                    2193,
                    3227
                ],
                "angle": 0,
                "content": "Through an extensive empirical study on nine datasets spanning four IE tasks, we find that LLMs, despite their superiority in extreme low-resource scenarios, are not effective few-shot information extractors in general. They struggle with IE-related prompts, have limited demonstration capacity, and incur high inference costs. However, LLMs significantly improve the performance on hard samples when combined with SLM. Building on these insights, we propose an adaptive filter-then-erank paradigm to leverage the strengths of SLMs and LLMs and mitigate their limitations. This approach consistently achieves promising results, with an average \\(2.4\\%\\) F1 gain across multiple few-shot IE tasks, while minimizing latency and budget costs."
            }
        ]
    },
    {
        "page_id": 9,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    285,
                    294,
                    548,
                    350
                ],
                "angle": 0,
                "content": "Limitations"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    280,
                    396,
                    1215,
                    957
                ],
                "angle": 0,
                "content": "We do work hard to find better prompts to elicit the power of LLMs on few-shot IE tasks in Section 3.5, by exploring various kinds of LLMs, demonstration strategies and prompt formats. We find that different prompt variants do not significantly impact in-context learning abilities. As an empirical study, we acknowledge the potential existence of a lottery prompt superior to our explored prompts. However, it seems unlikely that an improved prompt would substantially alter our conclusions."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    964,
                    1215,
                    1525
                ],
                "angle": 0,
                "content": "Another common risk when evaluating LLMs on public benchmark is their potential memorization of samples tested. To mitigate such potential contamination, we use earlier and stable versions of these models rather than the newer and updated ones (for example, gpt-4-0314 instead of gpt-4). Even if such contamination makes abilities of LLMs overestimated, our primary conclusions remain unchanged because we find that LLMs are NOT good few-shot information extractors."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1532,
                    1215,
                    2150
                ],
                "angle": 0,
                "content": "Regarding our adaptive filter-then-rerank paradigm, a key limitation lies in how to assess sample difficulty. In this work, we employ a simple unsupervised metric, i.e., the maximum probabilities from SLMs. This is predicated on the assumption that SLMs are well-calibrated (Guo et al., 2017). However, it is an obviously imperfect assumption. We envision that calibrating SLMs-based filters or developing an advanced difficulty metric could substantially enhance LLM rerankers' performance. We leave them for future work."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    285,
                    2210,
                    664,
                    2266
                ],
                "angle": 0,
                "content": "Acknowledgement"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2308,
                    1215,
                    2701
                ],
                "angle": 0,
                "content": "This study is supported under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant, as well as cash and in-kind contribution from the industry partner(s)."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    285,
                    2809,
                    533,
                    2866
                ],
                "angle": 0,
                "content": "References"
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    287,
                    2897,
                    1215,
                    3223
                ],
                "angle": 0,
                "content": "Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022. Large language models are few-shot clinical information extractors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1998-2022, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    1265,
                    298,
                    2198,
                    624
                ],
                "angle": 0,
                "content": "Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. TACRED revisited: A thorough evaluation of the TACRED relation extraction task. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1558-1569, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    1265,
                    663,
                    2198,
                    1354
                ],
                "angle": 0,
                "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    1265,
                    1392,
                    2195,
                    2360
                ],
                "angle": 0,
                "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1265,
                    2399,
                    2198,
                    2725
                ],
                "angle": 0,
                "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597-1607. PMLR."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1265,
                    2760,
                    2198,
                    2953
                ],
                "angle": 0,
                "content": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022a. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1265,
                    2988,
                    2198,
                    3223
                ],
                "angle": 0,
                "content": "Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022b. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In WWW '22: The ACM Web"
            },
            {
                "block_id": 14,
                "type": "list",
                "bbox": [
                    1265,
                    298,
                    2198,
                    3223
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 10,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    334,
                    301,
                    1205,
                    396
                ],
                "angle": 0,
                "content": "Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pages 2778-2788. ACM."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    292,
                    438,
                    1205,
                    715
                ],
                "angle": 0,
                "content": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with \\(90\\%\\) * chatgpt quality."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    292,
                    757,
                    1205,
                    1810
                ],
                "angle": 0,
                "content": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    292,
                    1852,
                    1205,
                    2399
                ],
                "angle": 0,
                "content": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    292,
                    2445,
                    1205,
                    2764
                ],
                "angle": 0,
                "content": "Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. Is GPT-3 a good data annotator? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11173–11195, Toronto, Canada. Association for Computational Linguistics."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    292,
                    2809,
                    1205,
                    3220
                ],
                "angle": 0,
                "content": "Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3198-3213, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 6,
                "type": "list",
                "bbox": [
                    292,
                    301,
                    1205,
                    3220
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    1270,
                    301,
                    2188,
                    666
                ],
                "angle": 0,
                "content": "George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ACE) program – tasks, data, and evaluation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04), Lisbon, Portugal. European Language Resources Association (ELRA)."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    1270,
                    705,
                    2188,
                    982
                ],
                "angle": 0,
                "content": "Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. 2020. Multi-sentence argument linking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057-8077, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1020,
                    2188,
                    1157
                ],
                "angle": 0,
                "content": "Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the feasibility of chatgpt for event extraction."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1196,
                    2188,
                    1518
                ],
                "angle": 0,
                "content": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1557,
                    2188,
                    1873
                ],
                "angle": 0,
                "content": "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1321-1330. PMLR."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1915,
                    2188,
                    2238
                ],
                "angle": 0,
                "content": "Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022. Thinking about GPT-3 in-context learning for biomedical IE? think again. In *Findings of the Association for Computational Linguistics: EMNLP* 2022, pages 4497–4512, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2273,
                    2188,
                    2455
                ],
                "angle": 0,
                "content": "Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023. Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2497,
                    2188,
                    2862
                ],
                "angle": 0,
                "content": "Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1266-1279, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2901,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    1270,
                    301,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 11,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    334,
                    301,
                    1213,
                    396
                ],
                "angle": 0,
                "content": "pages 7871-7880, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    290,
                    434,
                    1208,
                    799
                ],
                "angle": 0,
                "content": "Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. CodeIE: Large code generation models are better few-shot information extractors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15339-15353, Toronto, Canada. Association for Computational Linguistics."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    290,
                    838,
                    1208,
                    1206
                ],
                "angle": 0,
                "content": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100–114, Dublin, Ireland and Online. Association for Computational Linguistics."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    290,
                    1241,
                    1208,
                    1473
                ],
                "angle": 0,
                "content": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    290,
                    1511,
                    1208,
                    1736
                ],
                "angle": 0,
                "content": "Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    290,
                    1778,
                    1208,
                    2101
                ],
                "angle": 0,
                "content": "Keming Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu Derek Ma, and Muhao Chen. 2022a. Summarization as indirect supervision for relation extraction. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6575-6594, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    290,
                    2139,
                    1208,
                    2322
                ],
                "angle": 0,
                "content": "Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, and Jianshu Chen. 2023. Pivoine: Instruction tuning for open-world information extraction. ArXiv preprint, abs/2305.14898."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    290,
                    2360,
                    1208,
                    2729
                ],
                "angle": 0,
                "content": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    290,
                    2767,
                    1208,
                    3090
                ],
                "angle": 0,
                "content": "Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022c. Unified structure generation for universal information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5755-5772, Dublin, Ireland. Association for Computational Linguistics."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    290,
                    3125,
                    1205,
                    3220
                ],
                "angle": 0,
                "content": "Jie Ma, Miguel Ballesteros, Srikanth Doss, Rishita Anubhai, Sunil Mallya, Yaser Al-Onaizan, and Dan"
            },
            {
                "block_id": 10,
                "type": "list",
                "bbox": [
                    290,
                    301,
                    1213,
                    3220
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1312,
                    301,
                    2188,
                    529
                ],
                "angle": 0,
                "content": "Roth. 2022a. Label semantics for few shot named entity recognition. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1956-1971, Dublin, Ireland. Association for Computational Linguistics."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1270,
                    571,
                    2188,
                    936
                ],
                "angle": 0,
                "content": "Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, and Jing Shao. 2022b. Prompt for extraction? PAIE: Prompting argument interaction for event argument extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6759-6774, Dublin, Ireland. Association for Computational Linguistics."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1270,
                    975,
                    2188,
                    1294
                ],
                "angle": 0,
                "content": "Yubo Ma, Zehao Wang, Yixin Cao, and Aixin Sun. 2023. Few-shot event detection: An empirical study and a unified view. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11211-11236, Toronto, Canada. Association for Computational Linguistics."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1336,
                    2188,
                    1697
                ],
                "angle": 0,
                "content": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1739,
                    2188,
                    1925
                ],
                "angle": 0,
                "content": "Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1964,
                    2185,
                    2146
                ],
                "angle": 0,
                "content": "Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2185,
                    2188,
                    2367
                ],
                "angle": 0,
                "content": "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver?"
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2406,
                    2188,
                    2634
                ],
                "angle": 0,
                "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. *J. Mach. Learn. Res.*, 21:140:1-140:67."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2676,
                    2188,
                    2995
                ],
                "angle": 0,
                "content": "Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655-2671, Seattle, United States. Association for Computational Linguistics."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1270,
                    3037,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": "Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, and Xiaoyi Ma. 2015. From light to rich ERE: Annotation of entities, relations, and"
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    1270,
                    301,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 12,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    334,
                    301,
                    1213,
                    487
                ],
                "angle": 0,
                "content": "events. In Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 89-98, Denver, Colorado. Association for Computational Linguistics."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    290,
                    522,
                    1210,
                    754
                ],
                "angle": 0,
                "content": "Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022. Selective annotation makes language models better few-shot learners."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    290,
                    792,
                    1208,
                    978
                ],
                "angle": 0,
                "content": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-augmented language models. In International Conference on Learning Representations."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    290,
                    1013,
                    1210,
                    1287
                ],
                "angle": 0,
                "content": "Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142-147."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    290,
                    1329,
                    1208,
                    1606
                ],
                "angle": 0,
                "content": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    290,
                    1641,
                    1210,
                    1964
                ],
                "angle": 0,
                "content": "Somin Wadhwa, Silvio Amir, and Byron Wallace. 2023. Revisiting relation extraction in the era of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15566-15589, Toronto, Canada. Association for Computational Linguistics."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    290,
                    2003,
                    1210,
                    2276
                ],
                "angle": 0,
                "content": "Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai Du. 2023a. Instructuie: Multi-task instruction tuning for unified information extraction. ArXiv preprint, abs/2304.08085."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    290,
                    2315,
                    1210,
                    2680
                ],
                "angle": 0,
                "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, and Jie Zhou. 2020. MAVEN: A Massive General Domain Event Detection Dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1652-1671, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    290,
                    2722,
                    1210,
                    3044
                ],
                "angle": 0,
                "content": "Xingyao Wang, Sha Li, and Heng Ji. 2023b. Code4Struct: Code generation for few-shot event structure prediction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3640-3663, Toronto, Canada. Association for Computational Linguistics."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    290,
                    3080,
                    1210,
                    3223
                ],
                "angle": 0,
                "content": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves"
            },
            {
                "block_id": 10,
                "type": "list",
                "bbox": [
                    290,
                    301,
                    1213,
                    3223
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1312,
                    301,
                    2185,
                    442
                ],
                "angle": 0,
                "content": "chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations (ICLR 2023)."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1270,
                    473,
                    2190,
                    1297
                ],
                "angle": 0,
                "content": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-Natural Instructions: Generalization via declarative instructions on \\(1600+\\) NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1329,
                    2190,
                    1648
                ],
                "angle": 0,
                "content": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1683,
                    2190,
                    1957
                ],
                "angle": 0,
                "content": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. Proceedings of the 36th International Conference on Neural Information Processing Systems."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1989,
                    2190,
                    2220
                ],
                "angle": 0,
                "content": "Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. 2023. Zero-shot information extraction via chatting with chatgpt."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2248,
                    2190,
                    2483
                ],
                "angle": 0,
                "content": "Ralph Weischedel, Martha Palmer, Mitchell Marcus, Edward Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. 2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2511,
                    2190,
                    2792
                ],
                "angle": 0,
                "content": "Yi Yang and Arzoo Katiyar. 2020. Simple and effective few-shot named entity recognition with structured nearest neighbor learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6365-6375, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2820,
                    2190,
                    3097
                ],
                "angle": 0,
                "content": "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In International Conference for Learning Representation (ICLR 2023)."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1270,
                    3125,
                    2190,
                    3223
                ],
                "angle": 0,
                "content": "Kai Zhang, Bernal Jimenez Gutierrez, and Yu Su. 2023a. Aligning instruction tasks unlocks large language"
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    1270,
                    301,
                    2190,
                    3223
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 13,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    327,
                    301,
                    1215,
                    491
                ],
                "angle": 0,
                "content": "models as zero-shot relation extractors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 794-812, Toronto, Canada. Association for Computational Linguistics."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    287,
                    526,
                    1215,
                    855
                ],
                "angle": 0,
                "content": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Position-aware attention and supervised data improve slot filling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35-45, Copenhagen, Denmark. Association for Computational Linguistics."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    282,
                    894,
                    1215,
                    1126
                ],
                "angle": 0,
                "content": "Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompting in large language models. In *The Eleventh International Conference on Learning Representations* (ICLR 2023)."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    285,
                    1178,
                    563,
                    1227
                ],
                "angle": 0,
                "content": "A Datasets"
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    285,
                    1273,
                    664,
                    1322
                ],
                "angle": 0,
                "content": "A.1 Full Datasets"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    1350,
                    1213,
                    1571
                ],
                "angle": 0,
                "content": "We construct few-shot IE datasets and conduct the empirical study on nine datasets spanning four tasks, with varying schema complexities ranging from 4 to 168. We show their statistics in Table 6."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    285,
                    1617,
                    1017,
                    1669
                ],
                "angle": 0,
                "content": "A.2 Details of Few-shot IE Datasets"
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    280,
                    1694,
                    1215,
                    1750
                ],
                "angle": 0,
                "content": "Sampling Algorithm for Train/Valid Datasets."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    280,
                    1754,
                    1215,
                    3101
                ],
                "angle": 0,
                "content": "We downsample sentences from original training dataset to construct few-shot training and valid datasets. We adopt \\( K \\)-shot sampling strategy that each label has (at least) \\( K \\) samples. We set 6 \\( K \\)-values (1, 5, 10, 20, 50, 100) for RE tasks and 4 \\( K \\)-values (1, 5, 10, 20) for other tasks. For RE task, each sentence has exactly one relation and we simply select \\( K \\) sentences for each label. For NER, ED and EAE tasks, each sentences is possible to contain more than one entities/events/arguments. Since our sampling is at sentence-level, the algorithm of accurate sampling, i.e., finding exactly \\( K \\) samples for each label, is NP-complete<sup>8</sup> and unlikely to find a practical solution. Therefore we follow Yang and Katiyar (2020) adopting a greedy sampling algorithm to select sentences for NER and ED tasks, as shown in Algorithm 1. Note that the actual sample number of each label can be larger than \\( K \\) under this sampling strategy. For all three tasks, we additionally sample negative sentences (without any defined labels) and make the ratio of positive sentences (with at least one label) and negative sentences as 1:1. The statistics of the curated datasets are listed in Table 7."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1267,
                    294,
                    1843,
                    354
                ],
                "angle": 0,
                "content": "Algorithm 1 Greedy Sampling"
            },
            {
                "block_id": 10,
                "type": "algorithm",
                "bbox": [
                    1265,
                    361,
                    2193,
                    1669
                ],
                "angle": 0,
                "content": "Require: shot number \\(K\\), original full dataset \\(\\mathcal{D} = \\{(\\mathbf{X},\\mathbf{Y})\\}\\) tagged with label set \\(E\\)  \n1: Sort \\(E\\) based on their frequencies in \\(\\{\\mathbf{Y}\\}\\) as an ascending order  \n2: \\(S \\gets \\phi\\), Counter \\(\\leftarrow\\) dict()  \n3: for \\(y \\in E\\) do  \n4: \\(\\text{Counter}(y) \\gets 0\\)  \n5: end for  \n6: for \\(y \\in E\\) do  \n7: while \\(\\text{Counter}(y) < K\\) do  \n8: Sample \\((\\mathbf{X},\\mathbf{Y}) \\in \\mathcal{D}\\) s.t. \\(\\exists j, y_j = y\\)  \n9: \\(\\mathcal{D} \\gets \\mathcal{D} \\setminus (\\mathbf{X},\\mathbf{Y})\\)  \n10: Update Counter (not only \\(y\\) but all event types in \\(\\mathbf{Y}\\))  \n11: end while  \n12: end for  \n13: for \\(s \\in S\\) do  \n14: \\(S \\gets S \\setminus s\\) and update Counter  \n15: if \\(\\exists y \\in E\\), s.t. \\(\\text{Counter}(y) < K\\) then  \n16: \\(S \\gets S \\cup s\\)  \n17: end if  \n18: end for  \n19: return \\(S\\)"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    1761,
                    2195,
                    2153
                ],
                "angle": 0,
                "content": "Based on the subsets constructed above, we optionally further split them into training and valid sets. For few-shot datasets with more than 300 sentences, we additionally split \\(10\\%\\) sentences as the valid set and the remaining sentences as training set. Otherwise, we do not construct valid set and conduct 5-fold cross validation to avoid overfitting."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1265,
                    2206,
                    1714,
                    2259
                ],
                "angle": 0,
                "content": "B Details on SLMs"
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1260,
                    2304,
                    2190,
                    2413
                ],
                "angle": 0,
                "content": "We adopt five representative supervised methods to evaluate the ability of SLMs on few-shot IE tasks."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1262,
                    2420,
                    2188,
                    2532
                ],
                "angle": 0,
                "content": "(1). Fine-tuning (FT): Add a classifier head on SLMs to predict the labels of each sentence/word."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1262,
                    2536,
                    2193,
                    2757
                ],
                "angle": 0,
                "content": "(2). FSLS (Ma et al., 2022a): The state-of-the-art extractive-based method for few-shot NER task. Ma et al. (2023) also validate its competitive performance on few-shot ED tasks."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1262,
                    2764,
                    2188,
                    2873
                ],
                "angle": 0,
                "content": "(3). KnowPrompt (Chen et al., 2022b): The best extractive-based method for few-shot RE task."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1262,
                    2880,
                    2190,
                    2988
                ],
                "angle": 0,
                "content": "(4). PAIE (Ma et al., 2022b): The best extractive-based method for few-shot EAE task."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1262,
                    2995,
                    2188,
                    3164
                ],
                "angle": 0,
                "content": "(5). UIE (Lu et al., 2022c): A competitive unified generation-based method for few-shot IE tasks. We introduce their implementation details below:"
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1262,
                    3171,
                    2190,
                    3223
                ],
                "angle": 0,
                "content": "Fine-tuning/FSLS. We implement these two meth-"
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    1260,
                    2304,
                    2193,
                    3223
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "page_footnote",
                "bbox": [
                    282,
                    3129,
                    1213,
                    3223
                ],
                "angle": 0,
                "content": "The Subset Sum Problem, a classical NP-complete problem, can be reduced to this sampling problem."
            }
        ]
    },
    {
        "page_id": 14,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    295,
                    431,
                    2183,
                    827
                ],
                "angle": 0,
                "content": "<table><tr><td colspan=\"2\">Dataset</td><td colspan=\"3\">Named Entity Recognition\nCONLL OntoNotes FewNERD</td><td colspan=\"2\">Relation Extraction\nTACREV TACRED</td><td colspan=\"3\">Event Detection\nACE05 MAVEN ERE</td><td colspan=\"3\">Event Arg Extraction\nACE05 RAMS ERE</td></tr><tr><td colspan=\"2\">#Label Type</td><td>4</td><td>18</td><td>66</td><td>41</td><td>41</td><td>33</td><td>168</td><td>38</td><td>33</td><td>139</td><td>38</td></tr><tr><td rowspan=\"2\">#Sents</td><td>Train</td><td>14,041</td><td>49,706</td><td>131,965</td><td>68,124</td><td>68,124</td><td>14,024</td><td>32,360</td><td>14,736</td><td>14,024</td><td>7329</td><td>14,736</td></tr><tr><td>Test</td><td>3,453</td><td>10,348</td><td>37,648</td><td>15,509</td><td>15,509</td><td>728</td><td>8,035</td><td>1,163</td><td>728</td><td>871</td><td>1,163</td></tr><tr><td rowspan=\"2\">#Mentions</td><td>Train</td><td>23,499</td><td>128,738</td><td>340,247</td><td>13,012</td><td>13,012</td><td>5,349</td><td>77,993</td><td>6,208</td><td>4859</td><td>17026</td><td>8924</td></tr><tr><td>Test</td><td>5,648</td><td>12,586</td><td>96,902</td><td>3,123</td><td>3,123</td><td>424</td><td>18,904</td><td>551</td><td>576</td><td>2023</td><td>822</td></tr></table>",
                "caption": "Table 7: The statistics of few-shot training sets. We set different random seeds and generate 5 training sets for each setting. We report their average statistics."
            },
            {
                "block_id": 3,
                "type": "table",
                "bbox": [
                    285,
                    1122,
                    1242,
                    3181
                ],
                "angle": 0,
                "content": "<table><tr><td colspan=\"2\">Dataset Settings</td><td># Labels</td><td># Sent</td><td># Sample</td><td># Avg shot</td></tr><tr><td rowspan=\"4\">CONLL&#x27;03</td><td>1-shot</td><td rowspan=\"4\">4</td><td>4.8</td><td>5.8</td><td>1.4</td></tr><tr><td>5-shot</td><td>16.2</td><td>21.8</td><td>5.5</td></tr><tr><td>10-shot</td><td>29.2</td><td>42.6</td><td>10.7</td></tr><tr><td>20-shot</td><td>65.6</td><td>82.0</td><td>20.5</td></tr><tr><td rowspan=\"4\">OntoNotes</td><td>1-shot</td><td rowspan=\"4\">18</td><td>20.0</td><td>33.4</td><td>1.9</td></tr><tr><td>5-shot</td><td>84.8</td><td>148.0</td><td>8.2</td></tr><tr><td>10-shot</td><td>158.6</td><td>281.0</td><td>15.6</td></tr><tr><td>20-shot</td><td>332.8</td><td>547.2</td><td>30.4</td></tr><tr><td rowspan=\"4\">FewNERD</td><td>1-shot</td><td rowspan=\"4\">66</td><td>89.8</td><td>147.0</td><td>2.2</td></tr><tr><td>5-shot</td><td>286.2</td><td>494.8</td><td>7.5</td></tr><tr><td>10-shot</td><td>538.0</td><td>962.0</td><td>14.6</td></tr><tr><td>20-shot</td><td>1027.2</td><td>1851.4</td><td>28.1</td></tr><tr><td rowspan=\"6\">TACREV</td><td>1-shot</td><td rowspan=\"6\">41</td><td>81.6</td><td>41.0</td><td>1.0</td></tr><tr><td>5-shot</td><td>387.6</td><td>205.0</td><td>5.0</td></tr><tr><td>10-shot</td><td>741.2</td><td>406.0</td><td>9.9</td></tr><tr><td>20-shot</td><td>1367.2</td><td>806.0</td><td>19.7</td></tr><tr><td>50-shot</td><td>2872.0</td><td>1944.0</td><td>47.4</td></tr><tr><td>100-shot</td><td>4561.0</td><td>3520.0</td><td>85.9</td></tr><tr><td rowspan=\"6\">TACRED</td><td>1-shot</td><td rowspan=\"6\">41</td><td>81.6</td><td>41.0</td><td>1.0</td></tr><tr><td>5-shot</td><td>387.6</td><td>205.0</td><td>5.0</td></tr><tr><td>10-shot</td><td>741.2</td><td>406.0</td><td>9.9</td></tr><tr><td>20-shot</td><td>1367.2</td><td>806.0</td><td>19.7</td></tr><tr><td>50-shot</td><td>2871.2</td><td>1944.0</td><td>47.4</td></tr><tr><td>100-shot</td><td>4575.2</td><td>3520.0</td><td>85.9</td></tr><tr><td rowspan=\"4\">ACE05</td><td>1-shot</td><td rowspan=\"4\">33</td><td>47.4</td><td>41.0</td><td>1.2</td></tr><tr><td>5-shot</td><td>192.8</td><td>165.0</td><td>5.0</td></tr><tr><td>10-shot</td><td>334.6</td><td>319.4</td><td>9.7</td></tr><tr><td>20-shot</td><td>579.4</td><td>598.2</td><td>18.1</td></tr><tr><td rowspan=\"4\">MAVEN</td><td>1-shot</td><td rowspan=\"4\">168</td><td>157.6</td><td>298.0</td><td>1.8</td></tr><tr><td>5-shot</td><td>540.4</td><td>1262.2</td><td>7.5</td></tr><tr><td>10-shot</td><td>891.2</td><td>2413.8</td><td>14.4</td></tr><tr><td>20-shot</td><td>1286.4</td><td>4611.4</td><td>27.4</td></tr><tr><td rowspan=\"4\">ERE</td><td>1-shot</td><td rowspan=\"4\">38</td><td>48.4</td><td>54.6</td><td>1.4</td></tr><tr><td>5-shot</td><td>175.0</td><td>219.2</td><td>5.8</td></tr><tr><td>10-shot</td><td>304.8</td><td>432.4</td><td>11.4</td></tr><tr><td>20-shot</td><td>521.6</td><td>806.6</td><td>21.2</td></tr><tr><td rowspan=\"4\">ACE05</td><td>1-shot</td><td rowspan=\"4\">33</td><td>23.4</td><td>40.2</td><td>1.2</td></tr><tr><td>5-shot</td><td>79.8</td><td>178.2</td><td>5.4</td></tr><tr><td>10-shot</td><td>130.8</td><td>337.4</td><td>10.2</td></tr><tr><td>20-shot</td><td>213.4</td><td>630.2</td><td>19.1</td></tr><tr><td rowspan=\"4\">RAMS</td><td>1-shot</td><td rowspan=\"4\">139</td><td>130.2</td><td>332.6</td><td>2.4</td></tr><tr><td>5-shot</td><td>514.0</td><td>1599.6</td><td>11.5</td></tr><tr><td>10-shot</td><td>795.2</td><td>3193.2</td><td>23.0</td></tr><tr><td>20-shot</td><td>1070.4</td><td>6095.4</td><td>43.9</td></tr><tr><td rowspan=\"4\">ERE</td><td>1-shot</td><td rowspan=\"4\">38</td><td>21.6</td><td>102.8</td><td>2.7</td></tr><tr><td>5-shot</td><td>74.2</td><td>403.4</td><td>10.6</td></tr><tr><td>10-shot</td><td>127.2</td><td>775.6</td><td>20.4</td></tr><tr><td>20-shot</td><td>190.2</td><td>1397.2</td><td>36.8</td></tr></table>"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    1260,
                    912,
                    2190,
                    1529
                ],
                "angle": 0,
                "content": "ods by ourselves. We use RoBERTa-large (Liu et al., 2019) as the backbones. We adopt Automatic Mixed Precision (AMP) training strategy to save memory. We run each experiment on a single NVIDIA V100 GPU. We train each model with the AdamW (Loshchilov and Hutter, 2019) optimizer with linear scheduler and 0.1 warm-up steps. We set the weight-decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 64, the maximum input length as 192, the training step as 500 and the learning rate as 5e-5."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1260,
                    1532,
                    2188,
                    1918
                ],
                "angle": 0,
                "content": "KnowPrompt We implement this method based on original source code\\(^{10}\\), and use RoBERTa-large as our backbones. We set 10 maximum epochs for 50- and 100-shot datasets, and as 50 epochs for other datasets. We keep all other hyperparameters as default, and run each experiment on a single NVIDIA V100 GPU."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1260,
                    1929,
                    2188,
                    2318
                ],
                "angle": 0,
                "content": "PAIE We implement this method on original source code<sup>11</sup>, and use BART-large (Lewis et al., 2020) as backbones. We keep all hyperparameters as default for ACE and RAMS dataset. For ERE dataset, we set the training step as 1000, the batch size as 16 and the learning rate as 2e-5. We run each experiment on a single NVIDIA V100 GPU."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1260,
                    2325,
                    2188,
                    2711
                ],
                "angle": 0,
                "content": "UIE We implement this method based on original source code\\(^{12}\\), and use T5-large (Raffel et al., 2020) as the backbones. We run each experiment on a single NVIDIA Quadro RTX8000 GPU. We set the batch size as 4 with 4000 training steps. We set the maximum input length as 800 and the learning rate as 1e-4."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1262,
                    2757,
                    1868,
                    2816
                ],
                "angle": 0,
                "content": "C LLMs Implementations"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1260,
                    2844,
                    2188,
                    3013
                ],
                "angle": 0,
                "content": "Regarding our empirical study, we explore the ICL abilities of LLMs on few-shot IE tasks. We mainly use five LLMs from two sources. (1) OpenAI"
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    1317,
                    3037,
                    1947,
                    3083
                ],
                "angle": 0,
                "content": "\\(^{9}\\)https://pytorch.org/docs/stable/amp.html"
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1317,
                    3083,
                    1932,
                    3129
                ],
                "angle": 0,
                "content": "\\(^{10}\\)https://github.com/zjunlp/KnowPrompt"
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1317,
                    3129,
                    1910,
                    3174
                ],
                "angle": 0,
                "content": "\\(^{11}\\)https://github.com/mayubo2333/PAIE"
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1317,
                    3174,
                    1875,
                    3220
                ],
                "angle": 0,
                "content": "\\(^{12}\\)https://github.com/universal-ie/UIE"
            },
            {
                "block_id": 14,
                "type": "list",
                "bbox": [
                    1317,
                    3037,
                    1947,
                    3220
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 15,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1215,
                    694
                ],
                "angle": 0,
                "content": "models: CODEX (code-davinci-002; Chen et al. 2021), InstructGPT (text-davinci-003; Ouyang et al. 2022), and ChatGPT (gpt-3.5-turbo-0301). (2) Open-source models: LLaMA-13B (Touvron et al., 2023) and its instruction-tuned counterpart, Vicuna-13B (Chiang et al., 2023). We detail their implementation details in the next sections below."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    282,
                    740,
                    811,
                    796
                ],
                "angle": 0,
                "content": "C.1 Open-source Models"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    820,
                    1213,
                    1385
                ],
                "angle": 0,
                "content": "We implement multiple ICL approaches on LLaMA-13B and Vicuna-13B without fine-tuning. We set the maximum input length as 2048 and the batch size as 1. We run each experiment on a single NVIDIA V100 GPU. To achieve this, we leverage the Accelerate \\(^{13}\\) framework and fp16 inference to save memory. We set maximum output length as 96 and sampling temperature as 0 (i.e., greedy decoding). We set both frequency_penalty and presence_penalty as 0."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    282,
                    1434,
                    721,
                    1487
                ],
                "angle": 0,
                "content": "C.2 OpenAI Models"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    1511,
                    1215,
                    2024
                ],
                "angle": 0,
                "content": "We implement multiple ICL approaches on OpenAI models by calling their official APIs \\(^{14}\\). We set the maximum input length as 3600 for all tasks and models. The only exception occurs when we use CODEX on RE tasks, where we set the maximum input length as 7000. We unify the maximum output length as 32 for RE task, and 96 for other three tasks. We set the sampling temperature coefficient as 0, i.e., greedy decoding."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    282,
                    2073,
                    989,
                    2129
                ],
                "angle": 0,
                "content": "D Pivot Experiments on LLMs"
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    282,
                    2171,
                    860,
                    2227
                ],
                "angle": 0,
                "content": "D.1 Sampling Temperature"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    2248,
                    1213,
                    2704
                ],
                "angle": 0,
                "content": "Existing prompt-engineering discussion<sup>15</sup> suggests setting the sampling temperature \\( t = 0 \\) for tasks with structured outputs, including IE tasks. We validate this conclusion in Table 8, from which we could see the generated quality when \\( t = 0 \\) is much higher than the quality when \\( t \\neq 0 \\). Therefore we set \\( t = 0 \\) in all main experiments, and do not take self-consistency (Wang et al., 2023c) into account."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    282,
                    2753,
                    962,
                    2806
                ],
                "angle": 0,
                "content": "D.2 Automatic Chain-of-thought"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    280,
                    2830,
                    1213,
                    3002
                ],
                "angle": 0,
                "content": "We additionally investigate whether rationales could facilitate LLMs' performance on few-shot IE tasks. Since there exists no golden rationales in"
            },
            {
                "block_id": 11,
                "type": "table",
                "bbox": [
                    1319,
                    424,
                    2138,
                    726
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>FewNERD</td><td>TACREV</td><td>ACE05</td></tr><tr><td>t = 0</td><td>48.5(1.9)</td><td>53.7(2.3)</td><td>42.9(2.2)</td></tr><tr><td>+ 5-ensemble</td><td>53.5(1.3)</td><td>58.6(1.5)</td><td>46.3(0.8)</td></tr><tr><td>t = 0.7</td><td>40.9(2.3)</td><td>39.9(1.2)</td><td>35.6(1.0)</td></tr><tr><td>+ self-consistency</td><td>52.1(0.9)</td><td>53.4(1.3)</td><td>45.6(3.0)</td></tr></table>",
                "caption": "Table 8: F1-scores across different \\( t \\) values. Experiments run on 10-shot settings with CODEX."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1260,
                    848,
                    2193,
                    1017
                ],
                "angle": 0,
                "content": "original datasets, we follow Automatic Chain-of-thought (Auto-CoT; Zhang et al. 2023b) method as below. Regarding each sample, we query LLMs"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1307,
                    1038,
                    2193,
                    1094
                ],
                "angle": 0,
                "content": "According to [sentence], Why [span] is a [label]."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1257,
                    1115,
                    2193,
                    1789
                ],
                "angle": 0,
                "content": "For example, given the sentence \"DSC and Traction Control on all Speed3 models is also standard.\", we would feed LLM the query that \"Could you explain why Speed3 is a kind of car\". Then we insert the bootstrapped rationales between the sentences and ground-truth answers. If a sentence has no positive labels, however, we do not ask LLMs and keep the original format as the vanilla ICL approach. Here we prompt InstructGPT to generate the rationales with temperature \\( t = 0.7 \\). We compare the performance with and without Auto-CoT as shown in Table 9."
            },
            {
                "block_id": 16,
                "type": "table",
                "bbox": [
                    1347,
                    2104,
                    2113,
                    2339
                ],
                "angle": 0,
                "content": "<table><tr><td>10-shot train set</td><td>FewNERD (NER)</td><td>TACREV (RE)</td><td>ACE05 (ED)</td></tr><tr><td>wo. Auto-CoT</td><td>54.0(1.4)</td><td>57.3(1.8)</td><td>47.7(2.8)</td></tr><tr><td>w. Auto-CoT</td><td>36.6(1.7)</td><td>22.0(1.2)</td><td>43.1(3.4)</td></tr></table>",
                "caption": "Table 9: The F1-score difference between with and without Auto-CoT. We generate rationales by Instruct-GPT, then adopt ICL w. Auto-CoT approach and use CODEX as our backbone for inference."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1257,
                    2434,
                    2193,
                    3223
                ],
                "angle": 0,
                "content": "We are frustrated to find Auto-CoT degrades the performance with a large margin. We speculate this degradation could be attributed to three main reasons. (1) The rationale increase the length of each sample and thus decrease the overall example number in demos. (2) There exists an obvious discrepancy between sentences with and without positive labels. The rationales are only provided for sentences with positive labels because it is hard to explain why a sentence dose not contain any label. (3) Some auto-generated rationales are low-quality, especially for RE tasks. We would explore better strategy to exploit auto-genertaed rationales in the future work."
            },
            {
                "block_id": 18,
                "type": "page_footnote",
                "bbox": [
                    329,
                    3037,
                    937,
                    3087
                ],
                "angle": 0,
                "content": "\\(^{13}\\)https://huggingface.co/docs/accelerate"
            },
            {
                "block_id": 19,
                "type": "page_footnote",
                "bbox": [
                    329,
                    3087,
                    885,
                    3136
                ],
                "angle": 0,
                "content": "\\(^{14}\\)https://openai.com/blog/openai-api"
            },
            {
                "block_id": 20,
                "type": "page_footnote",
                "bbox": [
                    287,
                    3136,
                    1121,
                    3227
                ],
                "angle": 0,
                "content": "\\(^{15}\\)https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api"
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    287,
                    3037,
                    1121,
                    3227
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 16,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    320,
                    378,
                    2163,
                    719
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\"></td><td colspan=\"3\">NER (20-shot)</td><td colspan=\"2\">RE (100-shot)</td><td colspan=\"3\">ED (20-shot)</td><td colspan=\"3\">EAE (20-shot)</td></tr><tr><td>CONLL</td><td>OntoNotes</td><td>FewNERD</td><td>TACREV</td><td>TACRED</td><td>ACE05</td><td>MAVEN</td><td>ERE</td><td>ACE05</td><td>RAMS</td><td>ERE</td></tr><tr><td>InstructGPT</td><td>77.2</td><td>47.7</td><td>57.2</td><td>62.7</td><td>53.8</td><td>49.3</td><td>25.4</td><td>40.8</td><td>45.8</td><td>42.2</td><td>41.9</td></tr><tr><td>CODEX</td><td>81.1</td><td>55.6</td><td>55.9</td><td>62.4</td><td>53.6</td><td>47.9</td><td>22.8</td><td>39.0</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GPT-4</td><td>84.7</td><td>65.6</td><td>57.8</td><td>59.3</td><td>50.4</td><td>52.1</td><td>30.2</td><td>40.5</td><td>42.9</td><td>38.6</td><td>38.2</td></tr><tr><td>Supervised SoTA</td><td>72.3</td><td>74.9</td><td>61.4</td><td>72.6</td><td>63.1</td><td>65.8</td><td>54.7</td><td>56.2</td><td>55.2</td><td>57.7</td><td>55.6</td></tr></table>",
                "caption": "Table 10: F1-scores difference among GPT-4, CODEX and InstructGPT."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    282,
                    796,
                    759,
                    852
                ],
                "angle": 0,
                "content": "D.3 GPT-4 v.s. Others"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    873,
                    1215,
                    1497
                ],
                "angle": 0,
                "content": "We tend to minimize the GPT-4 calls due to its high price. Thus we utilize 20-/100-shot settings across each dataset to compare GPT-4's performance with other LLMs. Table 10 reveals that GPT-4 does not outperform other LLMs significantly, except on OntoNotes and MAVEN. However, even on these datasets, GPT-4 still falls behind supervised SLMs by a significant margin. Consequently, the exclusion of GPT-4 does not undermine the conclusions drawn from our main experiments, and we omit it from our empirical study."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    1543,
                    870,
                    1606
                ],
                "angle": 0,
                "content": "E Auxiliary Experiments"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    282,
                    1641,
                    1195,
                    1697
                ],
                "angle": 0,
                "content": "E.1 LLMs struggle on Fine-grained Datasets"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    280,
                    1715,
                    1215,
                    2115
                ],
                "angle": 0,
                "content": "Based on the results shown in Figure 2, we additionally provide a quantitative analysis to show that LLMs struggle with fine-grained datasets. Under the 5-shot setting, we compare the performance difference of LLMs (ChatGPT) and SLMs (SoTA few-shot models) among different datasets. For each IE task, we observe a clear negative corre-"
            },
            {
                "block_id": 8,
                "type": "table",
                "bbox": [
                    332,
                    2381,
                    1171,
                    3220
                ],
                "angle": 0,
                "content": "<table><tr><td colspan=\"4\">Named Entity Recognition</td></tr><tr><td></td><td>CoNLL</td><td>OntoNotes</td><td>FewNERD</td></tr><tr><td># Entity</td><td>4</td><td>18</td><td>66</td></tr><tr><td>Micro-F1 (SLM)</td><td>52.5</td><td>59.7</td><td>59.4</td></tr><tr><td>Micro-F1 (LLM)</td><td>77.8</td><td>59.4</td><td>55.5</td></tr><tr><td>ΔF1 (LLM, SLM)</td><td>25.3</td><td>-0.3</td><td>-3.9</td></tr><tr><td colspan=\"4\">Event Detection</td></tr><tr><td></td><td>ACE05</td><td>ERE</td><td>MAVEN</td></tr><tr><td># Event</td><td>33</td><td>38</td><td>168</td></tr><tr><td>Micro-F1 (SLM)</td><td>55.1</td><td>48.0</td><td>49.4</td></tr><tr><td>Micro-F1 (LLM)</td><td>39.6</td><td>33.8</td><td>25.3</td></tr><tr><td>ΔF1 (LLM, SLM)</td><td>-15.5</td><td>-14.2</td><td>-24.1</td></tr><tr><td colspan=\"4\">Event Argument Extraction</td></tr><tr><td></td><td>ACE05</td><td>ERE</td><td>RAMS</td></tr><tr><td># Event / #Role</td><td>33 / 22</td><td>38 / 26</td><td>139 / 65</td></tr><tr><td>Head-F1 (SLM)</td><td>45.9</td><td>40.4</td><td>54.1</td></tr><tr><td>Head-F1 (LLM)</td><td>52.8</td><td>40.7</td><td>44.2</td></tr><tr><td>ΔF1 (LLM, SLM)</td><td>6.9</td><td>0.3</td><td>-9.9</td></tr></table>",
                "caption": "Table 11: Performance comparison between LLMs (ChatGPT) and SLM-based methods among datasets with various schema complexities."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1260,
                    796,
                    2195,
                    1083
                ],
                "angle": 0,
                "content": "lation between the label number (row 2) and the performance difference (row 5). In other words, with more label types, LLMs tend to perform relatively worse than SLMs. Therefore we conclude that LLMs struggle on fine-grained datasets."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1262,
                    1115,
                    1902,
                    1171
                ],
                "angle": 0,
                "content": "E.2 Finding Better Instruction"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    1189,
                    2188,
                    1413
                ],
                "angle": 0,
                "content": "To investigate whether LLMs would benefit from complex instructions, we explored six instruction variants from simple to complex. Take NER task as an example, we illustrate them as below."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1262,
                    1417,
                    1706,
                    1469
                ],
                "angle": 0,
                "content": "Instruction0: [empty]"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1260,
                    1473,
                    2195,
                    1862
                ],
                "angle": 0,
                "content": "Instruction1: Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are: [Type_1], [Type_2], ..., [Type_N]. If you do not find any entity in this sentence, just output 'Answer: No entities found.'"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1260,
                    1866,
                    2195,
                    2090
                ],
                "angle": 0,
                "content": "Instruction2: Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are:"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1317,
                    2125,
                    1915,
                    2181
                ],
                "angle": 0,
                "content": "- [Type_1]: [Definition_1]"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1319,
                    2217,
                    1912,
                    2273
                ],
                "angle": 0,
                "content": "- [Type_2]: [Definition_2]"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1319,
                    2315,
                    1434,
                    2357
                ],
                "angle": 0,
                "content": "."
            },
            {
                "block_id": 18,
                "type": "list",
                "bbox": [
                    1317,
                    2125,
                    1915,
                    2357
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    1319,
                    2402,
                    1912,
                    2455
                ],
                "angle": 0,
                "content": "- [Type_N]: [Definition_N]"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1260,
                    2494,
                    2188,
                    2655
                ],
                "angle": 0,
                "content": "If you do not find any entity in this sentence, just output 'Answer: No entities found.'"
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1260,
                    2662,
                    2195,
                    3227
                ],
                "angle": 0,
                "content": "Instruction3: Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below: [Type_1], [Type_2], ..., [Type_N]. Please note that your annotation results must follow such format: ` ` Answer: ([Type_1] <SEP>"
            }
        ]
    },
    {
        "page_id": 17,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1210,
                    575
                ],
                "angle": 0,
                "content": "identified-entity: [Entity_1], ([Type_2] <SEP> identified-entity: [Entity_2]), .......\". If you do not find any entity in this sentence, just output 'Answer: No entities found.'"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    280,
                    585,
                    1213,
                    1655
                ],
                "angle": 0,
                "content": "Instruction4: Assume you are an entity-instance annotator. Your objective is to perform a series of intricate steps for Named Entity Recognition. Firstly, you have to identify a particular word or phrase in the sentence that corresponds to an entity. Following this, classify the entity into one of the potential entity types. The potential entity types are provided as below: [Type_1], [Type_2], ..., [Type_N]. Please note that your annotation results must follow such format: 'Answer: ([Type_1] <SEP> identified-entity:[Entity_1]), ([Type_2] <SEP> identified-entity:[Entity_2]), ....'. If you do not find any entity in this sentence, just output 'Answer: No entities found.'"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    1662,
                    1210,
                    2055
                ],
                "angle": 0,
                "content": "Instruction5: Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    339,
                    2108,
                    930,
                    2164
                ],
                "angle": 0,
                "content": "- [Type_1]: [Definition_1]"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    339,
                    2217,
                    930,
                    2269
                ],
                "angle": 0,
                "content": "- [Type_2]: [Definition_2]"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    339,
                    2336,
                    451,
                    2374
                ],
                "angle": 0,
                "content": "."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    339,
                    2434,
                    930,
                    2490
                ],
                "angle": 0,
                "content": "- [Type_N]: [Definition_N]"
            },
            {
                "block_id": 7,
                "type": "list",
                "bbox": [
                    339,
                    2108,
                    930,
                    2490
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    280,
                    2546,
                    1210,
                    2988
                ],
                "angle": 0,
                "content": "Please note that your annotation results must follow such format: 'Answer: ([Type_1] <SEP> identified_entity:[Entity_1]), ([Type_2] <SEP> identified-entity:[Entity_2]), ....'. If you do not find any entity in this sentence, just output 'Answer: No entities found.'"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    280,
                    3002,
                    1213,
                    3227
                ],
                "angle": 0,
                "content": "Regarding these six instructions, we evaluate their performance of ChatGPT on four 20-shot IE tasks. As shown in Table 12, there is no significant correlation between the instruction complexity"
            },
            {
                "block_id": 11,
                "type": "table",
                "bbox": [
                    1280,
                    427,
                    2170,
                    824
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>FewNERD (NER)</td><td>TACREV (RE)</td><td>ACE (ED)</td><td>ACE (EAE)</td></tr><tr><td>I0</td><td>57.6(2.1)</td><td>49.1(2.4)</td><td>44.0(1.4)</td><td>50.9(0.1)</td></tr><tr><td>I1</td><td>58.3(0.5)</td><td>49.6(1.2)</td><td>42.6(1.0)</td><td>51.5(1.1)</td></tr><tr><td>I2</td><td>57.7(1.0)</td><td>50.0(1.5)</td><td>41.8(0.9)</td><td>50.3(1.5)</td></tr><tr><td>I3</td><td>57.6(2.3)</td><td>52.3(1.8)</td><td>42.9(1.3)</td><td>49.2(2.3)</td></tr><tr><td>I4</td><td>56.8(0.9)</td><td>49.6(2.9)</td><td>41.6(1.9)</td><td>49.9(1.2)</td></tr><tr><td>I5</td><td>57.8(0.5)</td><td>47.2(1.8)</td><td>43.1(1.8)</td><td>50.6(1.8)</td></tr></table>",
                "caption": "Table 12: F1-scores across six instruction formats. Experiments run on 20-shot settings with ChatGPT."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    905,
                    2193,
                    1185
                ],
                "angle": 0,
                "content": "and LLMs' performance. Even the prompt without instruction (I0) leads to comparable, if not better, results than prompt with complex instructions. Therefore, we use simple instruction (I1) in our main experiment."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1260,
                    1224,
                    2051,
                    1280
                ],
                "angle": 0,
                "content": "E.3 Do More Samples in Demos Help?"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1257,
                    1294,
                    2193,
                    2143
                ],
                "angle": 0,
                "content": "We wonder whether longer demos bring more powerful ICL abilities for LLMs. Thus we investigate the impact of increasing the number of demonstrations on LLMs' performance in Figure 8. We observe that: (1) The performance of the RE task consistently improves with more demos, indicating its potential benefiting from additional annotations. (2) The NER and ED tasks reach a stable or degraded performance with increased demo numbers, suggesting that they are limited even before reaching the maximum input length. (3) Open-source LLMs, i.e., LLaMA and Vicuna, have more limited capacities in leveraging demos compared to OpenAI models, with their performance stagnating or even collapsing with only a few (2-4) demos."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1260,
                    2181,
                    2155,
                    2234
                ],
                "angle": 0,
                "content": "E.4 Finding Better Demo Selection Strategy"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1257,
                    2252,
                    2193,
                    2873
                ],
                "angle": 0,
                "content": "The maximum input length of LLMs usually limits the sentence number in demos even under few-shot settings. For each test sentence \\( s \\), we demand a demo retriever \\( \\mathcal{E}(D, s) \\) which selects a subset from \\( D \\) as the sentences in demo. Following previous work, we consider three commonly-used strategies. (1) Random sampling. (2) Sentence-embedding (Liu et al., 2022; Su et al., 2022): retrieving the top-K nearest sentences measured by sentence embedding. We compute the embeddings by SimCSE-RoBERTa-large (Gao et al., 2021)."
            },
            {
                "block_id": 17,
                "type": "equation",
                "bbox": [
                    1290,
                    2911,
                    2188,
                    2974
                ],
                "angle": 0,
                "content": "\\[\n\\mathcal {E} (D, s) = \\arg \\operatorname {t o p K} _ {s ^ {\\prime} \\in D} [ \\text {S e n t - e m b e d} (s ^ {\\prime}, s) ] \\tag {3}\n\\]"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1260,
                    3013,
                    2190,
                    3125
                ],
                "angle": 0,
                "content": "(3) Efficient Prompt Retriever (Rubin et al., 2022): retrieving by a neural retriever \\(R\\) trained on \\(D\\)."
            },
            {
                "block_id": 19,
                "type": "equation",
                "bbox": [
                    1391,
                    3167,
                    2188,
                    3230
                ],
                "angle": 0,
                "content": "\\[\n\\mathcal {E} (D, s) = \\arg \\operatorname {t o p K} _ {s ^ {\\prime} \\in D} \\left[ R _ {D} \\left(s ^ {\\prime}, s\\right) \\right] \\tag {4}\n\\]"
            }
        ]
    },
    {
        "page_id": 18,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    334,
                    298,
                    942,
                    687
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    942,
                    294,
                    1535,
                    691
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    1538,
                    329,
                    2138,
                    687
                ],
                "angle": 0,
                "content": null,
                "caption": "(a) OpenAI LLMs"
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    339,
                    799,
                    940,
                    1196
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    942,
                    803,
                    1535,
                    1196
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    1540,
                    848,
                    2138,
                    1196
                ],
                "angle": 0,
                "content": null,
                "caption": "(b) Open-source LLMs"
            },
            {
                "block_id": 8,
                "type": "image_caption",
                "bbox": [
                    280,
                    1322,
                    2188,
                    1476
                ],
                "angle": 0,
                "content": "Figure 8: Relationship between demo number and F1-score among three datasets. Note that the x-axis in each subfigure represents the number of demos (not the shot value \\( K \\)) during ICL. We adopt sentence embedding as the demo selection strategy and text prompt in this experiment."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    282,
                    1561,
                    1215,
                    2388
                ],
                "angle": 0,
                "content": "For each test sentence \\(s\\), we pre-retrieve \\(M\\) similar sentences \\(\\bar{D} = \\{(s_{i}',y_{i}')\\}_{i=1}^{M} \\subset D\\). Then we score each sentence in \\(\\bar{D}\\) by their likelihoods \\(P_{\\mathcal{L}}(f(y_{i}')|f(s_{i}'))\\) where \\(f\\) denotes the prompt format adopted and \\(\\mathcal{L}\\) the scoring LM. We randomly select positive samples \\(s_{i}'^{\\mathrm{(pos)}}\\) from the top-\\(K_{D}\\) sentences and hard negative samples \\(s_{i}'^{\\mathrm{(hard - neg)}}\\) from the bottom-\\(K_{D}\\) ones. Then we train \\(R_{D}\\) by in-batch contrastive learning (Chen et al., 2020). For each sentence \\(s_{i}'\\) within the batch, there are 1 positive sentences \\(s_{i}'^{\\mathrm{(pos)}}\\) and \\(2B - 1\\) negative sentences \\(\\{s_{j}'^{\\mathrm{(hard - neg)}}\\}_{j=1}^{B} \\cup \\{s_{j}'\\}_{j \\neq i}\\). Here we adopt \\(M\\) as 40, \\(K_{D}\\) as 5, \\(f\\) as text prompt, the batch size \\(B\\) as 128, and the scoring LM \\(\\mathcal{L}\\) as FLAN-T5-x1."
            },
            {
                "block_id": 11,
                "type": "table",
                "bbox": [
                    297,
                    2567,
                    1200,
                    2841
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>FewNERD (NER)</td><td>TACREV (RE)</td><td>ACE (ED)</td></tr><tr><td>Random Sampling</td><td>53.2(0.4)</td><td>43.0(3.3)</td><td>38.0(1.5)</td></tr><tr><td>Sentence Embedding</td><td>57.6(2.3)</td><td>49.6(1.2)</td><td>42.9(1.3)</td></tr><tr><td>Efficient Prompt Retriever</td><td>57.2(0.6)</td><td>48.0(0.8)</td><td>43.5(1.4)</td></tr></table>",
                "caption": "Table 13: F1-scores on three demo-selection strategies. Experiments run on 20-shot settings with ChatGPT."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    280,
                    2887,
                    1213,
                    3227
                ],
                "angle": 0,
                "content": "Table 13 demonstrates the F1-score performance on different selection strategies. We find that both the sentence embedding and EPR surpass random sampling by a large margin. Given the simplicity of the sentence embedding, we adopt it, rather than EPR, as our selection strategy in main experiment."
            },
            {
                "block_id": 14,
                "type": "table",
                "bbox": [
                    1367,
                    1690,
                    2091,
                    1922
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>FewNERD (NER)</td><td>TACREV (RE)</td><td>ACE (ED)</td><td>ACE (EAE)</td></tr><tr><td>Text</td><td>57.6(2.3)</td><td>49.6(1.2)</td><td>42.9(1.3)</td><td>51.5(1.1)</td></tr><tr><td>Code</td><td>53.2(0.9)</td><td>50.2(1.8)</td><td>44.3(2.0)</td><td>47.3(1.5)</td></tr></table>",
                "caption": "Table 14: F1-scores across three prompt formats. Experiments run on 20-shot settings with ChatGPT."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1262,
                    2013,
                    1992,
                    2069
                ],
                "angle": 0,
                "content": "E.5 Finding Better Prompt Format"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1257,
                    2090,
                    2193,
                    2652
                ],
                "angle": 0,
                "content": "Previous studies on LLMs for few-shot IE tasks have explored different prompt formats and highlighted the importance of selecting an appropriate format for achieving competitive performance. Therefore, we investigate two commonly-used variants in previous work: (1) Text prompt as shown in Figure 1. (2) Code prompt: We follow Wang et al. (2023b); Li et al. (2023) and recast the output of IE tasks in the form of code. See more details about this format in their original papers."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1257,
                    2659,
                    2188,
                    2827
                ],
                "angle": 0,
                "content": "Table 14 shows comparable performance across all formats. Based on simplicity, we choose the text prompt for our main experiment."
            },
            {
                "block_id": 18,
                "type": "title",
                "bbox": [
                    1262,
                    2880,
                    1597,
                    2936
                ],
                "angle": 0,
                "content": "F Case Study"
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    1262,
                    2978,
                    1659,
                    3034
                ],
                "angle": 0,
                "content": "F.1 Hard Samples"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1257,
                    3055,
                    2188,
                    3220
                ],
                "angle": 0,
                "content": "Table 15 showcases some hard examples which benefits from our LLM reranking. In accordance with our intuition, we observe that the LLM"
            }
        ]
    },
    {
        "page_id": 19,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    282,
                    298,
                    1218,
                    810
                ],
                "angle": 0,
                "content": "rerankers correct two kinds of erroneous predictions made by LLMs. (1) The lack of external knowledge, such as the first (Triptolemus is a figure in Greek mythology) and third examples (Minas Gerais is a state instead of city). (2) Limited reasoning abilities, such as the second (His wife's children are his children) and the fourth (The word \"fought\" in this sentence does not involve any physical violence) examples."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    285,
                    961,
                    667,
                    1017
                ],
                "angle": 0,
                "content": "F.2 Easy Samples"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    282,
                    1101,
                    1213,
                    1497
                ],
                "angle": 0,
                "content": "As discussed in Section 4.3, we attribute the inferior performance of LLMs on easy samples to their false-positive predictions for negative samples. Upon examining the incorrect cases, we speculate that two main reasons are responsible: (1) LLM hallucination and (2) span boundary mismatch. We provide details on them with specific cases below."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    282,
                    1532,
                    1215,
                    1929
                ],
                "angle": 0,
                "content": "LLM Hallucination. The most distinctive mistakes made by LLMs. Without specific fine-tuning on IE tasks, LLMs (with in-context learning approaches) sometimes hallucinate and provide meaningless, even confusing explanations or answers. For instance, LLMs might predict a verb, preposition or even special symbols as an entity."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    282,
                    1964,
                    1208,
                    2076
                ],
                "angle": 0,
                "content": "Case 1: LLMs predict the word die as entity person-other (Ground truth: None)"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    332,
                    2213,
                    1166,
                    2553
                ],
                "angle": 0,
                "content": "Sentence: Flavius Rumoridus (died 5th century AD) was a Roman soldier who was appointed consul in AD 403 in the Western Roman Empire at the same time as the eastern emperor Theodosius II served in the same capacity in the East."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    337,
                    2613,
                    726,
                    2662
                ],
                "angle": 0,
                "content": "Candidate Choices:"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    354,
                    2704,
                    1153,
                    2813
                ],
                "angle": 0,
                "content": "(a) died do/does not belong to any known entities."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    354,
                    2855,
                    1076,
                    2915
                ],
                "angle": 0,
                "content": "(b) died is a kind of geopolitical entity."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    354,
                    2950,
                    1163,
                    3171
                ],
                "angle": 0,
                "content": "(c) died is a person, but not affiliated with following professions: actor, artist, athlete, author, director, politician, scholar, soldier."
            },
            {
                "block_id": 10,
                "type": "list",
                "bbox": [
                    354,
                    2704,
                    1163,
                    3171
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1312,
                    333,
                    2146,
                    898
                ],
                "angle": 0,
                "content": "Analysis: The sentence provides information about Flavius Rumoridus, who was a Roman soldier and appointed consul in AD 403. The sentence also mentions that he died in the 5th century AD, which is a statement about his death. Therefore, the correct answer is (c) because the sentence does not provide any information about Flavius Rumoridus being affiliated with any of the professions listed in option (c)."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1314,
                    957,
                    1719,
                    1013
                ],
                "angle": 0,
                "content": "Correct Answer: (c)"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1262,
                    1098,
                    2188,
                    1206
                ],
                "angle": 0,
                "content": "Case 2: LLMs predict the word of as entity organization-other (Ground truth: None)"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1314,
                    1290,
                    2141,
                    1459
                ],
                "angle": 0,
                "content": "Sentence: During these years he studied at the South Kensington Schools of the Royal College of Art."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1317,
                    1518,
                    1706,
                    1571
                ],
                "angle": 0,
                "content": "Candidate Choices:"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1334,
                    1613,
                    2141,
                    1722
                ],
                "angle": 0,
                "content": "(a) of do/does not belong to any known entities."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1334,
                    1764,
                    1937,
                    1817
                ],
                "angle": 0,
                "content": "(b) of is an educational institute."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1337,
                    1859,
                    2146,
                    2196
                ],
                "angle": 0,
                "content": "(c) of pertains to an organization that does not fall under the categories of company, educational institution, government, media, political party, religion, sports league, sports team, band or musical group."
            },
            {
                "block_id": 19,
                "type": "list",
                "bbox": [
                    1334,
                    1613,
                    2146,
                    2196
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1312,
                    2294,
                    2146,
                    2802
                ],
                "angle": 0,
                "content": "Analysis: In the given sentence, of is a preposition that indicates the relationship between South Kensington Schools and Royal College of Art. None of the candidate choices directly address the correct relationship between these entities or state that they are not applicable. Therefore, the correct answer is (c): of does not pertain to any known entities mentioned in the choices provided."
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1314,
                    2862,
                    1719,
                    2915
                ],
                "angle": 0,
                "content": "Correct Answer: (c)"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    1262,
                    2999,
                    2193,
                    3223
                ],
                "angle": 0,
                "content": "Span boundary mismatch. Different from sentence-level approaches which take one or multiple sentences as an unit and assign each word's label globally, LLMs under our filter-then-erank"
            }
        ]
    },
    {
        "page_id": 20,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1213,
                    810
                ],
                "angle": 0,
                "content": "paradigm adopt sample-level approaches. In other words, LLMs only consider each word/ span candidate independently. The sample-level approaches enable LLMs to rerank candidates with the help of multi-choice question prompt. However, they also aggravate the span boundary mismatching problem in NER and ED tasks. We empirically find easy samples suffering much more from boundary mismatching problem than hard samples."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    282,
                    824,
                    1213,
                    1108
                ],
                "angle": 0,
                "content": "Case 3: LLMs predict Baron, a subspan of Baron Carl Hurleman, as the entity person-other (The ground-truth entity type of Baron is None. And the ground-truth entity type of Baron Carl Hurleman is person-artist/author)."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    332,
                    1210,
                    1158,
                    1326
                ],
                "angle": 0,
                "content": "Sentence: Additions in the 1740s were by Baron Carl Hurleman (1700 - 1753)."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    337,
                    1385,
                    726,
                    1438
                ],
                "angle": 0,
                "content": "Candidate Choices:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    354,
                    1476,
                    1156,
                    1585
                ],
                "angle": 0,
                "content": "(a) Baron do/does not belong to any known entities."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    354,
                    1627,
                    1163,
                    1845
                ],
                "angle": 0,
                "content": "(b) Baron is a person, but not affiliated with following professions: actor, artist, athlete, author, director, politician, scholar, soldier."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    354,
                    1894,
                    1104,
                    1946
                ],
                "angle": 0,
                "content": "(c) Baron is a kind of geopolitical entity"
            },
            {
                "block_id": 7,
                "type": "list",
                "bbox": [
                    354,
                    1476,
                    1163,
                    1946
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    332,
                    2045,
                    1166,
                    2666
                ],
                "angle": 0,
                "content": "Analysis: In the given sentence, Baron is a title given to a person named Carl Hurleman, who is mentioned as the one who made additions in the 1740s. The sentence does not provide any information about connections to geopolitical entities or any other known entities mentioned in the choices provided. Therefore, the correct answer is (b): Baron is a person, but not affiliated with following professions: actor, artist, athlete, author, director, politician, scholar, soldier."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    337,
                    2725,
                    739,
                    2778
                ],
                "angle": 0,
                "content": "Correct Answer: (b)"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    282,
                    2887,
                    1213,
                    3223
                ],
                "angle": 0,
                "content": "Case 4: LLMs predict 65th, a subspan of 65th British Academy Film Awards, as the entity other-awards (The ground-truth entity type of 65th is None. And the ground-truth entity type of 65th British Academy Film Awards is other-awards)."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1312,
                    333,
                    2141,
                    557
                ],
                "angle": 0,
                "content": "Sentence: Ralph Fiennes was nominated for the BAFTA Award for Outstanding Debut by a British Writer, Director or Producer at the 65th British Academy Film Awards."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1314,
                    617,
                    1706,
                    670
                ],
                "angle": 0,
                "content": "Candidate Choices:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1334,
                    712,
                    2136,
                    820
                ],
                "angle": 0,
                "content": "(a) 65th do/does not belong to any known entities."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1334,
                    862,
                    1845,
                    915
                ],
                "angle": 0,
                "content": "(b) 65th is a kind of award."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1337,
                    957,
                    2141,
                    1122
                ],
                "angle": 0,
                "content": "(c) 65th refers to some event except attack, election, natural disaster, protest, revolution and sports."
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    1334,
                    712,
                    2141,
                    1122
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1312,
                    1220,
                    2146,
                    1729
                ],
                "angle": 0,
                "content": "Analysis: The sentence mentions the 65th British Academy Film Awards, which is an event that recognizes outstanding achievements in the film industry. Therefore, option (c) is incorrect as it excludes events like award ceremonies. Option (a) is also incorrect as the number 65th is associated with the event. Hence, the correct answer is (b) as 65th refers to the specific edition of the BAFTA Awards."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1314,
                    1789,
                    1721,
                    1841
                ],
                "angle": 0,
                "content": "Correct Answer: (b)"
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    1262,
                    1953,
                    2173,
                    2073
                ],
                "angle": 0,
                "content": "G Details on Adaptive Filter-then-erank Paradigm"
            },
            {
                "block_id": 20,
                "type": "title",
                "bbox": [
                    1262,
                    2104,
                    1885,
                    2157
                ],
                "angle": 0,
                "content": "G.1 Demonstration Examples"
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1260,
                    2174,
                    2193,
                    2455
                ],
                "angle": 0,
                "content": "We convert few-shot IE tasks to multiple-choice questions in our filter-then-erank paradigm. We show 4 examples used in demonstrations for FewNERD dataset in Table 16, for TACREV dataset in Table 17, and for ACE05 datasets in Table 18."
            },
            {
                "block_id": 22,
                "type": "title",
                "bbox": [
                    1262,
                    2494,
                    1577,
                    2550
                ],
                "angle": 0,
                "content": "G.2 Template"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    1260,
                    2567,
                    2193,
                    2848
                ],
                "angle": 0,
                "content": "In our filter-then-erank paradigm, we utilize templates converting candidate labels to question options. We list the template for FewNERD dataset in Table 19, for TACREV dataset in Table 20, and for ACE05 datasets in Table 21."
            }
        ]
    },
    {
        "page_id": 21,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    300,
                    1154,
                    2175,
                    2588
                ],
                "angle": 0,
                "content": "<table><tr><td>Sentences (entity/event)</td><td>Before</td><td>After</td><td>Rationales</td></tr><tr><td colspan=\"4\">Named Entity Recognition</td></tr><tr><td>According to it, King Eleusis and Cothonea (Cytinia), parents of Trip-tolemus, are visited by Demeter, who rears their son, feeding him divine milk by day and placing him into the fire at night, which makes Triptolemus grow faster than mortal children normally do.</td><td>person-other ✘</td><td>other-god ✓</td><td>According to the context, Triptolemus is a figure in Greek mythology, the son of King Eleusis and Cothonea (Cytinia). He was a demigod who was sent by Demeter to teach mankind the art of agriculture. So Triptole-mus is a god in some legend/religious story.</td></tr><tr><td colspan=\"4\">Relation Extraction</td></tr><tr><td>Survivors include his wife, Sandra; four sons, Jeff, James, Douglas and Harris; a daughter, Leslie; his mother, Sally; and two brothers, Guy and Paul.</td><td>None ✘</td><td>per:children ✓</td><td>The sentence states &#x27;Survivors include his wife, Sandra; four sons, Jeff, James, Douglas and Harris; a daughter, Leslie; his mother, Sally; and two brothers, Guy and Paul.&#x27; This indicates &#x27;his&#x27; is the father of Jeff, James, Douglas and Harris, and the husband of Sandra. So his is the parent of Jeff.</td></tr><tr><td>Eliza Samudio, Bruno&#x27;s ex-girlfriend and his 4-month-old baby boy&#x27;s mother, disappeared from her hometown of Belo Horizonte, Minas Gerais.</td><td>per:cities_of _residence ✘</td><td>per:states_of _residence ✓</td><td>The sentence states that &#x27;Eliza Samudio, Bruno&#x27;s ex-girlfriend and his 4-month-old baby boy&#x27;s mother, disappeared from her hometown of Belo Horizonte, Minas Gerais.&#x27; This indicates that Eliza Samudio is from the city of Belo Horizonte, which is located in the state of Minas Gerais. So Eliza Samudio lives in the state or province Minas Gerais.</td></tr><tr><td colspan=\"4\">Event Detection</td></tr><tr><td>We have fought every institution, every law firm ... it&#x27;s cost us a fortune but now we&#x27;re going on a massive expansion.</td><td>Conflict.Attack ✘</td><td>None ✓</td><td>In the given sentence, the word fought is used to describe the action of the speaker fighting against various institutions and law firms. This does not involve any physical violence or court proceedings, so the word fought does not trigger any known event.</td></tr></table>",
                "caption": "Table 15: Examples of the samples corrected by LLM reranking. We sample four examples from NER, RE and ED tasks, respectively. Sentences: The sentences in which samples locate. We color the samples (entities or trigger words) to be identified. Before: The prediction before LLM reranking. Based on SLM-based methods. After: The reranked prediction using LLMs. Rationales: LLM-generated Explanations."
            }
        ]
    },
    {
        "page_id": 22,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    320,
                    869,
                    2158,
                    2722
                ],
                "angle": 0,
                "content": "<table><tr><td>Instruct: Read following sentences and identify what is the entity type of 392 quoted by &lt;t&gt;. \nSentence: Powell v. Texas, &lt;t&gt; 392 &lt;t&gt; U.S. 514 (1968), was a United States Supreme Court case that ruled that a Texas statute criminalizing public intoxication did not violate the Eighth Amendment protection against cruel and unusual punishment. \n(a) 392 is a legal document, a term or a convention in legal sense. \n(b) 392 does not belong to any known entities. \n(c) 392 refers to a protest, uprising or revolution event \n(d) 392 refers to a government or governmental agency \nAnalysis: In the context you provided, 392 refers to the volume number in the United States Reports where the Supreme Court&#x27;s decision in Powell v. Texas can be found. However, 392 itself does not refer to a legal document. So 392 do/does not belong to any known entities. \nAnswer: (b)</td></tr><tr><td>Instruct: Read following sentences and identify what is the entity type of The New Yorker quoted by &lt;t&gt;. \nSentence: In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for &quot; &lt;t&gt; The New Yorker &lt;t)&quot;. \n(a) The New Yorker does not belong to any known entities. \n(b) The New Yorker is a broadcast program. \n(c) The New Yorker is a kind of written art. \n(d) The New Yorker is a media/newspaper organization. \nAnalysis: The New Yorker is a well-known American magazine that has been published since 1925, and is primarily known for its long-form journalism, commentary, and satire. It has a reputation for publishing high-quality writing on a wide variety of topics, including politics, culture, and the arts. So The New Yorker is a media/newspaper organization. \nAnswer: (d)</td></tr><tr><td>Instruct: Read following sentence and identify what is the entity type of St. quoted by &lt;t&gt;. \nSentence: The May 1980 eruption of Mount &lt;t&gt; St. &lt;t&gt; Helens in the state of Washington seriously affected both 47th Air Division and 92d Bombardment Wing operations at Fairchild AFB, resulting in dispersal of Fairchild &#x27;s B-52 and KC-135 aircraft to various bases while around-the-clock shifts removed the volcanic ash from facilities within the base perimeter.&quot; \n(a) St. does not belong to any known entities. \n(b) St. is a natural disaster event. \n(c) St. is a geographic position about mountain. \nAnalysis: According to the context, St. is an abbreviation of Saint, used in the name of Mount St. Helens, which is an active volcano in the state of Washington. However, St. itself does not refer to anything. So St. do/does not belong to any known entities. \nAnswer: (a)</td></tr><tr><td>Instruct: Read following sentence and identify what is the entity type of Ridzuan quoted by &lt;t&gt;. \nSentence: &lt;t&gt; Ridzuan &lt;t&gt; was promoted to Harimau Muda A for 2014 season . \n(a) Ridzuan does not belong to any known entities. \n(b) Ridzuan is a person, but not affiliated with following professions: actor, artist, author, director, politician, scholar, soldier. \n(c) Ridzuan is an athlete. \nAnalysis: The mention of &#x27;Harimau Muda A&#x27; indicates that it is a sports-related context, and &#x27;promoted&#x27; implies a progression or advancement within the sports team. So Ridzuan is an athlete. \nAnswer: (c)</td></tr></table>",
                "caption": "Table 16: Demo examples used in FewNERD dataset. We color the entity in blue."
            }
        ]
    },
    {
        "page_id": 23,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    320,
                    855,
                    2158,
                    2739
                ],
                "angle": 0,
                "content": "<table><tr><td>Instruct: Read the sentence and determine the relation between she and lawyer quoted by &lt;t&gt;. \nSentence: The &lt;t&gt; lawyer &lt;t&gt; denied Italian news reports that she wept while addressing the court, but said Knox was upset as &lt;t&gt; she &lt;t&gt; recounted “ the pressure, the aggressiveness of the police who called her a liar . ” \n(a) she is the other family member of lawyer \n(b) she is a lawyer \n(c) she has no known relations to lawyer \nAnalysis: In the sentence, the word ‘she’ refers to someone who was upset while recounting certain events in court. The word ‘lawyer’ refers to someone who denied a news report about that same person weeping in court. There is no information in the sentence to indicate that the two individuals are related in any way. So she has no known relations to lawyer. \nAnswer: (c)</td></tr><tr><td>Instruct: Read the sentence and determine the relation between MEF and Myanmar Equestrian Federation quoted by &lt;t&gt;. \nSentence: YANGON , Dec. 27 -LRB- Xinhua -RRB- - Myanmar will hold a horse race in Yangon to commemorate the country ’63rd Anniversary Independence Day , the &lt;t&gt; Myanmar Equestrian Federation &lt;t&gt; -LRB- &lt;t&gt; MEF &lt;t&gt; -RRB- confirmed to Xinhua on Monday. \n(a) MEF is also known as Myanmar Equestrian Federation \n(b) MEF has political affiliation with Myanmar Equestrian Federation \n(c) MEF has no known relations to Myanmar Equestrian Federation \nAnalysis: The symbols -LRB- and -RRB- in the sentence stand for left and right round brackets and are used to enclose the abbreviation ‘MEF’ to indicate that it is a replacement for the longer name ‘Myanmar Equestrian Federation. So MEF is also known as Myanmar Equestrian Federation. \nAnswer: (a)</td></tr><tr><td>Instruct: Read the sentence and determine the relation between Douglas Flint and chairman quoted by &lt;t&gt;. \nSentence: At the same time , Chief Financial Officer &lt;t&gt; Douglas Flint &lt;t&gt; will become &lt;t&gt; chairman &lt;t&gt; , succeeding Stephen Green who is leaving to take a government job. \n(a) Douglas Flint has no known relations to chairman \n(b) Douglas Flint is a chairman \n(c) Douglas Flint is the employee of chairman \nAnalysis: The sentence states that Chief Financial Officer Douglas Flint Douglas Flint will succeed Stephen Green as a chairman. So Douglas Flint is a chairman. \nAnswer: (b)</td></tr><tr><td>Instruct: Read the sentence and determine the relation between FAA and U.S. quoted by &lt;t&gt;. \nSentence: On its Web site , the &lt;t&gt; U.S. &lt;t&gt; FAA &lt;t&gt; says the Category 2 rating means the country lacks the laws or regulations that are needed for the certification and oversight of air carriers , according to minimum international standards. \n(a) FAA is also known as U.S. \n(b) FAA has no known relations to U.S. \n(c) FAA has a headquarters in the country U.S. \nAnalysis: The sentence states that the FAA says the Category 2 rating means the country lacks the laws or regulations needed for the certification and oversight of air carriers, indicating that the FAA is responsible for overseeing aviation regulations in the country. Actually the FAA (Federal Aviation Administration) is a U.S. government agency responsible for regulating and overseeing civil aviation in the United States, and it has its headquarters in Washington, D.C.. So FAA has a headquarters in the country U.S.. \nAnswer: (c)</td></tr></table>",
                "caption": "Table 17: Demo examples used in TACREV dataset. We color the subject and object entities in blue."
            }
        ]
    },
    {
        "page_id": 24,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    320,
                    729,
                    2158,
                    2869
                ],
                "angle": 0,
                "content": "<table><tr><td>Instruct: Read following sentences and identify what event is triggered by the word loan quoted by &lt;t&gt;. \nSentence: Separately, former WorldCom CEO Bernard Ebbers failed on April 29 to make a first repayment of 25 million dollars, plus interest, on a 400-million - dollar &lt;t&gt; from MCI, the Journal said, citing SEC documents. \n(a) The word loan does not trigger any known event. \n(b) The word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something. \n(c) The word loan triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet. \nAnalysis: In the given sentence, the word loan is used to describe the 400-million-dollar loan from MCI to former WorldCom CEO Bernard Ebbers, which he failed to repay on time. This situation clearly involves a transfer of money without the context of purchasing something, which falls under the TRANSFER-MONEY event. So the word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something. \nAnswer: (b)</td></tr><tr><td>Instruct: Read following sentences and identify what event is triggered by the words treated quoted by &lt;t&gt;. \nSentence: When she &#x27;s in Germany, Lynch will be &lt;t&gt; treated &lt;t&gt; for bullet wounds and broken bones. \n(a) The word treated triggers an INJURE event: a PERSON gets/got injured whether it occurs accidentally, intentionally or even self-inflicted. \n(b) The word treated does not trigger any known event. \n(c) The word treated triggers a TRANSPORT event: an ARTifact (WEAPON or VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY, FACILITY, LOCATION) to another. \nAnalysis: The sentence suggests that Lynch has already been injured and will receive medical treatment in Germany for her injuries. The word &#x27;treated&#x27; simply describes the medical care she will receive and does not indicate a new event or action taking place. So the word treated does not trigger any known event. \nAnswer: (b)</td></tr><tr><td>Instruct: Read following sentences and identify what event is triggered by the words buy quoted by &lt;t&gt;. \nSentence: And I won&#x27;t dwell on the irony of an Oracle employee being driven out of Oracle, starting his own company, and forcing Ellison to spend $10.3 billion to get his company – but not him – back (though it does rather delightfully remind me of Coca - Cola basically giving away the bottling franchise and then spending billions to &lt;t&gt; buy &lt;t&gt; it back). \n(a) The word buy triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet. \n(b) The word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization. \n(c) The word buy does not trigger any known event. \nAnalysis: In the given sentence, the word buy is used to describe the action of Oracle spending $10.3 billion to get a company back. This clearly involves the transfer of ownership of the company from one entity to another. So the word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization. \nAnswer: (b)</td></tr><tr><td>Instruct: Read following sentences and identify what event is triggered by the words set quoted by &lt;t&gt;. \nSentence: British forces also began establishing the country&#x27;s first postwar administration Tuesday, granting a local sheik power to &lt;t&gt; set up an administrative committee representing the groups in the region. \n(a) The word set triggers a START-POSITION event: a PERSON elected or appointed begins working for (or changes offices within) an ORGANIZATION or GOVERNMENT. \n(b) The word set triggers a START-ORG event: a new ORGANIZATION is created. \n(c) The word set does not trigger any known event. \nAnalysis: The phrase &#x27;set up&#x27; specifically implies the creation or establishment of a new organization or entity, rather than simply the word &#x27;set&#x27;. So the word set does not trigger any known event. \nAnswer: (c)</td></tr></table>",
                "caption": "Table 18: Demo examples used in ACE05 dataset. We color the trigger word in blue."
            }
        ]
    },
    {
        "page_id": 25,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    325,
                    378,
                    2155,
                    3248
                ],
                "angle": 0,
                "content": "<table><tr><td>Entity</td><td>Template</td></tr><tr><td>no-entity</td><td>{ent} do/does not belong to any known entities.</td></tr><tr><td>person-artist/author</td><td>{ent} is an artist or author.</td></tr><tr><td>person-actor</td><td>{ent} is an actor.</td></tr><tr><td>art-written</td><td>{ent} is a kind of written.</td></tr><tr><td>person-director</td><td>{ent} is a director.</td></tr><tr><td>person-other</td><td>{ent} is a person, but not affiliated with following professions: actor, artist, athlete, author, director, politician, scholar, soldier.</td></tr><tr><td>organization-other</td><td>{ent} pertains to an organization that does not fall under the categories of company, educational institution, government, media, political party, religion, sports league, sports team, band or musical group.</td></tr><tr><td>organization-company</td><td>{ent} is a company</td></tr><tr><td>organization-sportsteam</td><td>{ent} is a sports team</td></tr><tr><td>organization-sportsleague</td><td>{ent} is a sports league</td></tr><tr><td>product-car</td><td>{ent} is a kind of car</td></tr><tr><td>event-protest</td><td>{ent} refers to a protest, uprising or revolution event</td></tr><tr><td>organization-government/governmentagency</td><td>{ent} refers to a government or governmental agency</td></tr><tr><td>other-biologything</td><td>{ent} is a special term about biology / life science.</td></tr><tr><td>location-GPE</td><td>{ent} is a kind of geopolitical entity</td></tr><tr><td>location-other</td><td>{ent} is a geographic locaton that does not fall under the categories of geopolitical entity, body of water, island, mountain, park, road, railway and transit.</td></tr><tr><td>person-athlete</td><td>{ent} is an athlete or coach.</td></tr><tr><td>art-broadcastprogram</td><td>{ent} is a broadcast program.</td></tr><tr><td>product-other</td><td>{ent} is a kind of product that does not fall under the categories of airplane, train, ship, car, weapon, food, electronic game and software.</td></tr><tr><td>building-other</td><td>{ent} is a kind of building that does not fall under the categories of airport, hospital, hotel, library, restaurant, sports facility and theater</td></tr><tr><td>product-weapon</td><td>{ent} is a kind of weapon.</td></tr><tr><td>building-airport</td><td>{ent} is an airport.</td></tr><tr><td>building-sportsfacility</td><td>{ent} is a sports facility building.</td></tr><tr><td>person-scholar</td><td>{ent} is a scholar.</td></tr><tr><td>art-music</td><td>{ent} is a music.</td></tr><tr><td>event-other</td><td>{ent} refers to some event except attack, election, natural disaster, protest, revolution and sports</td></tr><tr><td>other-language</td><td>{ent} is a kind of human language.</td></tr><tr><td>other-chemicalthing</td><td>{ent} is some special term about chemical science.</td></tr><tr><td>art-film</td><td>{ent} is a film.</td></tr><tr><td>building-hospital</td><td>{ent} is a hospital.</td></tr><tr><td>other-law</td><td>{ent} is a legal document, a term or a convention in legal sense.</td></tr><tr><td>product-airplane</td><td>{ent} is kind of airplane product.</td></tr><tr><td>location-road/railway/highway/transit</td><td>{ent} is a geographic position about roadways, railways, highways or public transit systems.</td></tr><tr><td>person-soldier</td><td>{ent} is a soldier</td></tr><tr><td>location-mountain</td><td>{ent} is geographic position about mountain.</td></tr><tr><td>organization-education</td><td>{ent} is an educational institute/organization.</td></tr><tr><td>organization-media/newspaper</td><td>{ent} is a media/newspaper organization.</td></tr></table>",
                "caption": "Table 19: Templates for FewNERD dataset, where {ent} is the placeholder for entity type."
            }
        ]
    },
    {
        "page_id": 26,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    285,
                    294,
                    2178,
                    2332
                ],
                "angle": 0,
                "content": "<table><tr><td>product-software</td><td>{ent} is a software product.</td></tr><tr><td>location-island</td><td>{ent} is geographic position about island.</td></tr><tr><td>location-bodiesofwater building-library</td><td>{ent} is geographic position situated near a body of water. {ent} is a library.</td></tr><tr><td>other-astronomything</td><td>{ent} is a special term about astronomy.</td></tr><tr><td>person-politician</td><td>{ent} is a politician or lawyer or judge.</td></tr><tr><td>building-hotel</td><td>{ent} is a hotel building.</td></tr><tr><td>product-game</td><td>{ent} is a electronic game product.</td></tr><tr><td>other-award</td><td>{ent} is a kind of award.</td></tr><tr><td>event-sportsevent</td><td>{ent} refers to some event related to sports.</td></tr><tr><td>organization-showorganization</td><td>{ent} is a band or musical organization.</td></tr><tr><td>other-educationaldegree</td><td>{ent} is a kind of educational degree.</td></tr><tr><td>building-theater</td><td>{ent} is a theater.</td></tr><tr><td>other-disease</td><td>{ent} is a kind of disease.</td></tr><tr><td>event-election</td><td>{ent} is an event about election.</td></tr><tr><td>organization-politicalparty</td><td>{ent} is a political party/organization.</td></tr><tr><td>other-currency</td><td>{ent} is a kind of currency.</td></tr><tr><td>event-attack/battle/war/militaryconflict</td><td>{ent} is an event about attack, battle, war or military conflict.</td></tr><tr><td>product-ship</td><td>{ent} is a ship.</td></tr><tr><td>building-restaurant</td><td>{ent} is a restaurant.</td></tr><tr><td>other-livingthing</td><td>{ent} is a living animal/crystal/organism.</td></tr><tr><td>art-other</td><td>{ent} is a work of art, but not belong to the categories of music, film, written art, broadcast or painting.</td></tr><tr><td>event-disaster</td><td>{ent} is a natural disaster event.</td></tr><tr><td>organization-religion</td><td>{ent} is a religious organization.</td></tr><tr><td>other-medical</td><td>{ent} refers to some kind of medicine-entity.</td></tr><tr><td>location-park</td><td>{ent} is a park.</td></tr><tr><td>other-god</td><td>{ent} is a god in some legend/religious story.</td></tr><tr><td>product-food</td><td>{ent} is a kind of food.</td></tr><tr><td>product-train</td><td>{ent} is a kind of train(vehicle).</td></tr><tr><td>art-painting</td><td>{ent} is an art painting.</td></tr></table>"
            }
        ]
    },
    {
        "page_id": 27,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    354,
                    431,
                    2121,
                    3248
                ],
                "angle": 0,
                "content": "<table><tr><td>Relation</td><td>Template</td></tr><tr><td>no Relation</td><td>{subj} has no known relations to {obj}</td></tr><tr><td>per:stateorprovince_of_death</td><td>{subj} died in the state or province {obj}</td></tr><tr><td>per:title</td><td>{subj} is a {obj}</td></tr><tr><td>org:member_of</td><td>{subj} is the member of {obj}</td></tr><tr><td>per:other_family</td><td>{subj} is the other family member of {obj}</td></tr><tr><td>org:country_of_HEADquarters</td><td>{subj} has a headquarters in the country {obj}</td></tr><tr><td>org:parents</td><td>{subj} has the parent company {obj}</td></tr><tr><td>per:stateorprovince_of_birth</td><td>{subj} was born in the state or province {obj}</td></tr><tr><td>per:spouse</td><td>{subj} is the spouse of {obj}</td></tr><tr><td>per:origin</td><td>{subj} has the nationality {obj}</td></tr><tr><td>per:date_of_birth</td><td>{subj} has birthday on {obj}</td></tr><tr><td>per:schools_attended</td><td>{subj} studied in {obj}</td></tr><tr><td>org:members</td><td>{subj} has the member {obj}</td></tr><tr><td>org:founded</td><td>{subj} was founded in {obj}</td></tr><tr><td>per:stateorprovinces_of_residence</td><td>{subj} lives in the state or province {obj}</td></tr><tr><td>per:date_of_death</td><td>{subj} died in the date {obj}</td></tr><tr><td>org:shareholders</td><td>{subj} has shares hold in {obj}</td></tr><tr><td>org:website</td><td>{subj} has the website {obj}</td></tr><tr><td>org:subsidiaries</td><td>{subj} owns {obj}</td></tr><tr><td>per:charges</td><td>{subj} is convicted of {obj}</td></tr><tr><td>org:dissolved</td><td>{subj} dissolved in {obj}</td></tr><tr><td>org:stateorprovince_of_HEADquarters</td><td>{subj} has a headquarters in the state or province {obj}</td></tr><tr><td>per:country_of_birth</td><td>{subj} was born in the country {obj}</td></tr><tr><td>per:siblings</td><td>{subj} is the siblings of {obj}</td></tr><tr><td>org:top_membership/employees</td><td>{subj} has the high level member {obj}</td></tr><tr><td>per:cause_of_death</td><td>{subj} died because of {obj}</td></tr><tr><td>per:alternate_names</td><td>{subj} has the alternate name {obj}</td></tr><tr><td>org:number_of_employees/members</td><td>{subj} has the number of employees {obj}</td></tr><tr><td>per:cities_of_residence</td><td>{subj} lives in the city {obj}</td></tr><tr><td>org:city_of_HEADquarters</td><td>{subj} has a headquarters in the city {obj}</td></tr><tr><td>per:children</td><td>{subj} is the parent of {obj}</td></tr><tr><td>per:employee_of</td><td>{subj} is the employee of {obj}</td></tr><tr><td>org:political/religious_affiliation</td><td>{subj} has political affiliation with {obj}</td></tr><tr><td>per:parents</td><td>{subj} has the parent {obj}</td></tr><tr><td>per:city_of_birth</td><td>{subj} was born in the city {obj}</td></tr><tr><td>per:age</td><td>{subj} has the age {obj}</td></tr><tr><td>per:countries_of_residence</td><td>{subj} lives in the country {obj}</td></tr><tr><td>org:alternate_names</td><td>{subj} is also known as {obj}</td></tr><tr><td>per:religion</td><td>{subj} has the religion {obj}</td></tr><tr><td>per:city_of_death</td><td>{subj} died in the city {obj}</td></tr><tr><td>per:country_of_death</td><td>{subj} died in the country {obj}</td></tr><tr><td>org:founded_by</td><td>{subj} was founded by {obj}</td></tr></table>",
                "caption": "Table 20: Templates for TACREV dataset, where {subj} and {obj} are the placeholders for subject and object entities. Copied from (Lu et al., 2022a)"
            }
        ]
    },
    {
        "page_id": 28,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    322,
                    378,
                    2155,
                    3388
                ],
                "angle": 0,
                "content": "<table><tr><td>Event</td><td>Template</td></tr><tr><td>no-event</td><td>The word {evt} does not trigger any known event.</td></tr><tr><td>Movement.Elect</td><td>The word {evt} triggers a TRANSPORT event: an ARTIFACT (WEAPON or VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY, FACILITY, LOCATION) to another.</td></tr><tr><td>Personnel.Elect</td><td>The word {evt} triggers an ELECT event which implies an election.</td></tr><tr><td>Personnel.Start-Position</td><td>The word {evt} triggers a START-POSITION event: a PERSON elected or appointed begins working for (or changes offices within) an ORGANIZATION or GOVERNMENT.</td></tr><tr><td>Personnel.Nominate</td><td>The word {evt} triggers a NOMINATE event: a PERSON is proposed for a position through official channels.</td></tr><tr><td>Conflict.Attack</td><td>The word {evt} triggers an ATTACK event: a violent physical act causing harm or damage.</td></tr><tr><td>Personnel.End-Position</td><td>The word {evt} triggers an END-POSITION event: a PERSON stops working for (or changes offices within) an ORGANIZATION or GOVERNMENT.</td></tr><tr><td>Contact.Meet</td><td>The word {evt} triggers a MEET event: two or more entities come together at a single location and interact with one another face-to-face.</td></tr><tr><td>Life.Marry</td><td>The word {evt} triggers a MARRY event: two people are married under the legal definition.</td></tr><tr><td>Contact.Phone-Write</td><td>The word {evt} triggers a PHONE-WRITE event: two or more people directly engage in discussion which does not take place &#x27;face-to-face&#x27;.</td></tr><tr><td>Transaction.Transform-Money</td><td>The word {evt} triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something.</td></tr><tr><td>Justice.Sue</td><td>The word {evt} triggers a SUE event: a court proceeding has been initiated for the purposes of determining the liability of a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime or neglecting a commitment</td></tr><tr><td>Conflict.Demonstrate</td><td>The word {evt} triggers a DEMONSTRATE event: a large number of people come together in a public area to protest or demand some sort of official action. For example: protests, sit-ins, strikes and riots.</td></tr><tr><td>Business.End-Org</td><td>The word {evt} triggers an END-ORG event: an ORGANIZATION ceases to exist (in other words, goes out of business).</td></tr><tr><td>Life.Injure</td><td>The word {evt} triggers an INJURE event: a PERSON gets/got injured whether it occurs accidentally, intentionally or even self-inflicted.</td></tr><tr><td>Life.Die</td><td>The word {evt} triggers a DIE event: a PERSON dies/died whether it occurs accidentally, intentionally or even self-inflicted.</td></tr><tr><td>Justice.Arrest-Jail</td><td>The word {evt} triggers a ARREST-JAIL event: a PERSON is sent to prison.</td></tr><tr><td>Transaction.Transfer-Ownership</td><td>The word {evt} triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization.</td></tr><tr><td>Justice.Execute</td><td>The word {evt} triggers an EXECUTE event: a PERSON is/was executed</td></tr><tr><td>Justice.Trial-Hearing</td><td>The word {evt} triggers a TRIAL-HEARING event: a court proceeding has been initiated for the purposes of determining the guilt or innocence of a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime.</td></tr><tr><td>Justice.Sentence</td><td>The word {evt} triggers a SENTENCE event: the punishment for the DEFENDANT is issued</td></tr><tr><td>Life.Be-Born</td><td>The word {evt} triggers a BE-BORN event: a PERSON is given birth to.</td></tr><tr><td>Justice.Charge-Indict</td><td>The word {evt} triggers a CHARGE-INDICT event: a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY is accused of a crime</td></tr><tr><td>Business.Start-Org</td><td>The word {evt} triggers a START-ORG event: a new ORGANIZATION is created.</td></tr><tr><td>Justice.Convict</td><td>The word {evt} triggers a CONVICT event: a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY is convicted whenever it has been found guilty of a CRIME.</td></tr><tr><td>Business.Declare-Bankruptcy</td><td>The word {evt} triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet.</td></tr><tr><td>Justice/release-Parole</td><td>The word {evt} triggers a RELEASE-PAROLE event.</td></tr></table>",
                "caption": "Table 21: Templates for ACE05 dataset, where \\(\\{\\mathrm{evt}\\}\\) is the placeholder for event type."
            }
        ]
    },
    {
        "page_id": 29,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    287,
                    294,
                    2111,
                    1017
                ],
                "angle": 0,
                "content": "<table><tr><td>Justice.Fine</td><td>The word {evt} triggers a FINE event: a GEOPOLITICAL ENTITY, PERSON OR ORGANIZATION get financial punishment typically as a result of court proceedings.</td></tr><tr><td>Justice.Pardon</td><td>The word {evt} triggers a PARDON event: a head-of-state or their appointed representative lifts a sentence imposed by the judiciary.</td></tr><tr><td>Justice.Appeal</td><td>The word {evt} triggers a APPEAL event: the decision of a court is taken to a higher court for review</td></tr><tr><td>Business.Merge-Org</td><td>The word {evt} triggers a MERGE-ORG event: two or more ORGANIZATION Entities come together to form a new ORGANIZATION Entity.</td></tr><tr><td>Justice.Extradite</td><td>The word {evt} triggers a EXTRADITE event.</td></tr><tr><td>Life.Divorce</td><td>The word {evt} triggers a DIVORCE event: two people are officially divorced under the legal definition of divorce.</td></tr><tr><td>Justice.Acquit</td><td>The word {evt} triggers a ACQUIT event: a trial ends but fails to produce a conviction.</td></tr></table>"
            }
        ]
    }
]