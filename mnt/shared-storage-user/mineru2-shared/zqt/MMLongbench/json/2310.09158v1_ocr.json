[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "aside_text",
                "bbox": [
                    58,
                    907,
                    147,
                    2336
                ],
                "angle": 270,
                "content": "arXiv:2310.09158v1 [cs.AI] 13 Oct 2023"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    262,
                    336,
                    2284,
                    422
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    466,
                    455,
                    714,
                    511
                ],
                "angle": 0,
                "content": "Meiqi Chen"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    436,
                    514,
                    747,
                    561
                ],
                "angle": 0,
                "content": "Peking University"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    357,
                    561,
                    823,
                    607
                ],
                "angle": 0,
                "content": "meiqichen@stu.pku.edu.cn"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1178,
                    455,
                    1366,
                    504
                ],
                "angle": 0,
                "content": "Yubo Ma"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1221,
                    514,
                    1326,
                    551
                ],
                "angle": 0,
                "content": "S-Lab"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    976,
                    561,
                    1573,
                    610
                ],
                "angle": 0,
                "content": "Nanyang Technological University"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1833,
                    455,
                    2085,
                    511
                ],
                "angle": 0,
                "content": "Kaitao Song"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1749,
                    514,
                    2167,
                    551
                ],
                "angle": 0,
                "content": "Microsoft Research Asia"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    481,
                    650,
                    691,
                    696
                ],
                "angle": 0,
                "content": "Yixin Cao"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    288,
                    706,
                    887,
                    752
                ],
                "angle": 0,
                "content": "Singapore Management University"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1157,
                    650,
                    1387,
                    706
                ],
                "angle": 0,
                "content": "Yan Zhang"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1116,
                    706,
                    1428,
                    752
                ],
                "angle": 0,
                "content": "Peking University"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1810,
                    650,
                    2106,
                    706
                ],
                "angle": 0,
                "content": "Dongsheng Li"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1749,
                    706,
                    2164,
                    745
                ],
                "angle": 0,
                "content": "Microsoft Research Asia"
            },
            {
                "block_id": 16,
                "type": "title",
                "bbox": [
                    214,
                    785,
                    469,
                    828
                ],
                "angle": 0,
                "content": "ABSTRACT"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    209,
                    844,
                    1236,
                    1854
                ],
                "angle": 0,
                "content": "Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across different scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-LR) involving multi-hop reasoning for evaluation and pre-training. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness and necessity of teaching LLMs with logic and provide insights for solving practical tasks with LLMs in future work. Codes will be available at https://github.com/chenmeiqi/Teach-LLM-LR."
            },
            {
                "block_id": 18,
                "type": "title",
                "bbox": [
                    214,
                    1894,
                    563,
                    1940
                ],
                "angle": 0,
                "content": "CCS CONCEPTS"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    211,
                    1956,
                    1229,
                    2049
                ],
                "angle": 0,
                "content": "- Computing methodologies \\(\\rightarrow\\) Knowledge representation and reasoning."
            },
            {
                "block_id": 20,
                "type": "title",
                "bbox": [
                    214,
                    2092,
                    484,
                    2135
                ],
                "angle": 0,
                "content": "KEYWORDS"
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    211,
                    2151,
                    1236,
                    2244
                ],
                "angle": 0,
                "content": "Large Language Models, Event Relation Extraction, Logical Reasoning"
            },
            {
                "block_id": 22,
                "type": "title",
                "bbox": [
                    214,
                    2300,
                    655,
                    2346
                ],
                "angle": 0,
                "content": "1 INTRODUCTION"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    209,
                    2362,
                    1231,
                    2772
                ],
                "angle": 0,
                "content": "Recently, Large Language Models (LLMs) have made incredible progress in many different downstream tasks, such as GPT-3 [3], ChatGPT [32], and LLaMA [39]. These models are typically trained on a combination of filtered web data and curated high-quality corpora (e.g., social media conversations, books, or technical publications) [34]. Studies have indicated that the emergent abilities of LLMs can exhibit promising reasoning capabilities [45] and the curation process is necessary to produce their zero-shot generalization abilities [34]."
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    211,
                    2775,
                    1234,
                    2960
                ],
                "angle": 0,
                "content": "Despite these notable achievements, current LLMs still have some issues in producing high-quality content with fluency and reliability. A good content generator should produce logically consistent answers that are reasonable for given or prior constraints."
            },
            {
                "block_id": 25,
                "type": "image",
                "bbox": [
                    1397,
                    785,
                    2254,
                    1808
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 1: An example of LLM in generating logically inconsistent answers. We let LLM (e.g., ChatGPT) answer the relations between events “FIRE” and “collapsed” from the given passage. We can find that LLM predicts an incorrect answer (i.e., SIMULTANEOUS) because it ignores some prior logic in this scenario, leading to logical inconsistency."
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    1308,
                    2227,
                    2333,
                    2729
                ],
                "angle": 0,
                "content": "However, LLMs sometimes output counterfactuals when dealing with practical tasks that require rigorous logical reasoning. As showcased in Figure 1, ChatGPT predicts the temporal and causal relations between events \"FIRE\" and \"collapsed\" being \"simultaneous\" and \"cause\". According to the prior logical constraints, we could readily claim the predictions are not fully correct even before reading the context, because \"simultaneous\" and \"cause\" conflict with each other in terms of semantics. Some works [26, 33, 48] attribute these phenomena to their inherent deficiencies (e.g., hallucination, unfaithfulness), however, how to disentangle and improve the capability of LLMs in these tasks is still an open problem."
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    1308,
                    2729,
                    2333,
                    2960
                ],
                "angle": 0,
                "content": "To deeply understand the deficiencies of LLMs in logical reasoning and explore the corresponding solutions, in this paper, we conduct an in-depth investigation of LLMs in solving reasoning tasks from multiple dimensions. We first evaluate the capacity of LLMs in two practical scenarios including event relation extraction"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    221,
                    356,
                    1229,
                    1537
                ],
                "angle": 0,
                "content": "and deductive reasoning tasks, both of which demand rigorous reasoning ability to infer [38, 42]. Our experimental results show that: 1) Even the cutting-edge LLMs still generate large amounts of inconsistent answers, e.g., over \\(60\\%\\) of the answers from ChatGPT on the MAVEN-ERE [42] dataset are logically inconsistent as shown in Figure 2; 2) Chain-of-thought (CoT) prompting [47], like \"Let's think step by step\" could stimulate the reasoning abilities for LLMs. However, some inherent issues (e.g., hallucination, unfaithfulness) in the LLM will cause such generated rationale to be unreliable or inconsistent; 3) Providing relevant logic to LLMs improves performance, but injecting irrelevant logic introduces fluctuations in results. Therefore, how to obtain the relevant logic and inject its information into LLMs is a non-trivial problem, deserving further exploration; 4) To verify the capacity of LLM for more complex reasoning, we contribute a synthesized dataset (i.e., LLM-LR) for evaluation, which involves multiple hops of logical reasoning. LLM-LR is automatically constructed by applying logic programming [11, 22] on our collected logical constraints, which could provide logical reasoning instances with any number of hops. Results show that as the number of logical hops increases (\\(2 \\sim 10\\) hops), LLMs struggle to output correct answers, and the proportion of logically inconsistent answers steadily rises. This indicates that LLMs will perform worse when the reasoning becomes more abstract and complicated. Therefore, how to alleviate the aforementioned issues and enable LLMs with a more powerful ability of logical reasoning is the critical point of our paper."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    221,
                    1544,
                    1229,
                    2541
                ],
                "angle": 0,
                "content": "Based on these findings, we put forward a series of solutions to teach LLMs to generate answers with better logical consistency. Here, we divide the procedure for teaching LLMs logical reasoning into three different kinds of approaches according to the ways of logic acquisition: 1) Generative-based approach, which encourages LLMs to generate reasoning rationale themselves, inspired by CoT prompting. In this paradigm, we find that incorporating logical constraints into LLM instruction will bring substantial improvements, but the uncertainty of the generated rationales may also bring some biases, leading to an incorrect subsequent answer; 2) Retrieval-based approach, which provides our manually designed logical constraints, then retrieves relevant contents and adds them to the LLM instruction. This kind of approach ensures the correctness of logical constraints and significantly improves performance, but requires some hand-crafted engineering; 3) Pretraining-based approach, which uses our curated dataset LLM-LR introduced before to train LLMs to perform complex logical reasoning. The pretraining dataset consists of 6776 instances containing \\(2 \\sim 5\\) hops of logical reasoning. This strategy encodes logic in model parameters inherently, while also requiring additional training time. Therefore, how to choose the most suitable strategy can be a trade-off based on the practical scenario."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    209,
                    2547,
                    1229,
                    2960
                ],
                "angle": 0,
                "content": "Furthermore, based on the above framework, we also conduct extensive quantitative and qualitative analyses on different tasks to validate the effectiveness of teaching LLMs with logic and provide insights for future work: 1) We investigate whether to add logical constraints before obtaining results or later, and find that directly conveying constraints to LLMs is more effective than adding post-processing operations based on the results; 2) Compared with the setting that uses more demonstrations, incorporating logical constraints into prompts can achieve better performance with fewer"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1308,
                    356,
                    2333,
                    851
                ],
                "angle": 0,
                "content": "demonstrations. This phenomenon further indicates that it is important to teach LLMs to balance demonstrations and logical constraints; 3) Benefits from LLMs' powerful interactive ability, we can further improve the performance through multi-turn conversation enhanced by iterative retrievals. However, when there are too many iterations, LLMs may have the problem of overthinking - more useless and redundant information interferes with their predictions; 4) When trained on LLM-LR, LLMs such as LlaMA2-13B [39] can achieve better performance, even surpassing that of greater LLMs (e.g., ChatGPT, 175B), which validates the effectiveness of our curated dataset."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1310,
                    858,
                    2328,
                    943
                ],
                "angle": 0,
                "content": "Overall, the contributions of our paper can be summarized as follows:"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1313,
                    963,
                    2328,
                    1095
                ],
                "angle": 0,
                "content": "- We provide an in-depth investigation of the logical inconsistency problems of current LLMs in solving practical tasks, and indicate the deficiency of LLMs in utilizing logic."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1313,
                    1102,
                    2328,
                    1369
                ],
                "angle": 0,
                "content": "- To enhance the reliability of the content generated by LLMs, we propose several solutions to incorporate relevant logic. Based on our approach, we construct a synthesized dataset (LLM-LR) involving multi-hop reasoning. By leveraging the LLM-LR, we endow specialized LLMs with logical reasoning ability, which enhances LLMs to generate more logically consistent answers."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1313,
                    1376,
                    2328,
                    1508
                ],
                "angle": 0,
                "content": "- Experimental results on different tasks with quantitative and qualitative analyses verify the importance of our investigation in empowering LLMs with logical reasoning."
            },
            {
                "block_id": 10,
                "type": "list",
                "bbox": [
                    1313,
                    963,
                    2328,
                    1508
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1313,
                    1554,
                    1751,
                    1600
                ],
                "angle": 0,
                "content": "2 PRELIMINARIES"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1310,
                    1617,
                    2328,
                    1706
                ],
                "angle": 0,
                "content": "In this section, we first introduce two tasks that this paper mainly explores."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1313,
                    1755,
                    1966,
                    1801
                ],
                "angle": 0,
                "content": "2.1 Event Relation Extraction"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1308,
                    1815,
                    2333,
                    2362
                ],
                "angle": 0,
                "content": "Event relation extraction (ERE) [21, 42] aims to identify relations (i.e., Coreference, Temporal, Causal, and Subevent) between two events in the text. Traditionally, it can be formulated as a multi-label classification problem, determining one label for each relation type. Compared with other common tasks, ERE tasks should take more considerations about the logical constraints between event relations (e.g., the constraints in Figure 1), and guarantee the predictions should conform to those constraints to avoid counterfactuals. Therefore, we need to rigorously consider the logical constraints between each event pair during the prediction. To better evaluate the capability of LLMs on the ERE task, we formulate the logical consistency for evaluation."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1308,
                    2366,
                    2333,
                    2953
                ],
                "angle": 0,
                "content": "Logical consistency plays a crucial role in understanding the relations between events. To assess the logical consistency, we collect a comprehensive set including 11 logical constraints for all relations between two events, as shown in Table 4. Based on these logical constraints, we introduce a logical inconsistency metric (i.e., LI) to measure LLMs' ability on ERE tasks. Specifically, for the answers of LLMs, logical inconsistency is calculated as the ratio of the number of conflicts (i.e., the answers that conflict with the given logical constraints) to the total number of combinations (i.e., all combinations of each two relations). To better illustrate the computation of logical inconsistency, here we introduce an example (as shown in Figure 1): if an LLM outputs the relations between two events as \"NO_COREFERENCE, SIMULTANEOUS, CAUSE, NO_SUBEVENT\"."
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    232,
                    363,
                    742,
                    732
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    747,
                    369,
                    1262,
                    732
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 2: Performance of ChatGPT in the pilot study."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    209,
                    924,
                    1231,
                    1425
                ],
                "angle": 0,
                "content": "Among these, \"SIMULTANEOUS\" and \"CAUSE\" are identified as conflicting with each other based on the logical constraints we have defined, resulting in a single conflict. Now, regarding the total number of combinations: for each pair of events, we have 4 types of relations to determine. The total combinations between these relations are calculated using the combinatorial formula: \\( 4*(4 - 1) / 2 = 6 \\). So, there are 6 possible combinations between the relations for two events. Hence, the logical inconsistency in this example is computed as \\( \\mathrm{LI} = 1 / 6 \\) (or approximately \\( 16.7\\% \\)). Obviously, given the logical constraints, an algorithm can be designed to automatically detect conflicts and calculate the value of logical inconsistency."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    209,
                    1428,
                    1229,
                    1567
                ],
                "angle": 0,
                "content": "Overall, intuitively, the smaller the value of logical inconsistency is, the more self-consistent and reasonable answer that LLM can produce. More descriptions about this task are in Appendix A."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    214,
                    1613,
                    765,
                    1666
                ],
                "angle": 0,
                "content": "2.2 Deductive Reasoning"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    209,
                    1676,
                    1231,
                    2316
                ],
                "angle": 0,
                "content": "Deductive reasoning typically begins with known facts and rules, then iteratively makes new inferences until the desired statement can be either confirmed or refuted [35]. To ensure the accuracy of these inferences, each step in deductive reasoning must adhere to the known logical constraints (rules). More specifically, the logical constraints in deductive reasoning are usually specific to individual cases rather than being universally applicable like that in the ERE task. Consequently, when engaging in deductive reasoning, it is essential to assess and apply logical constraints based on the distinct circumstances and known facts of each example to arrive at accurate conclusions. For the calculation of logical inconsistency of deductive reasoning, we need to manually count the number of reasoning processes generated by LLMs that are inconsistent with known facts or rules, and then calculate the proportion."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    214,
                    2366,
                    1226,
                    2412
                ],
                "angle": 0,
                "content": "3 UNVEILING LLMS IN LOGICAL REASONING"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    209,
                    2425,
                    1229,
                    2517
                ],
                "angle": 0,
                "content": "In this section, we conduct a pilot study to investigate how current LLMs exhibit in reasoning tasks and how logic benefits LLMs."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    214,
                    2567,
                    1022,
                    2673
                ],
                "angle": 0,
                "content": "3.1 How Is LLM Performing Practical Reasoning Tasks?"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    209,
                    2682,
                    1231,
                    2960
                ],
                "angle": 0,
                "content": "3.1.1 Data Source. We conduct a manual evaluation on MAVEN-ERE [42] and ProofWriter [38]. MAVEN-ERE is a unified large-scale dataset for the ERE task, which needs to identify four types of relations. ProofWriter is a commonly used dataset for deductive logical reasoning, where each example is a pair of (problem, goal) and the label is selected from {Proved, Disproved, Unknown}. To"
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    1351,
                    363,
                    2340,
                    874
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 3: Error analysis of ChatGPT in the pilot study by human evaluation. CE and FE denote incorrectness and unfaithfulness errors, respectively."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1310,
                    1161,
                    2328,
                    1254
                ],
                "angle": 0,
                "content": "employ our investigation, we randomly choose 100 samples (50 from MAVEN-ERE and 50 from ProofWriter)."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1310,
                    1280,
                    2335,
                    1692
                ],
                "angle": 0,
                "content": "3.1.2 Experimental Setup. Our experiments are conducted as a multi-turn conversation for zero-shot reasoning, to leverage LLM's interaction ability. Given a task input \\((X)\\), we also write a prompt \\((T)\\) describing the task, and let LLM generate output \\((Y)\\) by answering the given query. We also add \"Let's think step by step\" before each answer for prediction generation, which is a simple but effective trick to improve zero-shot reasoning for LLMs [19]. We adopt ChatGPT as the backbone and manually check its generated rationales under the following three settings:"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1313,
                    1702,
                    2330,
                    1745
                ],
                "angle": 0,
                "content": "- Vanilla LLM (i.e., ChatGPT) without any additional information;"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1313,
                    1749,
                    2330,
                    1834
                ],
                "angle": 0,
                "content": "- LLM (i.e., ChatGPT) plus the most relevant (i.e., ground truth) logic;"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    1313,
                    1841,
                    2187,
                    1884
                ],
                "angle": 0,
                "content": "- LLM (i.e., ChatGPT) plus irrelevant logical constraints."
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    1313,
                    1702,
                    2330,
                    1884
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1313,
                    1897,
                    2119,
                    1943
                ],
                "angle": 0,
                "content": "The prompt examples can be found in Figure \\(10\\sim 13\\)"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    1308,
                    1970,
                    2333,
                    2382
                ],
                "angle": 0,
                "content": "3.1.3 Analysis. As shown in Figure 2, we visualize the micro-F1 and the proportion of logically inconsistent answers generated by ChatGPT. We find that no matter whether on MAVEN-ERE or ProofWriter, Vanilla ChatGPT always achieves a bad result with low micro-F1 scores and high inconsistency values (e.g., \\(15\\%\\) micro-F1 and \\(63\\%\\) inconsistent answers on MAVEN-ERE), which indicates the deficiency of LLM in solving complex reasoning tasks. To investigate this issue in depth, we conduct analyses from the following two aspects."
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    1308,
                    2409,
                    2335,
                    2960
                ],
                "angle": 0,
                "content": "What Is The Relation Between Logical Consistency And Model Performance? From Figure 2, we find that: 1) The model directly receives significant improvements on both MAVEN-ERE and ProofWriter when adding relevant logic; 2) When adding some irrelevant logic, the results show some fluctuations (exaltation in MAVEN-ERE and degeneration in ProofWriter). That means directly adding logic without any constraints will bring some uncertainty; 3) Typically, a higher logical inconsistency corresponds to a poorer micro-F1, however, rectifying logical inconsistency does not necessarily lead to the same degree of increase in micro-F1. Generally, an intuitive observation is that incorporating relevant logic into the LLM instruction will be very helpful in solving reasoning"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    211,
                    353,
                    1229,
                    445
                ],
                "angle": 0,
                "content": "tasks. So, the challenges are how to obtain these relevant logic and how to utilize them for LLMs."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    209,
                    478,
                    1231,
                    1346
                ],
                "angle": 0,
                "content": "What Types of Errors Does LLM Usually Make? To delve into a deep understanding of the failures that Vanilla LLM encounters in logical reasoning, we also conduct a detailed error analysis for this. Here, we divide the error types into two aspects: 1) Incorrectness to the Constraint (CE): whether the rationale generated by LLM is wrong (CE1), incomplete (CE2), or redundant (CE3) compared with the true logical constraints. 2) Unfaithfulness to the Reasoning Process (FE): where LLM does not correctly use the constraints. We define two types of errors upon FE, i.e., i) Wrong start, LLM begins with an irrelevant fact or focuses on an improper perspective for the correct answer (FE1). ii) Wrong process, LLM starts from a proper point, but makes mistakes during the reasoning process (FE2). Annotators are asked to review 100 predictions generated by ChatGPT and mark the error types. Results in Figure 3 show that: 1) The quality of constraints produced by the Vanilla ChatGPT is not high enough, which limits its subsequent reasoning ability. 2) Incorporating relevant logical constraints could guarantee the correctness of constraints and thus greatly improve the generation quality of ChatGPT in faithfulness."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    214,
                    1392,
                    1229,
                    1501
                ],
                "angle": 0,
                "content": "3.2 How Is LLM Performing Abstract Multi-hop Reasoning?"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    211,
                    1508,
                    1231,
                    1696
                ],
                "angle": 0,
                "content": "Based on the above analyses, we can confirm the deficiency of LLMs in solving complex reasoning tasks and the effectiveness of incorporating logical constraints. Nevertheless, we also want to explore how LLMs exhibit in more challenging settings."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    209,
                    1725,
                    1231,
                    2316
                ],
                "angle": 0,
                "content": "3.2.1 Data Source. Considering that existing datasets lack multi-hop instances, we construct a synthesized dataset (LLM-LR) to evaluate the ability of LLMs to perform multi-hop reasoning. Specifically, we first collect 39 additional logical constraints for all the high-order relations among three events, as outlined in Table 7. The collection is based on transitive dependency (i.e., one event may affect another through an intermediate event). For example, BEFORE(A, B) ∧ BEFORE(B, C) → BEFORE(A, C) means that \"If event A happens BEFORE event B, and event B happens BEFORE event C, then event A happens BEFORE event C\". Thereby, we obtain a comprehensive set containing a total of 50 logical constraints (along with the 11 constraints between two events we introduced in Section 2.1)."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    209,
                    2319,
                    1231,
                    2960
                ],
                "angle": 0,
                "content": "As the number of events further increases (i.e., \\(>3\\)), there are more complex interactions involved, and it is inefficient to list all the constraints manually at this time. To address this, we introduce logic programming [11, 22] to automatically generate new event relations by inputting the known constraints and relations. We employ a forward- and backward-chaining rule-based method utilizing Prolog [10] as the foundation for our logic programming approach. For instance, when dealing with temporal relations involving four events \\((A, B, C, \\text{and } D)\\), given the known relations: “BEFORE(A, B) ∧ SIMULTANEOUS(B, C) ∧ OVERLAP(C, D)”, our logic programming approach can deduce a “BEFORE(A, D)” conclusion according to the constraints in Table 7. Then, we provide a task description and use the given relations as the input case to let LLMs reason the relation between events \\((A, D)\\), i.e., a 3-hop query. We could"
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    1326,
                    359,
                    2366,
                    739
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 4: Multi-hop reasoning performance of LLMs evaluated on our LLM-LR dataset."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1308,
                    963,
                    2335,
                    1197
                ],
                "angle": 0,
                "content": "use the description text provided in Table 6 to convert the symbolic representation into natural language forms. The conclusion deduced by our logic engine will serve as the ground truth to check LLMs' answers. A pseudo-code can be found in Appendix D.1 and a prompt example is in Figure 14."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1308,
                    1221,
                    2335,
                    1633
                ],
                "angle": 0,
                "content": "3.2.2 Experimental Setup. For evaluation, we randomly generate 50 samples for each \\(2 \\sim 10\\)-hop reasoning. In addition to the three variants of ChatGPT (gpt-turbo, text-davinci-003, and gpt4), we employ another two open-source LLMs (Vicuna-13B-v1.3<sup>1</sup> and Llama2-13B) for evaluation. Note that: 1) for 2-hop reasoning (i.e., high-order relations among three events), there are only 39 samples. 2) Our approach allows for the extension of the reasoning path, but we report results for clarity and due to the length limits of LLMs, covering only 2 to 10 hops."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1308,
                    1656,
                    2335,
                    2118
                ],
                "angle": 0,
                "content": "3.2.3 Analysis. As shown in Figure 4, we visualize the micro-F1 and the proportion of logically inconsistent answers generated by LLMs. We can see that: 1) When the number of hops is relatively small (i.e., \\(2 \\sim 5\\) hops), the performance of GPT-4 is outstanding compared with other models. 2) With the increase of hops, all the LLMs perform worse when the reasoning becomes more complicated, and the proportion of logically inconsistent answers is gradually increasing. Among them, Vicuna-13B fully fails after 6 hops and could not output any correct answers. This further demonstrates the necessity of teaching LLMs logical reasoning."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1310,
                    2154,
                    2256,
                    2201
                ],
                "angle": 0,
                "content": "4 TEACHING LLMS LOGICAL REASONING"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1308,
                    2217,
                    2335,
                    2494
                ],
                "angle": 0,
                "content": "Based on the aforementioned analysis, we expect to explore how to empower LLMs' capability with logical reasoning. Therefore, in this section, we first introduce the instruction-following technique we use in Section 4.1 and then propose three different approaches to instruct LLMs to generate answers with better logical consistency (Section \\(4.2 \\sim 4.4\\))."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1310,
                    2531,
                    2052,
                    2583
                ],
                "angle": 0,
                "content": "4.1 In-Context Learning for LLMs"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1308,
                    2593,
                    2333,
                    2880
                ],
                "angle": 0,
                "content": "We deploy LLMs for event relation extraction and deductive reasoning tasks via in-context learning (ICL, [3, 32]). Given a task input \\((X)\\), we also write a prompt \\((T)\\) describing the task, then further provide several demonstrations \\(D = \\{D_{i}\\}_{i=1}^{|D|}\\), where \\(D_{i} = (X_{i}, Y_{i})\\) are used for few-shot learning. Then, the LLM generates output \\((Y)\\) by completing the prompt \\((Y = \\mathcal{M}(T, D, X))\\), where \\(\\mathcal{M}\\) denotes"
            },
            {
                "block_id": 17,
                "type": "page_footnote",
                "bbox": [
                    1313,
                    2917,
                    1652,
                    2960
                ],
                "angle": 0,
                "content": "\\(^{1}\\)https://lmsys.org/projects/"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    359,
                    349,
                    2231,
                    1541
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 5: Incorporate logical constraints to LLMs by using generative, retrieval, and pretraining-based approaches. The dashed boxes indicate answers output by LLMs, and the underlined texts indicate the logical constraints."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    209,
                    1758,
                    1231,
                    1989
                ],
                "angle": 0,
                "content": "the LLM. In such a setting, the LLM can follow the structure of the provided demonstrates to output the expected format of answers for subsequent automatic evaluation. Besides, the whole process does not require any gradient update, allowing LLMs to generate predictions without massive training data."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    209,
                    2036,
                    1231,
                    2316
                ],
                "angle": 0,
                "content": "Compared Models. We choose three variants of ChatGPT (gpt-turbo, text-davinci-003, and gpt4), Vicuna-13B-v1.3, and Llama2-13B as the main experimental LLMs for evaluation. We also provide two fine-tuning RoBERTa-large baselines (one-shot and fully fine-tuned) for comparison. The training details of RoBERTa-large can be found in Appendix B.2."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    209,
                    2362,
                    1231,
                    2960
                ],
                "angle": 0,
                "content": "Dataset Construction. Our main experiments are evaluated on MAVEN-ERE, Causal-TimeBank [28], and ProofWriter. For the ERE task, we focus on relations between two events and conduct sampling at the sentence level. The samples of the two events that do not have any relations will be excluded. Here, we randomly sample 500 examples from the test set of MAVEN-ERE and 100 examples from the test set of Causal-TimeBank as our testbed. For the deductive reasoning task, we use the OWA subset of ProofWriter, which is divided into five parts, each part requiring 0, 1, 2, 3, and 5 hops of reasoning, respectively. We evaluate the hardest 5-hop subset. To reduce the computation cost, we randomly sample 200 examples in the test set and ensure a balanced label distribution. Other details can be found in Appendix B.1."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1310,
                    1758,
                    2328,
                    1940
                ],
                "angle": 0,
                "content": "Evaluation Metrics. We adopt the averaged micro-F1 score as the evaluation metric and also report the logical inconsistency (defined in Section 2.1) on ERE datasets. The reported value is averaged by the results of three runs to reduce random fluctuation."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1313,
                    1986,
                    2047,
                    2039
                ],
                "angle": 0,
                "content": "4.2 Generative-based Approaches"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1310,
                    2046,
                    2328,
                    2138
                ],
                "angle": 0,
                "content": "Generative-based approaches mean we let LLMs generate logic by using a form of one-shot ICL. Here, we study three variants:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1313,
                    2151,
                    2330,
                    2244
                ],
                "angle": 0,
                "content": "(1) Vanilla ICL: which utilizes the common prompts consisting of a task description, one demonstration, and the input case."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1313,
                    2244,
                    2333,
                    2425
                ],
                "angle": 0,
                "content": "(2) Vanilla ICL plus CoT: which first bootstraps rationales by using chain-of-thought as intermediate reasoning steps following the style of the given demonstration, then output answers. Rationales here do not involve the content of logical constraints."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1313,
                    2425,
                    2333,
                    2791
                ],
                "angle": 0,
                "content": "(3) CoT with self-generated logical constraints: which teaches LLMs to generate and utilize logical constraints based on CoT (Figure 5 (a)). Specifically, it will first extract the obvious relations/facts and generate the relevant logical constraints according to the extracted relations/facts, then we enforce LLMs to infer the remaining relations/facts based on the constraints and the known relations/facts. The prompt example can be seen in Appendix H.2."
            },
            {
                "block_id": 13,
                "type": "list",
                "bbox": [
                    1313,
                    2151,
                    2333,
                    2791
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1310,
                    2818,
                    2333,
                    2956
                ],
                "angle": 0,
                "content": "4.2.1 Results. From Table 1, We could observe that: 1) Compared with a smaller language model (SLM, i.e., RoBERTa-large), the generalization ability of vanilla LLMs on both two tasks under the"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    316,
                    343,
                    2231,
                    1369
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\" colspan=\"2\">Model</td><td colspan=\"2\">MAVEN-ERE</td><td colspan=\"2\">Causal-TimeBank</td><td>ProofWriter</td></tr><tr><td>Micro-F1 (%)</td><td>LI (%) ↓</td><td>Micro-F1 (%)</td><td>LI (%) ↓</td><td>Micro-F1 (%)</td></tr><tr><td colspan=\"2\">RoBERTa-Large (one-shot)</td><td>17.4</td><td>54.8</td><td>-</td><td>-</td><td>35</td></tr><tr><td colspan=\"2\">RoBERTa-Large (fully fine-tuned)</td><td>56.8</td><td>6.4</td><td>22.2</td><td>36.2</td><td>63</td></tr><tr><td rowspan=\"3\">Turbo</td><td>vanilla ICL</td><td>18.0</td><td>53.3</td><td>19.0</td><td>54.0</td><td>39</td></tr><tr><td>vanilla ICL w. CoT</td><td>18.8</td><td>49.3</td><td>17.0</td><td>30.3</td><td>42</td></tr><tr><td>CoT w. logical constraints</td><td>25.3</td><td>37.9</td><td>27.0</td><td>12.8</td><td>45</td></tr><tr><td rowspan=\"3\">Davinci</td><td>vanilla ICL</td><td>21.6</td><td>49.1</td><td>18.0</td><td>58.8</td><td>30</td></tr><tr><td>vanilla ICL w. CoT</td><td>20.5</td><td>60.5</td><td>21.0</td><td>64.7</td><td>40</td></tr><tr><td>CoT w. logical constraints</td><td>24.8</td><td>5.5</td><td>23.0</td><td>39.2</td><td>49</td></tr><tr><td rowspan=\"3\">GPT-4</td><td>vanilla ICL</td><td>29.3</td><td>50.7</td><td>22.5</td><td>30.5</td><td>47</td></tr><tr><td>vanilla ICL w. CoT</td><td>30.3</td><td>36.7</td><td>23.0</td><td>35.0</td><td>67</td></tr><tr><td>CoT w. logical constraints</td><td>32.3</td><td>13.7</td><td>24.5</td><td>24.0</td><td>70</td></tr><tr><td rowspan=\"3\">Vicuna</td><td>vanilla ICL</td><td>13.8</td><td>25.4</td><td>4.5</td><td>84.1</td><td>37</td></tr><tr><td>vanilla ICL w. CoT</td><td>11.6</td><td>47.4</td><td>6.0</td><td>57.6</td><td>40</td></tr><tr><td>CoT w. logical constraints</td><td>14.9</td><td>21.7</td><td>8.0</td><td>33.1</td><td>42</td></tr><tr><td rowspan=\"3\">Llama2</td><td>vanilla ICL</td><td>17.0</td><td>54.6</td><td>11.5</td><td>26.7</td><td>29</td></tr><tr><td>vanilla ICL w. CoT</td><td>17.8</td><td>58.4</td><td>10.5</td><td>33.6</td><td>31</td></tr><tr><td>CoT w. logical constraints</td><td>21.5</td><td>18.9</td><td>13.0</td><td>18.1</td><td>40</td></tr></table>",
                "caption": "Table 1: ChatGPT (gpt-turbo, text-davinci-003, and gpt-4), Vicuna-13B, and Llama2-13B's performance on MAVEN-ERE, Causal-TimeBank, and ProofWriter. We report averaged micro-F1 scores here and \"LI\" denotes the logical inconsistency metric. For each dataset, the best result of each LLM is in bold. RoBERTa-Large (one-shot) fails to output any correct answers on Causal-TimeBank."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    209,
                    1626,
                    1231,
                    2128
                ],
                "angle": 0,
                "content": "one-shot setting is remarkable, but there is still a gap with the fully-finetuned baseline. 2) Directly using CoT to infer logic does not help much for ERE tasks, a possible reason is that the inherent issues may lead to the failure of LLM in the precise rationale generation (i.e., a high ratio of logical inconsistency). We give a case study for this in Appendix E. 3) When using generative-based approaches to encourage LLMs to produce logical constraints in the reasoning process, LLMs can significantly improve their performance on both two tasks. It is worth mentioning that the performance of GPT-4 (CoT w. logical constraints) could even surpass that of the fully fine-tuned baseline on the Proofwriter dataset."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    214,
                    2207,
                    905,
                    2263
                ],
                "angle": 0,
                "content": "4.3 Retrieval-based Approaches"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    209,
                    2270,
                    1231,
                    2636
                ],
                "angle": 0,
                "content": "Although generative-based approaches enable models to automatically generate and utilize logic, the prediction of LLMs is usually uncertain and inaccuracy. Therefore, we also provide retrieval-based approaches, which aim to obtain relevant logic from our pre-defined constraints (Figure 5 (b)). We mainly conduct experiments on the ERE task by utilizing the collected logical constraints. Specifically, We take the collected 11 constraints in Section 2.1 as the retrieval set, and our solutions include:"
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    214,
                    2679,
                    1231,
                    2772
                ],
                "angle": 0,
                "content": "(1) with all logical constraints: which directly adds all the 11 logical constraints in the set."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    214,
                    2775,
                    1234,
                    2960
                ],
                "angle": 0,
                "content": "(2) with retrieved logical constraints: which means that we first detect logically inconsistent answers based on the prediction of LLMs, and then retrieve the corresponding information if we find any conflict. Finally, we add it to the LLM instruction and"
            },
            {
                "block_id": 9,
                "type": "list",
                "bbox": [
                    214,
                    2679,
                    1234,
                    2960
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    1366,
                    1626,
                    2328,
                    1716
                ],
                "angle": 0,
                "content": "let LLMs re-generate the answers. Please see Appendix C.1 for details."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1315,
                    1719,
                    2333,
                    1950
                ],
                "angle": 0,
                "content": "(3) with post-processing: which first obtains the answers of LLMs, then automatically generates some logically consistent candidates according to the constraints, and randomly selects one of them as the final answer. This approach ensures that there is no logical conflict \\((\\mathrm{LI} = 0\\%)\\). Please see Appendix C.2 for details."
            },
            {
                "block_id": 12,
                "type": "list",
                "bbox": [
                    1315,
                    1626,
                    2333,
                    1950
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1308,
                    1993,
                    2335,
                    2547
                ],
                "angle": 0,
                "content": "4.3.1 Main Results. From Table 2, We could observe that: 1) When using retrieval-based approaches to obtain logic constraints and incorporate them into LLM instruction, the logical inconsistency of LLMs' answers is greatly reduced and the overall performance on both two tasks is further improved. 2) Although our post-processing guarantees the absence of logical conflicts (resulting in LI of \\(0\\%\\)), it may severely affect the quality of the whole generation. On one hand, the semantics of the post-processing answer may be far from the ground truth due to the random selection. On the other hand, the size of the candidate set for each case will also affect the performance. It may also need more operations at the post-processing stage, which we leave as future work."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1310,
                    2590,
                    2335,
                    2682
                ],
                "angle": 0,
                "content": "4.3.2 Ablation Study. We conduct an ablation study using ChatGPT (gpt-turbo) in this subsection."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1308,
                    2729,
                    2335,
                    2963
                ],
                "angle": 0,
                "content": "Demonstrations. Following previous experiences [3], we also append demonstrations into the prompt to investigate how logical constraints will affect when combined with different numbers of demonstrations. Here, we select different numbers of demonstration samples \\( K \\) from \\( \\{1, 5, 10, 20\\} \\). The experiments are tested on vanilla"
            }
        ]
    },
    {
        "page_id": 6,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    464,
                    343,
                    2088,
                    1257
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\" colspan=\"2\">Model</td><td colspan=\"2\">MAVEN-ERE</td><td colspan=\"2\">Causal-TimeBank</td></tr><tr><td>Micro-F1 (%)</td><td>LI (%) ↓</td><td>Micro-F1 (%)</td><td>LI (%) ↓</td></tr><tr><td rowspan=\"3\">Turbo</td><td>w. all logical constraints</td><td>20.8</td><td>30.9</td><td>20.0</td><td>36.8</td></tr><tr><td>w. retrieved logical constraints</td><td>22.3</td><td>30.2</td><td>22.0</td><td>11.3</td></tr><tr><td>w. post-processing</td><td>14.0</td><td>0</td><td>15.0</td><td>0</td></tr><tr><td rowspan=\"3\">Davinci</td><td>w. all logical constraints</td><td>27.0</td><td>25.6</td><td>31.0</td><td>21.8</td></tr><tr><td>w. retrieved logical constraints</td><td>27.8</td><td>30.8</td><td>22.0</td><td>40.5</td></tr><tr><td>w. post-processing</td><td>14.8</td><td>0</td><td>19.0</td><td>0</td></tr><tr><td rowspan=\"3\">GPT-4</td><td>w. all logical constraints</td><td>37.3</td><td>8.3</td><td>26.0</td><td>20.0</td></tr><tr><td>w. retrieved logical constraints</td><td>33.5</td><td>28.8</td><td>24.0</td><td>13.5</td></tr><tr><td>w. post-processing</td><td>17.0</td><td>0</td><td>19.0</td><td>0</td></tr><tr><td rowspan=\"3\">Vicuna</td><td>w. all logical constraints</td><td>15.2</td><td>37.6</td><td>11.0</td><td>23.5</td></tr><tr><td>w. retrieved logical constraints</td><td>15.7</td><td>33.2</td><td>10.0</td><td>26.7</td></tr><tr><td>w. post-processing</td><td>9.8</td><td>0</td><td>9.0</td><td>0</td></tr><tr><td rowspan=\"3\">Llama2</td><td>w. all logical constraints</td><td>19.5</td><td>34.6</td><td>10.0</td><td>23.5</td></tr><tr><td>w. retrieved logical constraints</td><td>18.3</td><td>38.2</td><td>9.5</td><td>26.7</td></tr><tr><td>w. post-processing</td><td>12.0</td><td>0</td><td>9.5</td><td>0</td></tr></table>",
                "caption": "Table 2: Retrieval-based approaches' performance on MAVEN-ERE and Causal-TimeBank. For each dataset, the best result of each LLM is in bold and the second-best result is underlined."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    209,
                    1425,
                    1231,
                    2112
                ],
                "angle": 0,
                "content": "ICL and ICL plus all logical constraints. From Figure 6, we can observe that: 1) When the number of demonstrations increases from 1 to 5, we can observe an evident improvement, but the subsequent improvements are limited when continue to increase the number of demonstrations (e.g., \\(\\geq 10\\)); 2) Adding logical constraints into LLM instructions can provide stable improvements, especially with more demonstrations. 3) The performance of incorporating logical constraints with a smaller number of demonstrations can even surpass that of prompts with only a larger number of demonstrations (e.g., the performance of using 5 demonstrations on MAVEN-ERE w. logical constraints, \\(25.7\\%\\), surpasses that of 10 demonstrations w/o. logical constraints, \\(24.5\\%\\)). This indicates that it is important to tell LLMs both \"What\" (demonstrations) and \"How\" (logical constraints). Overall, these studies further confirm the merits of using logical constraints in solving reasoning tasks."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    209,
                    2118,
                    1231,
                    2762
                ],
                "angle": 0,
                "content": "Iterative Retrieval. Considering the outstanding ability of LLMs in interaction, we further explore whether we can introduce logical constraints into the multi-turn conversation (for the prompt design, please see Appendix H.3). Here, we adopt a retrieval-based approach to incorporate logical constraints iteratively and the results are shown in Figure 6. We find that the logical inconsistency of answers will gradually decrease with the increase of iterations, but the overall micro-F1 seems relatively stable. We guess the main reason for this phenomenon is the overthinking of LLMs, as although it can bring more reasoning rationale, it possibly produces correct but more useless or abundant information when inferring multiple iterations. Overall, instructing LLM with logic is beneficial for conversation, but how to support longer information is still challenging."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    214,
                    2805,
                    912,
                    2857
                ],
                "angle": 0,
                "content": "4.4 Pretraining-based Approach"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    209,
                    2864,
                    1231,
                    2960
                ],
                "angle": 0,
                "content": "Although the retrieval-based approach guarantees the correctness of logical constraints, it still needs to interact with an external set"
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    1341,
                    1435,
                    2368,
                    1811
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 6: Ablation Study of ChatGPT for demonstrations and iterative retrieval, where \"lc\" denotes the logical constraints."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1308,
                    2039,
                    2333,
                    2501
                ],
                "angle": 0,
                "content": "constantly. Therefore, we provide a pretraining-based approach to embed the logical constraints into LLMs themselves. We use the logic programming approach introduced in Section 3.2 to automatically generate 6776 instances containing all the \\(2 \\sim 5\\)-hop reasoning data. We do not generate longer hops for training here considering the computation complexity and the length limitation of LLMs. The dataset statistic can be found in Table 5. Then, we train LLMs to perform complex logical reasoning based on the curated dataset LLM-LR. Finally, we conduct inference with the trained LLMs. An example of the training data can be seen in Figure 5 (c) or Figure 14."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1310,
                    2521,
                    2340,
                    2709
                ],
                "angle": 0,
                "content": "4.4.1 Pretraining Details. We adopt Vicuna-13B-v1.3 and Llama2-13B as the base models and employ the LoRA [14] technique. During pre-training, only LoRA parameters are optimized. Other Details can be found in Appendix G."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1308,
                    2729,
                    2335,
                    2963
                ],
                "angle": 0,
                "content": "4.4.2 Results. As shown in Table 3, we find that: 1) Once trained on LLM-LR, the performance of LlaMA2-13B and Vicuna-13B improves greatly compared with that of Table 1 and 2, especially on the baselines without logical constraints. 2) The performance of LlaMA2-13B-PT could even surpass that of some greater LLMs (e.g.,"
            },
            {
                "block_id": 13,
                "type": "list",
                "bbox": [
                    1308,
                    2039,
                    2340,
                    2963
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 7,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    938,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    387,
                    343,
                    2164,
                    1122
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\" colspan=\"2\">Model</td><td colspan=\"2\">MAVEN-ERE</td><td colspan=\"2\">Causal-TimeBank</td></tr><tr><td>Micro-F1 (%)</td><td>LI (%) ↓</td><td>Micro-F1 (%)</td><td>LI (%) ↓</td></tr><tr><td rowspan=\"6\">Vicuna-13B-PT</td><td>vanilla ICL</td><td>15.3 (+1.5%)</td><td>21.2 (-4.2%)</td><td>8.0 (+3.5%)</td><td>35.5 (-48.6%)</td></tr><tr><td>vanilla ICL w. CoT</td><td>15.8 (+4.2%)</td><td>17.8 (-29.6%)</td><td>7.5 (+1.5%)</td><td>52.5 (-5.1%)</td></tr><tr><td>CoT w. logical constraints</td><td>18.0 (+3.1%)</td><td>6.0 (-15.7%)</td><td>8.5 (+0.5%)</td><td>2.0 (-31.1%)</td></tr><tr><td>w. all logical constraints</td><td>16.3 (+1.1%)</td><td>8.7 (-28.9%)</td><td>12.1 (+1.1%)</td><td>0 (-23.5%)</td></tr><tr><td>w. retrieved logical constraints</td><td>16.1 (+0.4%)</td><td>19.0 (-14.2%)</td><td>10.7 (+0.7%)</td><td>9.5 (-17.2%)</td></tr><tr><td>w. post-processing</td><td>11.0 (+1.2%)</td><td>0 (-)</td><td>8.0 (-1.0%)</td><td>0 (-)</td></tr><tr><td rowspan=\"6\">Llama2-13B-PT</td><td>vanilla ICL</td><td>19.0 (+2.0%)</td><td>45.8 (-8.8%)</td><td>12.0 (+0.5%)</td><td>22.7 (-4.0%)</td></tr><tr><td>vanilla ICL w. CoT</td><td>22.1 (+4.3%)</td><td>42.9 (-15.5%)</td><td>11.5 (+1.0%)</td><td>3.0 (-30.6%)</td></tr><tr><td>CoT w. logical constraints</td><td>26.4 (+4.9%)</td><td>15.7 (-3.2%)</td><td>13.3 (+0.3%)</td><td>13.0 (-5.1%)</td></tr><tr><td>w. all logical constraints</td><td>20.2 (+0.7%)</td><td>28.7 (-5.9%)</td><td>12.0 (+2.0%)</td><td>23.0 (-0.5%)</td></tr><tr><td>w. retrieved logical constraints</td><td>18.7 (+0.4%)</td><td>34.2 (-4.0%)</td><td>11.0 (+1.5%)</td><td>19.4 (-7.3%)</td></tr><tr><td>w. post-processing</td><td>11.0 (-1.0%)</td><td>0 (-)</td><td>11.0 (+1.5%)</td><td>0 (-)</td></tr></table>",
                "caption": "Table 3: Vicuna-13B and Llama2-13B's performance on MAVEN-ERE and Causal-TimeBank after pre-training on LLM-LR."
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    298,
                    1234,
                    1058,
                    2121
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 7: Case study on Llama-2-13B before and after pretraining (PT)."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    211,
                    2310,
                    1231,
                    2409
                ],
                "angle": 0,
                "content": "vanilla ChatGPT, 175B), which further validates the importance of teaching LLM with logic in solving reasoning tasks."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    211,
                    2425,
                    1231,
                    2659
                ],
                "angle": 0,
                "content": "4.4.3 Case Study. In Figure 7, We conduct a case study of Llama2-13B's answers to the same input before and after pre-training. From Figure 7 we can see that LlaMA2-13B-PT could output the correct answers after pre-training on LLM-LR, which validates the effectiveness of our pre-training approach."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    214,
                    2692,
                    663,
                    2742
                ],
                "angle": 0,
                "content": "5 RELATED WORK"
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    214,
                    2758,
                    966,
                    2811
                ],
                "angle": 0,
                "content": "5.1 Large Language Models (LLMs)"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    211,
                    2818,
                    1231,
                    2960
                ],
                "angle": 0,
                "content": "We are fortunate to witness the surging development of Large Language Models (LLMs [3, 8, 9, 32]), and a series of work aiming to leverage the reasoning abilities of LLMs such as chain-of-thought"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1310,
                    1244,
                    2335,
                    1478
                ],
                "angle": 0,
                "content": "prompting [19, 46, 51], self verification [18, 44], self learning [15, 49], etc. However, recent studies show LLMs still stumble across generating hallucination and logic inconsistency [2, 13, 16, 17, 20]. To solve such challenges, our work explores teaching LLMs logical reasoning through various approaches."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1313,
                    1537,
                    2096,
                    1590
                ],
                "angle": 0,
                "content": "5.2 Event Relation Extraction (ERE)"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1308,
                    1597,
                    2335,
                    1963
                ],
                "angle": 0,
                "content": "Events play crucial roles in comprehending narratives, and understanding the complex relationships between events is essential to understanding the text [37]. Thus Event Relation Extraction (ERE) tasks are fundamental information extraction (IE) tasks and support various downstream applications [5, 36, 50]. Extensive studies have been carried out on ERE tasks, including different kinds of relations such as coreference relations [24, 25], temporal relations [30, 40], causal relations [4, 6, 7], and subevent relations [1, 41]."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1310,
                    1966,
                    2335,
                    2240
                ],
                "angle": 0,
                "content": "There also have been some recent explorations on how to leverage the power of LLMs on event-related information extraction tasks [12, 27, 43]. To the best of our knowledge, however, our work is the first to (1) design elaborate experiments to evaluate the performance of LLMs on the ERE task, and (2) analyze the logical reasoning abilities of LLMs using ERE as an intermediate task."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1313,
                    2300,
                    1693,
                    2349
                ],
                "angle": 0,
                "content": "6 CONCLUSION"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1308,
                    2362,
                    2335,
                    2960
                ],
                "angle": 0,
                "content": "In this paper, we conduct a detailed investigation on how to enhance LLMs to produce more logically consistent answers. Specifically, we first investigate the existing issues of current LLMs in doing some complex reasoning tasks (e.g., event relation extraction and deductive reasoning). Then, we study multiple strategies to obtain and utilize logic for LLMs, including generative-based, retrieval-based, and pretraining-based approaches. Based on our approach, we also contribute a synthesized dataset (LLM-LR) involving multihop reasoning for evaluation and pre-training. We show that LLMs are not logically consistent reasoners, but their performance could be improved if we explicitly teach them the logical constraints. Comprehensive quantitative and qualitative analyses have been conducted to further provide insights."
            }
        ]
    },
    {
        "page_id": 8,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1611,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    216,
                    349,
                    517,
                    396
                ],
                "angle": 0,
                "content": "REFERENCES"
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    232,
                    405,
                    1229,
                    541
                ],
                "angle": 0,
                "content": "[1] Mohammed Aldawsari and Mark Finlayson. 2019. Detecting Subevents using Discourse and Narrative Features. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 4780-4790. https://doi.org/10.18653/v1/P19-1471"
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    234,
                    541,
                    1229,
                    673
                ],
                "angle": 0,
                "content": "[2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023)."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    234,
                    676,
                    1229,
                    1069
                ],
                "angle": 0,
                "content": "[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/ hash/1457c0d6bfbcb4967418bf8ac142f64a-AAbstract.html"
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    234,
                    1072,
                    1229,
                    1204
                ],
                "angle": 0,
                "content": "[4] Tommaso Caselli and Piek Vossen. 2017. The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction. In Proceedings of the Events and Stories in the News Workshop. Association for Computational Linguistics, Vancouver, Canada, 77-86. https://doi.org/10.18653/v1/W17-2711"
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    234,
                    1204,
                    1229,
                    1366
                ],
                "angle": 0,
                "content": "[5] Snigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017. Story Comprehension for Predicting What Happens Next. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Copenhagen, Denmark, 1603-1614. https://doi.org/10.18653/v1/D17-1168"
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    234,
                    1369,
                    1229,
                    1570
                ],
                "angle": 0,
                "content": "[6] Meiqi Chen, Yixin Cao, Kunquan Deng, Mukai Li, Kun Wang, Jing Shao, and Yan Zhang. 2022. ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification. In Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 2118-2128. https://aclanthology.org/2022.coling-1.185"
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    234,
                    1570,
                    1229,
                    1702
                ],
                "angle": 0,
                "content": "[7] Meiqi Chen, Yixin Cao, Yan Zhang, and Zhiwei Liu. 2023. CHEER: Centrality-aware High-order Event Reasoning Network for Document-level Event Causality Identification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 10804-10816."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    234,
                    1702,
                    1229,
                    1834
                ],
                "angle": 0,
                "content": "[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint abs/2204.02311 (2022). https://arxiv.org/abs/2204.02311"
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    234,
                    1834,
                    1229,
                    1966
                ],
                "angle": 0,
                "content": "[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint abs/2210.11416 (2022). https://arxiv.org/abs/2210.11416"
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    221,
                    1966,
                    1229,
                    2036
                ],
                "angle": 0,
                "content": "[10] William F Clocksin and Christopher S Mellish. 2003. Programming in PROLOG. Springer Science & Business Media."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    221,
                    2036,
                    1229,
                    2102
                ],
                "angle": 0,
                "content": "[11] Bruce Frederiksen. 2008. Applying expert system technology to code reuse with Pyke. *PyCon: Chicago* (2008)."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    221,
                    2102,
                    1229,
                    2168
                ],
                "angle": 0,
                "content": "[12] Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the Feasibility of ChatGPT for Event Extraction. https://arxiv.org/abs/2303.03836"
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    221,
                    2168,
                    1229,
                    2267
                ],
                "angle": 0,
                "content": "[13] Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. arXiv:2212.07919 [cs.CL]"
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    221,
                    2267,
                    1229,
                    2399
                ],
                "angle": 0,
                "content": "[14] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https://openreview.net/forum?id=nZeVKeeFYf9"
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    221,
                    2399,
                    1229,
                    2498
                ],
                "angle": 0,
                "content": "[15] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large Language Models Can Self-Improve. arXiv:2210.11610 [cs.CL]"
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    221,
                    2498,
                    1229,
                    2567
                ],
                "angle": 0,
                "content": "[16] Myeongjun Jang and Thomas Lukasiewicz. 2023. Consistency Analysis of ChatGPT. arXiv:2303.06273 [cs.CL]"
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    221,
                    2567,
                    1229,
                    2666
                ],
                "angle": 0,
                "content": "[17] Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu, and Nancy F Chen. 2023. LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. arXiv preprint arXiv:2305.13718 (2023)."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    221,
                    2666,
                    1229,
                    2864
                ],
                "angle": 0,
                "content": "[18] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1266-1279. https://aclanthology.org/2022.emnlp-main.82"
            },
            {
                "block_id": 21,
                "type": "ref_text",
                "bbox": [
                    221,
                    2864,
                    1229,
                    2933
                ],
                "angle": 0,
                "content": "[19] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv preprint"
            },
            {
                "block_id": 22,
                "type": "list",
                "bbox": [
                    221,
                    405,
                    1229,
                    2933
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 23,
                "type": "ref_text",
                "bbox": [
                    1379,
                    359,
                    2050,
                    396
                ],
                "angle": 0,
                "content": "abs/2205.11916 (2022). https://arxiv.org/abs/2205.11916"
            },
            {
                "block_id": 24,
                "type": "ref_text",
                "bbox": [
                    1320,
                    396,
                    2330,
                    495
                ],
                "angle": 0,
                "content": "[20] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439 (2023)."
            },
            {
                "block_id": 25,
                "type": "ref_text",
                "bbox": [
                    1320,
                    495,
                    2330,
                    597
                ],
                "angle": 0,
                "content": "[21] Kang Liu, Yubo Chen, Jian Liu, Xinyu Zuo, and Jun Zhao. 2020. Extracting Events and Their Relations from Texts: A Survey on Recent Research Progress and Challenges. AI Open 1 (2020), 22-39. https://doi.org/10.1016/j.aiopen.2021.02.004"
            },
            {
                "block_id": 26,
                "type": "ref_text",
                "bbox": [
                    1320,
                    597,
                    2330,
                    660
                ],
                "angle": 0,
                "content": "[22] John W Lloyd. 2012. Foundations of logic programming. Springer Science & Business Media."
            },
            {
                "block_id": 27,
                "type": "ref_text",
                "bbox": [
                    1320,
                    660,
                    2330,
                    792
                ],
                "angle": 0,
                "content": "[23] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=Bkg6RiCqY7"
            },
            {
                "block_id": 28,
                "type": "ref_text",
                "bbox": [
                    1320,
                    792,
                    2330,
                    960
                ],
                "angle": 0,
                "content": "[24] Jing Lu and Vincent Ng. 2021. Conundrums in Event Coreference Resolution: Making Sense of the State of the Art. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 1368-1380. https://doi.org/10.18653/v1/2021.emnlp-main.103"
            },
            {
                "block_id": 29,
                "type": "ref_text",
                "bbox": [
                    1320,
                    960,
                    2330,
                    1026
                ],
                "angle": 0,
                "content": "[25] Yaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, and Le Sun. 2022. End-to-end neural event coreference resolution. Artificial Intelligence 303 (2022), 103632."
            },
            {
                "block_id": 30,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1026,
                    2330,
                    1128
                ],
                "angle": 0,
                "content": "[26] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful Chain-of-Thought Reasoning. ArXiv preprint abs/2301.13379 (2023). https://arxiv.org/abs/2301.13379"
            },
            {
                "block_id": 31,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1128,
                    2330,
                    1227
                ],
                "angle": 0,
                "content": "[27] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples! arXiv:2303.08559 [cs.CL]"
            },
            {
                "block_id": 32,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1227,
                    2330,
                    1392
                ],
                "angle": 0,
                "content": "[28] Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating Causality in the TempEval-3 Corpus. In Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL). Association for Computational Linguistics, Gothenburg, Sweden, 10-19. https://doi.org/10.3115/v1/W14-0702"
            },
            {
                "block_id": 33,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1392,
                    2330,
                    1557
                ],
                "angle": 0,
                "content": "[29] Paramita Mirza and Sara Tonelli. 2014. An Analysis of Causality between Events and its Relation to Temporal Information. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics, Dublin, Ireland, 2097-2106. https://aclanthology.org/C14-1198"
            },
            {
                "block_id": 34,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1557,
                    2330,
                    1722
                ],
                "angle": 0,
                "content": "[30] Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018. Joint Reasoning for Temporal and Causal Relations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 2278-2288. https://doi.org/10.18653/v1/P18-1212"
            },
            {
                "block_id": 35,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1722,
                    2330,
                    1890
                ],
                "angle": 0,
                "content": "[31] Tim O'Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation. In Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016). Association for Computational Linguistics, Austin, Texas, 47-56. https://doi.org/10.18653/v1/W16-5706"
            },
            {
                "block_id": 36,
                "type": "ref_text",
                "bbox": [
                    1320,
                    1890,
                    2330,
                    2022
                ],
                "angle": 0,
                "content": "[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730-27744."
            },
            {
                "block_id": 37,
                "type": "ref_text",
                "bbox": [
                    1320,
                    2022,
                    2330,
                    2154
                ],
                "angle": 0,
                "content": "[33] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. LogicLM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. ArXiv preprint abs/2305.12295 (2023). https://arxiv.org/abs/2305.12295"
            },
            {
                "block_id": 38,
                "type": "ref_text",
                "bbox": [
                    1320,
                    2154,
                    2330,
                    2319
                ],
                "angle": 0,
                "content": "[34] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 (2023)."
            },
            {
                "block_id": 39,
                "type": "ref_text",
                "bbox": [
                    1320,
                    2319,
                    2330,
                    2389
                ],
                "angle": 0,
                "content": "[35] David L Poole and Alan K Mackworth. 2010. Artificial Intelligence: foundations of computational agents. Cambridge University Press."
            },
            {
                "block_id": 40,
                "type": "ref_text",
                "bbox": [
                    1320,
                    2389,
                    2330,
                    2653
                ],
                "angle": 0,
                "content": "[36] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019. AAAI Press, 3027-3035. https://doi.org/10.1609/aaai.v33i01.33013027"
            },
            {
                "block_id": 41,
                "type": "ref_text",
                "bbox": [
                    1320,
                    2653,
                    2330,
                    2755
                ],
                "angle": 0,
                "content": "[37] Beth M. Sundheim. 1991. Evaluating Text Understanding Systems. In Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, February 19-22, 1991. https://aclanthology.org/H91-1093"
            },
            {
                "block_id": 42,
                "type": "ref_text",
                "bbox": [
                    1320,
                    2755,
                    2330,
                    2920
                ],
                "angle": 0,
                "content": "[38] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 3621-3634. https://doi.org/10.18653/v1/2021-findings-acl.317"
            },
            {
                "block_id": 43,
                "type": "list",
                "bbox": [
                    1320,
                    359,
                    2330,
                    2920
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 9,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    219,
                    359,
                    1234,
                    495
                ],
                "angle": 0,
                "content": "[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    216,
                    495,
                    1234,
                    663
                ],
                "angle": 0,
                "content": "[40] Haoyu Wang, Muhao Chen, Hongming Zhang, and Dan Roth. 2020. Joint Constrained Learning for Event-Event Relation Extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 696-706. https://doi.org/10.18653/v1/2020.emnlp-main.51"
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    216,
                    663,
                    1229,
                    828
                ],
                "angle": 0,
                "content": "[41] Haoyu Wang, Hongming Zhang, Muhao Chen, and Dan Roth. 2021. Learning Constraints and Descriptive Segmentation for Subevent Detection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 5216-5226. https://doi.org/10.18653/v1/2021.emnlp-main.423"
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    216,
                    828,
                    1229,
                    1059
                ],
                "angle": 0,
                "content": "[42] Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. 2022. MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 926–941. https://aclanthology.org/2022.emnlp-main.60"
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    216,
                    1059,
                    1229,
                    1128
                ],
                "angle": 0,
                "content": "[43] Xingyao Wang, Sha Li, and Heng Ji. 2022. Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language. arXiv:2210.12810 [cs.CL]"
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    216,
                    1128,
                    1229,
                    1227
                ],
                "angle": 0,
                "content": "[44] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint abs/2203.11171 (2022). https://arxiv.org/abs/2203.11171"
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    216,
                    1227,
                    1229,
                    1392
                ],
                "angle": 0,
                "content": "[45] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models. Trans. Mach. Learn. Res. (2022)."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    216,
                    1392,
                    1229,
                    1524
                ],
                "angle": 0,
                "content": "[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint abs/2201.11903 (2022). https://arxiv.org/abs/2201.11903"
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    216,
                    1524,
                    1229,
                    1626
                ],
                "angle": 0,
                "content": "[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    216,
                    1626,
                    1229,
                    1758
                ],
                "angle": 0,
                "content": "[48] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023. Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. ArXiv preprint abs/2306.09841 (2023). https://arxiv.org/abs/2306.09841"
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    216,
                    1758,
                    1229,
                    1890
                ],
                "angle": 0,
                "content": "[49] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrapping Reasoning With Reasoning. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=3ELRdg2sgI"
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    216,
                    1890,
                    1229,
                    2055
                ],
                "angle": 0,
                "content": "[50] Hongming Zhang, Daniel Khashabi, Yangqiu Song, and Dan Roth. 2020. TransOMCS: From Linguistic Graphs to Commonsense Knowledge. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, Christian Bessiere (Ed.). ijcai.org, 4004-4010. https://doi.org/10.24963/ijcai.2020/554"
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    216,
                    2055,
                    1229,
                    2161
                ],
                "angle": 0,
                "content": "[51] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. ArXiv preprint abs/2210.03493 (2022). https://arxiv.org/abs/2210.03493"
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    216,
                    359,
                    1234,
                    2161
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 10,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    214,
                    349,
                    1129,
                    396
                ],
                "angle": 0,
                "content": "A UNDERSTANDING EVENT RELATIONS"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    211,
                    412,
                    1231,
                    504
                ],
                "angle": 0,
                "content": "There are four kinds of widely-used event relations: coreference, temporal, causal, and subevent relations [31, 42]."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    214,
                    544,
                    1231,
                    679
                ],
                "angle": 0,
                "content": "(1) Coreference relations between events occur when multiple event mentions in a text refer to the same underlying event. We call these event mentions cluster."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    214,
                    683,
                    1229,
                    818
                ],
                "angle": 0,
                "content": "(2) Temporal relations refer to the temporal ordering of events based on their occurrence in time. In this paper, we consider seven different types of temporal relations:"
            },
            {
                "block_id": 6,
                "type": "list",
                "bbox": [
                    214,
                    544,
                    1231,
                    818
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    270,
                    821,
                    1231,
                    907
                ],
                "angle": 0,
                "content": "- NO_TEMPORAL: if there is no clear temporal relation between event \\( A \\) and \\( B \\)."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    270,
                    910,
                    1185,
                    953
                ],
                "angle": 0,
                "content": "BEFORE: if event \\(A\\) happened completely before event \\(B\\)."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    270,
                    956,
                    1088,
                    999
                ],
                "angle": 0,
                "content": "- OVERLAP: if event \\( A \\) has an overlap with event \\( B \\)."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    270,
                    1003,
                    1137,
                    1042
                ],
                "angle": 0,
                "content": "CONTAINS: if event \\(A\\) 's time contains event \\(B\\) 's time."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    270,
                    1046,
                    1229,
                    1092
                ],
                "angle": 0,
                "content": "- SIMULTANEOUS: if events A and B happen at the same time."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    270,
                    1095,
                    1040,
                    1135
                ],
                "angle": 0,
                "content": "- ENDS-ON: if event \\( A \\) ends when event \\( B \\) starts."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    270,
                    1138,
                    1229,
                    1224
                ],
                "angle": 0,
                "content": "- BEGINS-ON: if event \\( A \\) and event \\( B \\) start at the same time, but end at different times."
            },
            {
                "block_id": 14,
                "type": "list",
                "bbox": [
                    270,
                    821,
                    1231,
                    1224
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    267,
                    1230,
                    1231,
                    1316
                ],
                "angle": 0,
                "content": "In Figure 8, we list all the types of temporal relations and illustrate their distinctions on a unified timeline."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    214,
                    1323,
                    1231,
                    1593
                ],
                "angle": 0,
                "content": "(3) Causal relations refer to that one event (the cause) brings about or influences the occurrence of another event (the effect). They can be classified into two different types: CAUSE relation where the tail event is inevitable given the head event, and PRECONNECTION where the tail event would not have happened if the head event had not happened."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    214,
                    1597,
                    1229,
                    1824
                ],
                "angle": 0,
                "content": "(4) Subevent relations refer to the connection where one event (the subevent) is a component or a smaller part of another event (the main event). Identifying and understanding subevent relations helps to reveal the underlying hierarchy and organizational structure of events in a given text."
            },
            {
                "block_id": 18,
                "type": "list",
                "bbox": [
                    214,
                    1323,
                    1231,
                    1824
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    209,
                    1880,
                    1231,
                    2339
                ],
                "angle": 0,
                "content": "Event Relation Extraction. Event Relation Extraction (ERE) includes identifying coreference, temporal, causal, and subevent relations between every two events in the text. We formulate ERE as a multi-classification problem, determining one label (relation) for each of these four relation types. For coreference relations, the labels \\(\\in\\) {NO_COREFERENCE, COREFERENCE}; for temporal relations, the labels \\(\\in\\) {NO_TEMPORAL, BEFORE, OVERLAP, CONTAINS, SIMULTANEOUS, ENDS-ON, BEGINS-ON}; for causal relations, the labels \\(\\in\\) {NO_CAUSAL, PRECONDITION, CAUSE}; for subevent relations, the labels \\(\\in\\) {NO_SUBEVENT, SUBEVENT}."
            },
            {
                "block_id": 20,
                "type": "title",
                "bbox": [
                    214,
                    2412,
                    1198,
                    2511
                ],
                "angle": 0,
                "content": "B TRAINING DETAILS OF ROBERTA-LARGE ON TWO TASKS"
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    214,
                    2531,
                    775,
                    2577
                ],
                "angle": 0,
                "content": "B.1 Dataset Construction"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    209,
                    2590,
                    1231,
                    2953
                ],
                "angle": 0,
                "content": "MAVEN-ERE. contains 4,480 documents, 103,193 events coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude [42]. MAVEN-ERE has released the train and valid set, but does not release the ground-truth test set, so we randomly split its train set into train-valid sets with a ratio of 8:2, and then use its original valid set as the new test set."
            },
            {
                "block_id": 23,
                "type": "table",
                "bbox": [
                    1399,
                    346,
                    2246,
                    891
                ],
                "angle": 0,
                "content": "<table><tr><td>A</td><td>time</td><td>Relation(A, B)</td></tr><tr><td>B</td><td>BEFORE</td><td></td></tr><tr><td>B</td><td>OVERLAP</td><td></td></tr><tr><td>B</td><td>CONTAINS</td><td></td></tr><tr><td>B</td><td>SIMULTANEOUS</td><td></td></tr><tr><td>B</td><td>ENDS-ON</td><td></td></tr><tr><td>B</td><td>BEGINS-ON</td><td></td></tr></table>",
                "caption": "Figure 8: Interpretations of the temporal relation between two events A and B. Brackets represent time intervals along the time axis."
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    1308,
                    1214,
                    2333,
                    1584
                ],
                "angle": 0,
                "content": "Causal-TimeBank. contains 184 documents, 6,813 events, and 7,608 event pairs [29]. Among them, 318 event pairs are annotated with causal relations, and 6,115 event pairs are annotated with temporal relations. Due to Causal-TimeBank does not split train-valid/test sets, we randomly split it to train-valid/test sets with a ratio of 6:1:3. We do not evaluate coreference and subevent relations in the Causal-TimeBank dataset since there are no annotations for these two relation types."
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    1310,
                    1620,
                    2333,
                    1758
                ],
                "angle": 0,
                "content": "ProofWriter. We use the OWA subset of ProofWriter and consider the hardest 5-hop subset. The training, valid, and test sets contain 3000, 600, and 600 samples, respectively."
            },
            {
                "block_id": 27,
                "type": "title",
                "bbox": [
                    1313,
                    1815,
                    1846,
                    1867
                ],
                "angle": 0,
                "content": "B.2 Experimental Setup"
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    1308,
                    1874,
                    2333,
                    2148
                ],
                "angle": 0,
                "content": "Our experiments include two settings. (1) fully fine-tuned: we fine-tune SLMs with complete and abundant samples. This setting is for reference to see the performance limit of SLMs. (2) one-shot: we sample only one example for each label and construct a tiny training set. This setting is for direct comparison with our experiments on LLMs (similar training/demonstration sample number)."
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    1308,
                    2148,
                    2333,
                    2468
                ],
                "angle": 0,
                "content": "We implement vanilla fine-tuning approaches on three datasets and use RoBERTa-Large as backbones. We run each experiment on a single NVIDIA V100 GPU. We adopt the AdamW [23] optimizer with a linear scheduler and 0.1 warm-up steps. We set the weight-decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 16 with 20 or 50 epochs. We set the maximum input length as 256 and the learning rate as 2e-5."
            },
            {
                "block_id": 30,
                "type": "title",
                "bbox": [
                    1310,
                    2521,
                    2295,
                    2620
                ],
                "angle": 0,
                "content": "C LOGICAL CONSTRAINTS BETWEEN TWO EVENTS"
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    1308,
                    2636,
                    2333,
                    2960
                ],
                "angle": 0,
                "content": "In Table 4, we provide a comprehensive set of logical constraints for the relations between two events to assess their logical consistency. We also manually design description text for each constraint to let LLMs follow the prompt. As shown in Table 6, COREFERENCE(A, B) \\(\\rightarrow\\) ¬TEMPORAL(A, B), ¬CAUSAL(A, B), ¬SUBEVENT(A, B) indicates that \"if event A and event B have a coreference relation, they will not have temporal, causal, and subevent relations\"."
            }
        ]
    },
    {
        "page_id": 11,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    938,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    530,
                    343,
                    2017,
                    887
                ],
                "angle": 0,
                "content": "<table><tr><td>If Relation(A,B)</td><td>Then Relation (A,B)</td><td>Then Relation (B,A)</td></tr><tr><td>COREREFERENCE</td><td>-TEMPORAL, -CAUSAL, -SUBEVENT</td><td>COREREFERENCE</td></tr><tr><td>-TEMPORAL</td><td>-CAUSAL, -SUBEVENT</td><td>/</td></tr><tr><td>BEFORE</td><td>-COREREFERENCE, -SUBEVENT</td><td>-TEMPORAL</td></tr><tr><td>OVERLAP</td><td>-COREREFERENCE, -SUBEVENT</td><td>-TEMPORAL</td></tr><tr><td>CONTAINS</td><td>-COREREFERENCE, -CAUSAL</td><td>-TEMPORAL</td></tr><tr><td>SIMULTANEOUS</td><td>-COREREFERENCE, -CAUSAL, -SUBEVENT</td><td>SIMULTANEOUS</td></tr><tr><td>ENDS-ON</td><td>-COREREFERENCE, -CAUSAL, -SUBEVENT</td><td>-TEMPORAL</td></tr><tr><td>BEGINS-ON</td><td>-COREREFERENCE, -CAUSAL, -SUBEVENT</td><td>BEGINS-ON</td></tr><tr><td>CAUSE</td><td>-COREREFERENCE, BEFORE ∨ OVERLAP, -SUBEVENT</td><td>-TEMPORAL</td></tr><tr><td>PRECONDITION</td><td>-COREREFERENCE, BEFORE ∨ OVERLAP, -SUBEVENT</td><td>-TEMPORAL</td></tr><tr><td>SUBEVENT</td><td>-COREREFERENCE, CONTAINS -CAUSAL</td><td>-TEMPORAL</td></tr></table>",
                "caption": "Table 4: Logical Constraints of relations between two events, where \\(\\neg\\) denotes \"NOT\", \\(\\lor\\) denotes \"OR\"."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    209,
                    1049,
                    1144,
                    1158
                ],
                "angle": 0,
                "content": "C.1 An Example of Detecting Conflicts and Retrieving Relevant Constraints"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    209,
                    1168,
                    1231,
                    1349
                ],
                "angle": 0,
                "content": "As described above, for the ERE task, we meticulously collect 11 logical constraints covering all relations between two events. These constraints serve as our benchmark to identify inconsistencies in the predictions made by LLMs."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    209,
                    1353,
                    1231,
                    1570
                ],
                "angle": 0,
                "content": "Let us consider an illustrative example. If LLM produces an answer such as \"NO_COREFERENCE, SIMULTANEOUS, CAUSE, NO_SUBEVENT\" (refer to Figure 5), we could detect the inconsistency between \"SIMULTANEOUS\" and \"CAUSE\", as shown in Table 4:"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    1610,
                    1328,
                    1696
                ],
                "angle": 0,
                "content": "- A “SIMULTANEOUS” relation implies a “NO_CAUSAL” (\\( \\neg \\)CAUSAL) relation."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    280,
                    1702,
                    1231,
                    1791
                ],
                "angle": 0,
                "content": "- Conversely, a \"CAUSE\" relation suggests the presence of either a \"BEFORE\" or an \"OVERLAP\" relation."
            },
            {
                "block_id": 9,
                "type": "list",
                "bbox": [
                    280,
                    1610,
                    1328,
                    1791
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    209,
                    1824,
                    1236,
                    2191
                ],
                "angle": 0,
                "content": "Given this, \"SIMULTANEOUS\" and \"CAUSE\" are inherently contradictory, and they cannot coexist in a consistent prediction. To rectify this, we retrieve the associated textual description from Table 6. Specifically, the statements \"If event A CAUSES event \\( B \\), then event \\( A \\) happens BEFORE or OVERLAP event \\( B \\ldots \\)\" and \"If event \\( A \\) and event \\( B \\) happen SIMULTANEOUSly, then they won't have coreference, causal, and subevent relations ...\" are integrated into the LLM's instruction."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    211,
                    2257,
                    986,
                    2310
                ],
                "angle": 0,
                "content": "C.2 An Example of Post-processing"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    209,
                    2319,
                    1264,
                    2960
                ],
                "angle": 0,
                "content": "As shown in Figure 5, if LLMs predict the relations between two events as “NO_COREREFERENCE, SIMULTANEOUS, CAUSE, NO_SUBEVENT”, we can detect that “SIMULTANEOUS” and “CAUSE” are in conflict according to the logical constraints. In order to eliminate conflicts, one relation can be fixed first, and then the other relation can be randomly decided by the candidates that do not conflict with the current relation. For example, when the fixed temporal relation is “SIMULTANEOUS”, the causal relations can only be “NO_CAUSAL”, while when the fixed causal relation is “CAUSE”, the temporal relation can be either “BEFORE” or “OVERLAP”. We also add a negative option “NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT” to the candidate set because it is possible that neither relation exits. Finally, we randomly select one option from:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1379,
                    1056,
                    2261,
                    1141
                ],
                "angle": 0,
                "content": "- NO_COREREFERENCE, SIMULTANEOUS, NO_CAUSAL, NO_SUBEVENT"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1379,
                    1148,
                    2320,
                    1191
                ],
                "angle": 0,
                "content": "- NO_COREREFERENCE, OVERLAP, CAUSE, NO_SUBEVENT"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1382,
                    1194,
                    2292,
                    1234
                ],
                "angle": 0,
                "content": "- NO_COREREFERENCE, BEFORE, CAUSE, NO_SUBEVENT"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1382,
                    1240,
                    2254,
                    1326
                ],
                "angle": 0,
                "content": "- NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT"
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    1379,
                    1056,
                    2320,
                    1326
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1310,
                    1376,
                    2328,
                    1471
                ],
                "angle": 0,
                "content": "as the ultimate answer, thus ensuring that the results must be logically consistent (i.e., \\(\\mathrm{LI} = 0\\))."
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    1313,
                    1547,
                    2295,
                    1650
                ],
                "angle": 0,
                "content": "D LOGICAL CONSTRAINTS AMONG THREE EVENTS"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1310,
                    1663,
                    2328,
                    1805
                ],
                "angle": 0,
                "content": "We provide a comprehensive set of 39 logical constraints for the relations among three events in Table 7. We also manually design prompt for each constraint, as shown in Table 8."
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    1310,
                    1880,
                    2180,
                    1933
                ],
                "angle": 0,
                "content": "D.1 Pseudo Code of Logic Programming"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    1308,
                    1940,
                    2330,
                    2171
                ],
                "angle": 0,
                "content": "Once obtain 11 constraints between two events and 39 constraints among three events, we apply logic programming to automatically reason new event relations by inputting known constraints and relations. The pseudo-code mentioned in the main context is shown in Algorithm 1."
            },
            {
                "block_id": 23,
                "type": "title",
                "bbox": [
                    1310,
                    2247,
                    2162,
                    2346
                ],
                "angle": 0,
                "content": "E CASE STUDY ON SELF-GENERATED LOGICAL CONSTRAINTS"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    1308,
                    2362,
                    2338,
                    2960
                ],
                "angle": 0,
                "content": "In the main context, we have found that directly using CoT to infer logic does not help much for ERE tasks. One possible reason is that the inherent issues may lead to the failure of LLM in the precise rationale generation. To further illustrate an intuitive impression, we conduct a case study on MAVEN-ERE and find that the logical constraints generated by LLMs themselves are often inaccurate in content. As shown in Figure 9, ChatGPT could follow the logical constraint provided in the demonstration to a certain extent. However, it wrongly applies this to other relations – knowing that event \\( A \\) is event \\( B \\)'s precondition, it is wrong to think that event \\( B \\) will cause event \\( A \\). Actually, according to the logical constraints in Table 4, the relations between \\( (B, A) \\) should be “NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT”."
            }
        ]
    },
    {
        "page_id": 12,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "code_caption",
                "bbox": [
                    219,
                    349,
                    935,
                    399
                ],
                "angle": 0,
                "content": "Algorithm 1 An Example of 3-hop Reasoning"
            },
            {
                "block_id": 3,
                "type": "algorithm",
                "bbox": [
                    252,
                    412,
                    1224,
                    1366
                ],
                "angle": 0,
                "content": "Initialize the knowledge base with facts and rules  \nKnowledge Base:  \nFact: BEFORE(A, B)  \nFact: SIMULTANEOUS(B, C)  \nFact: OVERLAP(C, D)  \nRule: BEFORE \\(\\leftarrow\\) BEFORE \\(\\land\\) SIMULTANEOUS  \nRule: BEFORE \\(\\leftarrow\\) BEFORE \\(\\land\\) OVERLAP  \nRule: OVERLAP \\(\\leftarrow\\) SIMULTANEOUS \\(\\land\\) OVERLAP  \nInitialize the logic engine with the query  \nQuery: BEFORE(A, D)?  \nwhile obtain new facts do  \nfor each rule \\(r\\) of the Knowledge Base do  \nif \\(r\\)'s premise is satisfied by the current known facts then  \nAdd \\(r\\)'s conclusion to the knowledge base  \nend if  \nend for  \nend while  \nQuery result: BEFORE(A, D) is satisfied with BEFORE(A, C) and OVERLAP(B, D)"
            },
            {
                "block_id": 4,
                "type": "table",
                "bbox": [
                    1440,
                    343,
                    2208,
                    617
                ],
                "angle": 0,
                "content": "<table><tr><td>Hop</td><td># Count</td><td># Average Tokens</td></tr><tr><td>2</td><td>39</td><td>71</td></tr><tr><td>3</td><td>179</td><td>83</td></tr><tr><td>4</td><td>945</td><td>97</td></tr><tr><td>5</td><td>5613</td><td>109</td></tr></table>",
                "caption": "Table 5: Statistics of LLM-LR."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1313,
                    782,
                    2154,
                    884
                ],
                "angle": 0,
                "content": "G IMPLEMENTATION DETAILS OF PRETRAINING-BASED APPROACH"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1308,
                    897,
                    2333,
                    1174
                ],
                "angle": 0,
                "content": "We set the rank of LoRA modules to be 64. Our model is optimized with a learning rate of 2e-4 and a linear warm-up for the first \\(3\\%\\) steps. We clip the gradients of model parameters to a max norm of 0.3. The batch size is 8 and the number of epochs is 3. All the LoRA parameters are trained on an NVIDIA A100 GPU with 80GB memory."
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    298,
                    1508,
                    1127,
                    2491
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 9: A case study that ChatGPT generates inaccurate logical constraints."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    214,
                    2805,
                    798,
                    2851
                ],
                "angle": 0,
                "content": "F STATISTICS OF LLM-LR"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    211,
                    2864,
                    1229,
                    2956
                ],
                "angle": 0,
                "content": "As shown in Table 5, we provide the statistics of our LLM-LR dataset."
            }
        ]
    },
    {
        "page_id": 13,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    573,
                    343,
                    1973,
                    2227
                ],
                "angle": 0,
                "content": "<table><tr><td>If Relation(A, B)</td><td>Prompt Text</td></tr><tr><td>COREREFERENCE</td><td>If event A and event B are COREREFERENCE, then they won’t have temporal, causal, and subevent relations, and COREREFERENCE relation is bidirectional.</td></tr><tr><td>NO_TEMPORAL</td><td>If event A and event B do not have a temporal relation, then they won’t have causal and subevent relations.</td></tr><tr><td>BEFORE</td><td>If event A happens BEFORE event B, then they won’t have coreference and subevent relations, and event B has NO_TEMPORAL relation with event A.</td></tr><tr><td>OVERLAP</td><td>If event A happens OVERLAP with event B, then they won’t have coreference and subevent relations, and event B has NO_TEMPORAL relation with event A.</td></tr><tr><td>CONTAINS</td><td>If event A’s time CONTAINS event B’s time, then they won’t have coreference and causal relations, and event B has NO_TEMPORAL relation with event A.</td></tr><tr><td>SIMULTANEOUS</td><td>If event A and event B happen SIMULTANEOUSly, then they won’t have coreference, causal, and subevent relations, and SIMULTANEOUS relation is bidirectional.</td></tr><tr><td>ENDS-ON</td><td>If event A ENDS-ON event B, then they won’t have coreference, causal and subevent relations, and event B has NO_TEMPORAL relation with event A.</td></tr><tr><td>BEGINNS-ON</td><td>If event A BEGINS-ON event B, then they won’t have coreference, causal and subevent relations and BEGINS-ON relation is bidirectional.</td></tr><tr><td>CAUSE</td><td>If event A CAUSES event B, then event A happens BEFORE or OVERLAP event B, and they won’t have coreference and subevent relations, and event B has NO_TEMPORAL relation with event A.</td></tr><tr><td>PRECONDITION</td><td>If event A is event B’s PRECONDITION, then event A happens BEFORE or OVERLAP event B, and they won’t have coreference and subevent relations, and event B has NO_TEMPORAL relation with event A.</td></tr><tr><td>SUBEVENT</td><td>If event B is a SUBEVENT of event A, then they won’t have coreference and causal relations, and event A’s time should CONTAINS event B’s time, and event B has NO_TEMPORAL relation with event A.</td></tr></table>",
                "caption": "Table 6: Prompt text of relations between two events."
            }
        ]
    },
    {
        "page_id": 14,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    359,
                    346,
                    2185,
                    2217
                ],
                "angle": 0,
                "content": "<table><tr><td>If Relation(A, B) ∧ Relation(B, C)</td><td>Then Relation (A, C)</td></tr><tr><td>COREREFERENCE ∧ COREREFERENCE</td><td>COREREFERENCE, ¬TEMPORAL, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ BEFORE</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ OVERLAP</td><td>OVERLAP, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ CONTAINS</td><td>CONTAINS, ¬COREREFERENCE, ¬CAUSAL</td></tr><tr><td>COREREFERENCE ∧ SIMULTANEOUS</td><td>SIMULTANEOUS, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ ENDS-ON</td><td>ENDS-ON, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ BEGINNS-ON</td><td>BEGINNS-ON, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ CAUSE</td><td>CAUSE, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ PRECONDITION</td><td>PRECONDITION, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>COREREFERENCE ∧ SUBEVENT</td><td>SUBEVENT, ¬COREREFERENCE, CONTAINS ¬CAUSAL</td></tr><tr><td>BEFORE ∧ BEFORE</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>BEFORE ∧ OVERLAP</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>BEFORE ∧ CONTAINS</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>BEFORE ∧ SIMULTANEOUS</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>BEFORE ∧ ENDS-ON</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>BEFORE ∧ BEGINNS-ON</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>OVERLAP ∧ BEFORE</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>OVERLAP ∧ SIMULTANEOUS</td><td>OVERLAP, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>CONTAINS ∧ CONTAINS</td><td>CONTAINS, ¬COREREFERENCE, ¬CAUSAL</td></tr><tr><td>CONTAINS ∧ SIMULTANEOUS</td><td>CONTAINS, ¬COREREFERENCE, ¬CAUSAL</td></tr><tr><td>SIMULTANEOUS ∧ BEFORE</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>SIMULTANEOUS ∧ OVERLAP</td><td>OVERLAP, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>SIMULTANEOUS ∧ CONTAINS</td><td>CONTAINS, ¬COREREFERENCE, ¬CAUSAL</td></tr><tr><td>SIMULTANEOUS ∧ SIMULTANEOUS</td><td>SIMULTANEOUS, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>SIMULTANEOUS ∧ ENDS-ON</td><td>ENDS-ON, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>SIMULTANEOUS ∧ BEGINNS-ON</td><td>BEGINNS-ON, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>SIMULTANEOUS ∧ PRECONDITION</td><td>SIMULTANEOUS, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>ENDS-ON ∧ CONTAINS</td><td>BEFORE, ¬COREREFERENCE, ¬SUBEVENT</td></tr><tr><td>ENDS-ON ∧ BEGINNS-ON</td><td>ENDS-ON, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>ENDS-ON ∧ SIMULTANEOUS</td><td>ENDS-ON, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>BEGINNS-ON ∧ SIMULTANEOUS</td><td>BEGINNS-ON, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>BEGINNS-ON ∧ BEGINNS-ON</td><td>BEGINNS-ON, ¬COREREFERENCE, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>BEGINNS-ON ∧ COREREFERENCE</td><td>BEGINNS-ON, ¬CAUSAL, ¬SUBEVENT</td></tr><tr><td>CAUSE ∧ CAUSE</td><td>CAUSE, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>CAUSE ∧ PRECONDITION</td><td>PRECONDITION, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>CAUSE ∧ SUBEVENT</td><td>CAUSE, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>PRECONDITION ∧ PRECONDITION</td><td>PRECONDITION, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>PRECONDITION ∧ SUBEVENT</td><td>PRECONDITION, ¬COREREFERENCE, BEFORE ∨ OVERLAP, ¬SUBEVENT</td></tr><tr><td>SUBEVENT ∧ SUBEVENT</td><td>SUBEVENT, ¬COREREFERENCE, CONTAINS ¬CAUSAL</td></tr></table>",
                "caption": "Table 7: Logical Constraints of relations among three events, where \\( \\land \\) denotes \"AND\", \\( \\neg \\) denotes \"NOT\", \\( \\lor \\) denotes \"OR\"."
            }
        ]
    },
    {
        "page_id": 15,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    507,
                    343,
                    2040,
                    3161
                ],
                "angle": 0,
                "content": "<table><tr><td>If Relation(A, B) ∧ Relation(B, C)</td><td>Prompt Text</td></tr><tr><td>COREREFERENCE ∧ COREREFERENCE</td><td rowspan=\"10\">If event A and event B are COREREFERENCE, then the relations between event B and event C should be the same as that between event A and event C.</td></tr><tr><td>COREREFERENCE ∧ BEFORE</td></tr><tr><td>COREREFERENCE ∧ OVERLAP</td></tr><tr><td>COREREFERENCE ∧ CONTAINS</td></tr><tr><td>COREREFERENCE ∧ SIMULTANEOUS</td></tr><tr><td>COREREFERENCE ∧ ENDS-ON</td></tr><tr><td>COREREFERENCE ∧ BEGINS-ON</td></tr><tr><td>COREREFERENCE ∧ CAUSE</td></tr><tr><td>COREREFERENCE ∧ PRECONDITION</td></tr><tr><td>COREREFERENCE ∧ SUBEVENT</td></tr><tr><td>BEFORE ∧ BEFORE</td><td rowspan=\"6\">If event A happens BEFORE event B, and Relation(B, C), then event A happens BEFORE event C.</td></tr><tr><td>BEFORE ∧ OVERLAP</td></tr><tr><td>BEFORE ∧ CONTAINS</td></tr><tr><td>BEFORE ∧ SIMULTANEOUS</td></tr><tr><td>BEFORE ∧ ENDS-ON</td></tr><tr><td>BEFORE ∧ BEGINS-ON</td></tr><tr><td>OVERLAP ∧ BEFORE</td><td>If event A happens OVERLAP with event B, and event B happens BEFORE event C, then event A happens BEFORE event C.</td></tr><tr><td>OVERLAP ∧ SIMULTANEOUS</td><td>If event A happens OVERLAP with event B, and event B and event C happen SIMULTANEOUSly, then event A happens BEFORE event C.</td></tr><tr><td>CONTAINS ∧ CONTAINS</td><td>If event A&#x27;s time CONTAINS event B&#x27;s time, and event B&#x27;s time CONTAINS event C&#x27;s time, then event A&#x27;s time CONTAINS event C&#x27;s time.</td></tr><tr><td>CONTAINS ∧ SIMULTANEOUS</td><td>If event A&#x27;s time CONTAINS event B&#x27;s time, and event B and event C happen SIMULTANEOUSly, then event A&#x27;s time CONTAINS event C&#x27;s time.</td></tr><tr><td>SIMULTANEOUS ∧ BEFORE</td><td rowspan=\"6\">If events A and B happen SIMULTANEOUSly, and Relation(B, C), then event A&#x27;s time CONTAINS event C&#x27;s time.</td></tr><tr><td>SIMULTANEOUS ∧ OVERLAP</td></tr><tr><td>SIMULTANEOUS ∧ CONTAINS</td></tr><tr><td>SIMULTANEOUS ∧ SIMULTANEOUS</td></tr><tr><td>SIMULTANEOUS ∧ ENDS-ON</td></tr><tr><td>SIMULTANEOUS ∧ BEGINS-ON</td></tr><tr><td>ENDS-ON ∧ CONTAINS</td><td rowspan=\"2\">If event A ENDS-ON event B, and event B&#x27;s time CONTAINS event C&#x27;s time, then event A happens BEFORE event C.</td></tr><tr><td>ENDS-ON ∧ BEGINS-ON</td></tr><tr><td>ENDS-ON ∧ SIMULTANEOUS</td><td>If event A ENDS-ON event B, and Relation(B, C), then event A ENDS-ON event C.</td></tr><tr><td>BEGIN-ON ∧ SIMULTANEOUS</td><td rowspan=\"2\">If event A BEGIN-ON event B, and Relation(B, C), then event A BEGIN-ON event C.</td></tr><tr><td>BEGIN-ON ∧ BEGINS-ON</td></tr><tr><td>CAUSE ∧ CAUSE</td><td>If event A CAUSES event B, and event B CAUSES event C, then event A CAUSES event C.</td></tr><tr><td>CAUSE ∧ PRECONDITION</td><td>If event A CAUSES event B, and event B is event C&#x27;s PRECONDITION, then event A is event C&#x27;s PRECONDITION.</td></tr><tr><td>CAUSE ∧ SUBEVENT</td><td>If event A CAUSES event B, and event C is a SUBEVENT of event B, then event A CAUSES event C.</td></tr><tr><td>PRECONDITION ∧ PRECONDITION</td><td>If event A is event B&#x27;s PRECONDITION, and event B is event C&#x27;s PRECONDITION, then event A is event C&#x27;s PRECONDITION.</td></tr><tr><td>PRECONDITION ∧ SUBEVENT</td><td>If event A is event B&#x27;s PRECONDITION, and event C is a SUBEVENT of event B, then event A is event C&#x27;s PRECONDITION.</td></tr><tr><td>SUBEVENT ∧ SUBEVENT</td><td>If event B is a SUBEVENT of event A, and event C is a SUBEVENT of event B, then event C is a SUBEVENT of event A.</td></tr></table>",
                "caption": "Table 8: Prompt text of relations among three events."
            }
        ]
    },
    {
        "page_id": 16,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    214,
                    336,
                    754,
                    386
                ],
                "angle": 0,
                "content": "H PROMPT EXAMPLES"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    214,
                    399,
                    1466,
                    448
                ],
                "angle": 0,
                "content": "In this section, we provide examples of prompts used for each task and approach."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    214,
                    485,
                    680,
                    537
                ],
                "angle": 0,
                "content": "H.1 Pilot Case Study"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    214,
                    544,
                    867,
                    590
                ],
                "angle": 0,
                "content": "This section includes prompt examples of:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    280,
                    600,
                    1150,
                    643
                ],
                "angle": 0,
                "content": "- MAVEN-ERE w. relevant logic constraints (Figure 10);"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    646,
                    1173,
                    689
                ],
                "angle": 0,
                "content": "- MAVEN-ERE w. irrelevant logic constraints (Figure 11);"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    280,
                    693,
                    1137,
                    735
                ],
                "angle": 0,
                "content": "ProofWriter w. relevant logic constraints (Figure 12);"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    280,
                    739,
                    1155,
                    782
                ],
                "angle": 0,
                "content": "ProofWriter w. irrelevant logic constraints (Figure 13)."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    280,
                    785,
                    816,
                    828
                ],
                "angle": 0,
                "content": "- Multi-hop reasoning (Figure 14)"
            },
            {
                "block_id": 11,
                "type": "list",
                "bbox": [
                    280,
                    600,
                    1173,
                    828
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    336,
                    871,
                    1224,
                    927
                ],
                "angle": 0,
                "content": "MAVEN-ERE w. relevant logic constraints"
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    1234,
                    841,
                    1351,
                    943
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    247,
                    947,
                    334,
                    1075
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    362,
                    950,
                    673,
                    986
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    362,
                    990,
                    1874,
                    1062
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 17,
                "type": "title",
                "bbox": [
                    362,
                    1095,
                    456,
                    1128
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    362,
                    1128,
                    1838,
                    1237
                ],
                "angle": 0,
                "content": "The 2013 Lahad Datu < standoff > ( also known as the Lahad Datu incursion ) was a military < conflict > that started on 11 February 2013 and fully ended on 24 March 2013 ."
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    364,
                    1273,
                    581,
                    1306
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    364,
                    1310,
                    892,
                    1379
                ],
                "angle": 0,
                "content": "<standoff> and <conflict> <conflict> and <standoff>"
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    364,
                    1415,
                    510,
                    1452
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    367,
                    1531,
                    1142,
                    1567
                ],
                "angle": 0,
                "content": "Event Pair 1: < standoff > and < conflict >"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    367,
                    1567,
                    1053,
                    1600
                ],
                "angle": 0,
                "content": "(1) Coreference relation : COREFERENCE"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    1193,
                    1567,
                    1807,
                    1607
                ],
                "angle": 0,
                "content": "(2) Temporal relation: CONTAINS"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    367,
                    1603,
                    907,
                    1640
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    1012,
                    1607,
                    1626,
                    1640
                ],
                "angle": 0,
                "content": "(4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    367,
                    1673,
                    1137,
                    1709
                ],
                "angle": 0,
                "content": "Event Pair 2: < conflict > and < standoff >"
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    367,
                    1712,
                    1785,
                    1745
                ],
                "angle": 0,
                "content": "(1) Coreference relation: NO_COREREFERENCE (2) Temporal relation: NO_TEMPORAL"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    367,
                    1749,
                    1629,
                    1782
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 30,
                "type": "list",
                "bbox": [
                    367,
                    1712,
                    1785,
                    1782
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 31,
                "type": "image",
                "bbox": [
                    247,
                    1894,
                    334,
                    2022
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    359,
                    1890,
                    1843,
                    2042
                ],
                "angle": 0,
                "content": "Let's think step by step. You have identified that \\( < \\) standoff \\( > \\) and \\( < \\) conflict \\( > \\) are coreferences. The coreference should be bidirectional. And if two events are COREFERENCE, they won't have temporal relations. So please modify your Coreference and Temporal answers."
            },
            {
                "block_id": 33,
                "type": "image",
                "bbox": [
                    1948,
                    1527,
                    2034,
                    1610
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 34,
                "type": "title",
                "bbox": [
                    1930,
                    1617,
                    2091,
                    1656
                ],
                "angle": 0,
                "content": "ChatGPT"
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    362,
                    2108,
                    1894,
                    2178
                ],
                "angle": 0,
                "content": "Apologies for the confusion. Based on the revised information, here are the corrected answers:"
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    367,
                    2184,
                    1137,
                    2217
                ],
                "angle": 0,
                "content": "Event Pair 1: < standoff > and < conflict >"
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    367,
                    2220,
                    1861,
                    2257
                ],
                "angle": 0,
                "content": "(1) Coreference relation : COREFERENCE (2) Temporal relation: NO_TEMPORAL"
            },
            {
                "block_id": 38,
                "type": "text",
                "bbox": [
                    367,
                    2257,
                    1621,
                    2290
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 39,
                "type": "list",
                "bbox": [
                    367,
                    2220,
                    1861,
                    2290
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    367,
                    2323,
                    1137,
                    2359
                ],
                "angle": 0,
                "content": "Event Pair 2: < conflict > and < standoff >"
            },
            {
                "block_id": 41,
                "type": "text",
                "bbox": [
                    367,
                    2362,
                    1749,
                    2395
                ],
                "angle": 0,
                "content": "(1) Coreference relation: COREFERENCE (2) Temporal relation: NO_TEMPORAL"
            },
            {
                "block_id": 42,
                "type": "text",
                "bbox": [
                    367,
                    2399,
                    1621,
                    2435
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 43,
                "type": "list",
                "bbox": [
                    367,
                    2362,
                    1749,
                    2435
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 44,
                "type": "image",
                "bbox": [
                    1948,
                    2105,
                    2034,
                    2187
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 45,
                "type": "title",
                "bbox": [
                    1930,
                    2194,
                    2091,
                    2237
                ],
                "angle": 0,
                "content": "ChatGPT"
            },
            {
                "block_id": 46,
                "type": "image_caption",
                "bbox": [
                    826,
                    2501,
                    1716,
                    2550
                ],
                "angle": 0,
                "content": "Figure 10: MAVEN-ERE w. relevant logic constraints"
            }
        ]
    },
    {
        "page_id": 17,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    349,
                    363,
                    1366,
                    432
                ],
                "angle": 0,
                "content": "MAVEN-ERE w. irrelevant logic constraints"
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1374,
                    346,
                    1504,
                    442
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    249,
                    455,
                    349,
                    597
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    377,
                    455,
                    721,
                    498
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    377,
                    498,
                    2060,
                    577
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    379,
                    617,
                    481,
                    653
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    377,
                    656,
                    2022,
                    775
                ],
                "angle": 0,
                "content": "The 2013 Lahad Datu < standoff > ( also known as the Lahad Datu incursion ) was a military < conflict > that started on 11 February 2013 and fully ended on 24 March 2013 ."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    379,
                    815,
                    624,
                    854
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 10,
                "type": "code",
                "bbox": [
                    379,
                    858,
                    966,
                    933
                ],
                "angle": 0,
                "content": "<standoff> and <conflict> <conflict> and <standoff>"
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    379,
                    973,
                    543,
                    1013
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    379,
                    1098,
                    1244,
                    1138
                ],
                "angle": 0,
                "content": "Event Pair 1: < standoff > and < conflict >"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    382,
                    1141,
                    1981,
                    1181
                ],
                "angle": 0,
                "content": "(1) Coreference relation : COREFERENCE (2) Temporal relation: CONTAINS"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    382,
                    1184,
                    1779,
                    1221
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    382,
                    1141,
                    1981,
                    1221
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    379,
                    1257,
                    1244,
                    1296
                ],
                "angle": 0,
                "content": "Event Pair 2: < conflict > and < standoff >"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    382,
                    1300,
                    1958,
                    1339
                ],
                "angle": 0,
                "content": "(1) Coreference relation: NO_COREREFERENCE (2) Temporal relation: NO_TEMPORAL"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    382,
                    1343,
                    1779,
                    1379
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 19,
                "type": "list",
                "bbox": [
                    382,
                    1300,
                    1958,
                    1379
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 20,
                "type": "image",
                "bbox": [
                    2139,
                    1092,
                    2236,
                    1194
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 22,
                "type": "image",
                "bbox": [
                    249,
                    1478,
                    349,
                    1620
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    377,
                    1498,
                    2080,
                    1620
                ],
                "angle": 0,
                "content": "Let's think step by step. If \\( < \\) standoff \\( > \\) happens BEFORE \\( < \\) conflict \\( > \\), then they won't have coreference and subevent relations, and \\( < \\) conflict \\( > \\) will have NO_TEMPORAL relation with \\( < \\) standoff \\( > \\)."
            },
            {
                "block_id": 24,
                "type": "image",
                "bbox": [
                    966,
                    1580,
                    1060,
                    1669
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    379,
                    1706,
                    1756,
                    1755
                ],
                "angle": 0,
                "content": "Yes, you are correct. Based on the information, here are the answers:"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    379,
                    1785,
                    1244,
                    1824
                ],
                "angle": 0,
                "content": "Event Pair 1: < standoff > and < conflict >"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    382,
                    1828,
                    2017,
                    1871
                ],
                "angle": 0,
                "content": "(1) Coreference relation : NO_COREFERENCE (2) Temporal relation: BEFORE"
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    382,
                    1871,
                    1779,
                    1910
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 29,
                "type": "list",
                "bbox": [
                    382,
                    1828,
                    2017,
                    1910
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 30,
                "type": "image",
                "bbox": [
                    2139,
                    1702,
                    2236,
                    1798
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    379,
                    1947,
                    1244,
                    1986
                ],
                "angle": 0,
                "content": "Event Pair 2: < conflict > and < standoff >"
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    382,
                    1989,
                    1978,
                    2029
                ],
                "angle": 0,
                "content": "(1) Coreference relation: NO_COREREFERENCE (2) Temporal relation: NO_TEMPORAL"
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    382,
                    2032,
                    1779,
                    2069
                ],
                "angle": 0,
                "content": "(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT"
            },
            {
                "block_id": 35,
                "type": "list",
                "bbox": [
                    382,
                    1989,
                    1978,
                    2069
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 36,
                "type": "image_caption",
                "bbox": [
                    810,
                    2138,
                    1728,
                    2184
                ],
                "angle": 0,
                "content": "Figure 11: MAVEN-ERE w. irrelevant logic constraints"
            }
        ]
    },
    {
        "page_id": 18,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    249,
                    468,
                    349,
                    613
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    349,
                    386,
                    1303,
                    445
                ],
                "angle": 0,
                "content": "ProofWriter w. relevant logic constraints ("
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1315,
                    346,
                    1438,
                    458
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    377,
                    471,
                    724,
                    514
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    377,
                    511,
                    1981,
                    597
                ],
                "angle": 0,
                "content": "Given a problem statement as contexts, the task is to answer a logical reasoning question."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    379,
                    633,
                    484,
                    666
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    377,
                    669,
                    2083,
                    917
                ],
                "angle": 0,
                "content": "Bob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white. Charlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry is white. All red people are round. Round, smart people are cold. If someone is cold and young then they are white. Red people are round. All round, young people are red. If someone is smart then they are rough. Rough, young people are red. If Bob is round then Bob is smart. If Charlie is white and Charlie is rough then Charlie is young."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    379,
                    950,
                    563,
                    986
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    377,
                    990,
                    2040,
                    1072
                ],
                "angle": 0,
                "content": "Based on the above information, is the following statement true, false, or unknown? Harry is not round."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    374,
                    1158,
                    1881,
                    1204
                ],
                "angle": 0,
                "content": "Based on the given information, the statement \"Harry is not round\" is true."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    374,
                    1204,
                    2080,
                    1320
                ],
                "angle": 0,
                "content": "The information states that all red people are round, and Harry is described as white, smart, and rough, but not round. Therefore, Harry does not fit the criteria of being round, and the statement is true."
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    2136,
                    1145,
                    2236,
                    1244
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    249,
                    1379,
                    349,
                    1521
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    377,
                    1402,
                    1876,
                    1448
                ],
                "angle": 0,
                "content": "Let's think step by step. From the text, we could first get relevant facts:"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    379,
                    1448,
                    719,
                    1485
                ],
                "angle": 0,
                "content": "- Harry is smart."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    379,
                    1488,
                    724,
                    1527
                ],
                "angle": 0,
                "content": "- Harry is white."
            },
            {
                "block_id": 19,
                "type": "list",
                "bbox": [
                    379,
                    1448,
                    724,
                    1527
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 20,
                "type": "image",
                "bbox": [
                    1963,
                    1521,
                    2068,
                    1636
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    377,
                    1564,
                    826,
                    1603
                ],
                "angle": 0,
                "content": "And the relevant rule:"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    379,
                    1607,
                    928,
                    1646
                ],
                "angle": 0,
                "content": "- All red people are round."
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    379,
                    1646,
                    1007,
                    1686
                ],
                "angle": 0,
                "content": "- Round, smart people are cold."
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    379,
                    1686,
                    1068,
                    1729
                ],
                "angle": 0,
                "content": "- All round, young people are red."
            },
            {
                "block_id": 25,
                "type": "list",
                "bbox": [
                    379,
                    1607,
                    1068,
                    1729
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    374,
                    1762,
                    1461,
                    1805
                ],
                "angle": 0,
                "content": "Given these statements, we can conclude the following:"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    374,
                    1805,
                    1955,
                    1848
                ],
                "angle": 0,
                "content": "Harry is smart and white, but there is no direct statement about his being red."
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    374,
                    1927,
                    2057,
                    2092
                ],
                "angle": 0,
                "content": "Apologies for the confusion. You are correct, there is no direct statement in the given information that explicitly mentions whether Harry is round or not. Therefore, the statement \"Harry is not round\" is unknown based solely on the provided information."
            },
            {
                "block_id": 29,
                "type": "image",
                "bbox": [
                    2139,
                    1923,
                    2236,
                    2019
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 31,
                "type": "image_caption",
                "bbox": [
                    828,
                    2161,
                    1713,
                    2211
                ],
                "angle": 0,
                "content": "Figure 12: ProofWriter w. relevant logic constraints"
            }
        ]
    },
    {
        "page_id": 19,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    349,
                    366,
                    1389,
                    432
                ],
                "angle": 0,
                "content": "ProofWriter w. irrelevant logic constraints ("
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1336,
                    346,
                    1463,
                    442
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    247,
                    455,
                    349,
                    597
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    377,
                    455,
                    721,
                    498
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    377,
                    498,
                    1981,
                    577
                ],
                "angle": 0,
                "content": "Given a problem statement as contexts, the task is to answer a logical reasoning question."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    379,
                    617,
                    484,
                    653
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    377,
                    656,
                    2080,
                    900
                ],
                "angle": 0,
                "content": "Bob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white. Charlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry is white. All red people are round. Round, smart people are cold. If someone is cold and young then they are white. Red people are round. All round, young people are red. If someone is smart then they are rough. Rough, young people are red. If Bob is round then Bob is smart. If Charlie is white and Charlie is rough then Charlie is young."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    379,
                    933,
                    563,
                    973
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    377,
                    973,
                    2040,
                    1056
                ],
                "angle": 0,
                "content": "Based on the above information, is the following statement true, false, or unknown? Harry is not round."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    374,
                    1141,
                    1940,
                    1188
                ],
                "angle": 0,
                "content": "Based on the given information, the statement \"Harry is not round\" is true."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    374,
                    1188,
                    2080,
                    1306
                ],
                "angle": 0,
                "content": "The information states that all red people are round, and Harry is described as white, smart, and rough, but not round. Therefore, Harry does not fit the criteria of being round, and the statement is true."
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    2136,
                    1128,
                    2236,
                    1227
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    249,
                    1372,
                    349,
                    1475
                ],
                "angle": 0,
                "content": null,
                "caption": "User"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    377,
                    1386,
                    1677,
                    1432
                ],
                "angle": 0,
                "content": "Let's think step by step. From the text, we could get some facts:"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    379,
                    1432,
                    724,
                    1471
                ],
                "angle": 0,
                "content": "- Harry is smart."
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    379,
                    1471,
                    724,
                    1511
                ],
                "angle": 0,
                "content": "- Harry is white."
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    379,
                    1432,
                    724,
                    1511
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    379,
                    1547,
                    683,
                    1587
                ],
                "angle": 0,
                "content": "And some rules:"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    382,
                    1590,
                    1402,
                    1630
                ],
                "angle": 0,
                "content": "- If someone is cold and young then they are white."
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    382,
                    1633,
                    1224,
                    1669
                ],
                "angle": 0,
                "content": "- If someone is smart then they are rough."
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    382,
                    1673,
                    986,
                    1712
                ],
                "angle": 0,
                "content": "- Rough, young people are red."
            },
            {
                "block_id": 25,
                "type": "list",
                "bbox": [
                    382,
                    1590,
                    1402,
                    1712
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    379,
                    1745,
                    1397,
                    1791
                ],
                "angle": 0,
                "content": "Given these statements, please output your answers."
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    374,
                    1874,
                    2022,
                    1966
                ],
                "angle": 0,
                "content": "Apologies for the confusion. Based on the information, the statement “Harry is not round” is False."
            },
            {
                "block_id": 28,
                "type": "image",
                "bbox": [
                    2136,
                    1867,
                    2236,
                    1966
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 30,
                "type": "image_caption",
                "bbox": [
                    816,
                    2102,
                    1723,
                    2148
                ],
                "angle": 0,
                "content": "Figure 13: ProofWriter w. irrelevant logic constraints"
            },
            {
                "block_id": 31,
                "type": "title",
                "bbox": [
                    821,
                    2204,
                    1254,
                    2257
                ],
                "angle": 0,
                "content": "Multi-hop Reasoning"
            },
            {
                "block_id": 32,
                "type": "title",
                "bbox": [
                    828,
                    2277,
                    1132,
                    2316
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    828,
                    2316,
                    1680,
                    2352
                ],
                "angle": 0,
                "content": "Given a problem statement as contexts, your task"
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    828,
                    2352,
                    1637,
                    2385
                ],
                "angle": 0,
                "content": "is to induce the relations between two events."
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    828,
                    2385,
                    1328,
                    2422
                ],
                "angle": 0,
                "content": "（…morecontexthere…）"
            },
            {
                "block_id": 36,
                "type": "title",
                "bbox": [
                    828,
                    2455,
                    971,
                    2488
                ],
                "angle": 0,
                "content": "Context:"
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    828,
                    2491,
                    1713,
                    2593
                ],
                "angle": 0,
                "content": "Given that event A happens BEFORE event B, event B and event C happen SIMULTANEOUSly, event C happens OVERLAP with event D."
            },
            {
                "block_id": 38,
                "type": "title",
                "bbox": [
                    828,
                    2630,
                    989,
                    2663
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 39,
                "type": "text",
                "bbox": [
                    828,
                    2666,
                    1695,
                    2702
                ],
                "angle": 0,
                "content": "Based on the above information, what are the four"
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    828,
                    2702,
                    1552,
                    2735
                ],
                "angle": 0,
                "content": "types of relations between event A and D?"
            },
            {
                "block_id": 41,
                "type": "title",
                "bbox": [
                    828,
                    2768,
                    971,
                    2805
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 42,
                "type": "image_caption",
                "bbox": [
                    920,
                    2874,
                    1624,
                    2920
                ],
                "angle": 0,
                "content": "Figure 14: Abstract Multi-hop Reasoning"
            }
        ]
    },
    {
        "page_id": 20,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    214,
                    339,
                    1025,
                    396
                ],
                "angle": 0,
                "content": "H.2 Incorporating Logical Constraints"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    441,
                    402,
                    2103,
                    448
                ],
                "angle": 0,
                "content": "The highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    306,
                    471,
                    545,
                    518
                ],
                "angle": 0,
                "content": "Vanilla ICL"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    313,
                    544,
                    606,
                    580
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    313,
                    580,
                    1165,
                    716
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    316,
                    749,
                    405,
                    778
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    316,
                    778,
                    1147,
                    953
                ],
                "angle": 0,
                "content": "Before her death, Todd posted a video on YouTube in which she used a series of flash cards to < tell > her experience of being blackmailed into exposing her breasts via webcam, and of being < bullied > and physically assaulted."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    316,
                    980,
                    520,
                    1013
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    316,
                    1016,
                    726,
                    1049
                ],
                "angle": 0,
                "content": "< tell > and < bullied >"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    316,
                    1049,
                    726,
                    1082
                ],
                "angle": 0,
                "content": "< bullied > and < tell >"
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    316,
                    1115,
                    453,
                    1148
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    316,
                    1148,
                    1152,
                    1221
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.  \nNO_COREREFERENCE, NO_TEMPORAL, PRECONDITION, NO_SUBEVENT."
            },
            {
                "block_id": 14,
                "type": "title",
                "bbox": [
                    300,
                    1323,
                    719,
                    1376
                ],
                "angle": 0,
                "content": "Vanilla ICL plus CoT"
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    308,
                    1399,
                    599,
                    1432
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    308,
                    1435,
                    1157,
                    1570
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 17,
                "type": "title",
                "bbox": [
                    311,
                    1600,
                    397,
                    1633
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    308,
                    1633,
                    1137,
                    1805
                ],
                "angle": 0,
                "content": "Before her death , Todd posted a video on YouTube in which she used a series of flash cards to <tell > her experience of being blackmailed into exposing her breasts via webcam , and of being <bullied > and physically assaulted ."
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    311,
                    1834,
                    512,
                    1864
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    311,
                    1867,
                    719,
                    1900
                ],
                "angle": 0,
                "content": "< tell > and < bullied >"
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    311,
                    1904,
                    719,
                    1937
                ],
                "angle": 0,
                "content": "< bullied > and < tell >"
            },
            {
                "block_id": 22,
                "type": "title",
                "bbox": [
                    311,
                    1970,
                    479,
                    2003
                ],
                "angle": 0,
                "content": "Reasoning:"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    311,
                    2003,
                    1134,
                    2098
                ],
                "angle": 0,
                "content": "< bullied > happens before < tell >, and being < bullied > leads her to < tell >.  \nAnswers:"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    311,
                    2102,
                    1055,
                    2171
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.  \nNO_COREREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT."
            },
            {
                "block_id": 25,
                "type": "title",
                "bbox": [
                    293,
                    2214,
                    1076,
                    2267
                ],
                "angle": 0,
                "content": "CoT w. self-generated logical constraints"
            },
            {
                "block_id": 26,
                "type": "title",
                "bbox": [
                    306,
                    2290,
                    589,
                    2323
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    300,
                    2326,
                    1150,
                    2458
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 28,
                "type": "title",
                "bbox": [
                    303,
                    2491,
                    390,
                    2521
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    300,
                    2524,
                    1129,
                    2696
                ],
                "angle": 0,
                "content": "Before her death , Todd posted a video on YouTube in which she used a series of flash cards to <tell > her experience of being blackmailed into exposing her breasts via webcam , and of being <bullied > and physically assaulted ."
            },
            {
                "block_id": 30,
                "type": "title",
                "bbox": [
                    303,
                    2725,
                    504,
                    2758
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    306,
                    2762,
                    714,
                    2791
                ],
                "angle": 0,
                "content": "< tell > and < bullied >"
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    306,
                    2795,
                    714,
                    2824
                ],
                "angle": 0,
                "content": "< bullied > and < tell >"
            },
            {
                "block_id": 33,
                "type": "list",
                "bbox": [
                    306,
                    2762,
                    714,
                    2824
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 34,
                "type": "title",
                "bbox": [
                    303,
                    2857,
                    874,
                    2894
                ],
                "angle": 0,
                "content": "From the text, we could first get:"
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    300,
                    2894,
                    1155,
                    2960
                ],
                "angle": 0,
                "content": "< bullied > happens before < tell >, and being < bullied > leads her to < tell >"
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    300,
                    2960,
                    1155,
                    3154
                ],
                "angle": 0,
                "content": "Due to the logical constraint: If event A is event B's PRECONDITION, then event A happens BEFORE or OVERLAP event B, and they won't have coreference and subevent relations, and event B has NO_TEMPORAL relation with event A. We can deduce that the answers are:"
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    300,
                    3158,
                    1099,
                    3227
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.  \nNO_COREREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT."
            },
            {
                "block_id": 38,
                "type": "title",
                "bbox": [
                    1379,
                    570,
                    1848,
                    620
                ],
                "angle": 0,
                "content": "w. all logical constraints"
            },
            {
                "block_id": 39,
                "type": "title",
                "bbox": [
                    1389,
                    643,
                    1677,
                    676
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    1387,
                    679,
                    2236,
                    815
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 41,
                "type": "title",
                "bbox": [
                    1389,
                    844,
                    1728,
                    881
                ],
                "angle": 0,
                "content": "Logical constraints:"
            },
            {
                "block_id": 42,
                "type": "text",
                "bbox": [
                    1389,
                    881,
                    2218,
                    980
                ],
                "angle": 0,
                "content": "There are some rules among the relations, you can select some of them to reason or check your answers:"
            },
            {
                "block_id": 43,
                "type": "text",
                "bbox": [
                    1389,
                    980,
                    2236,
                    1115
                ],
                "angle": 0,
                "content": "(1) If two events are COREFERENCE, then they won't have temporal, causal, and subevent relations, and COREFERENCE relation is bidirectional. (… more context here …)"
            },
            {
                "block_id": 44,
                "type": "text",
                "bbox": [
                    1389,
                    1115,
                    2149,
                    1280
                ],
                "angle": 0,
                "content": "(11) If event B is a SUBEVENT of event A, then they won't have coreference and causal relations, and event A's time should CONTAINS event B's time, and event B has NO_TEMPORAL relation with event A."
            },
            {
                "block_id": 45,
                "type": "list",
                "bbox": [
                    1389,
                    980,
                    2236,
                    1280
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 46,
                "type": "title",
                "bbox": [
                    1389,
                    1313,
                    1476,
                    1346
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 47,
                "type": "text",
                "bbox": [
                    1387,
                    1349,
                    2218,
                    1521
                ],
                "angle": 0,
                "content": "Before her death , Todd posted a video on YouTubein which she used a series of flash cards to <tell > her experience of being blackmailed intoexposing her breasts via webcam , and of being <bullied > and physically assaulted ."
            },
            {
                "block_id": 48,
                "type": "title",
                "bbox": [
                    1389,
                    1547,
                    1593,
                    1580
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 49,
                "type": "text",
                "bbox": [
                    1389,
                    1580,
                    1797,
                    1646
                ],
                "angle": 0,
                "content": "< tell > and < bullied > < bullied > and < tell >"
            },
            {
                "block_id": 50,
                "type": "title",
                "bbox": [
                    1389,
                    1683,
                    1524,
                    1712
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 51,
                "type": "text",
                "bbox": [
                    1389,
                    1716,
                    2187,
                    1782
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.  \nNO_COREREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT."
            },
            {
                "block_id": 52,
                "type": "title",
                "bbox": [
                    1379,
                    1910,
                    1973,
                    1960
                ],
                "angle": 0,
                "content": "w. retrieved logical constraints"
            },
            {
                "block_id": 53,
                "type": "title",
                "bbox": [
                    1389,
                    1983,
                    1677,
                    2016
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 54,
                "type": "text",
                "bbox": [
                    1387,
                    2016,
                    2236,
                    2151
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 55,
                "type": "title",
                "bbox": [
                    1389,
                    2181,
                    1726,
                    2217
                ],
                "angle": 0,
                "content": "Logical constraints:"
            },
            {
                "block_id": 56,
                "type": "text",
                "bbox": [
                    1387,
                    2217,
                    2218,
                    2316
                ],
                "angle": 0,
                "content": "There are some rules among the relations, you can select some of them to reason or check your answers:"
            },
            {
                "block_id": 57,
                "type": "text",
                "bbox": [
                    1389,
                    2316,
                    2185,
                    2484
                ],
                "angle": 0,
                "content": "(1) If event A is event B's PRECONDITION, then event A happens BEFORE or OVERLAP event B, and they won't have coreference and subevent relations, and event B has NO_TEMPORAL relation with event A."
            },
            {
                "block_id": 58,
                "type": "text",
                "bbox": [
                    1389,
                    2484,
                    2233,
                    2583
                ],
                "angle": 0,
                "content": "(2) If event A and event B do not have a temporal relation, then they won't have causal and subevent relations."
            },
            {
                "block_id": 59,
                "type": "title",
                "bbox": [
                    1389,
                    2616,
                    1476,
                    2649
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 60,
                "type": "text",
                "bbox": [
                    1387,
                    2653,
                    2218,
                    2824
                ],
                "angle": 0,
                "content": "Before her death , Todd posted a video on YouTube in which she used a series of flash cards to <tell > her experience of being blackmailed into exposing her breasts via webcam , and of being <bullied > and physically assaulted ."
            },
            {
                "block_id": 61,
                "type": "title",
                "bbox": [
                    1389,
                    2854,
                    1593,
                    2887
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 62,
                "type": "text",
                "bbox": [
                    1389,
                    2887,
                    1797,
                    2953
                ],
                "angle": 0,
                "content": "< tell > and < bullied > < bullied > and < tell >"
            },
            {
                "block_id": 63,
                "type": "title",
                "bbox": [
                    1389,
                    2989,
                    1524,
                    3019
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 64,
                "type": "text",
                "bbox": [
                    1387,
                    3019,
                    2185,
                    3088
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.  \nNO_COREREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT."
            }
        ]
    },
    {
        "page_id": 21,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    216,
                    250,
                    935,
                    287
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    2182,
                    250,
                    2328,
                    287
                ],
                "angle": 0,
                "content": "Chen, et al."
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    451,
                    356,
                    550,
                    495
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    578,
                    359,
                    907,
                    399
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    578,
                    399,
                    1583,
                    551
                ],
                "angle": 0,
                "content": "There is a piece of text with two events marked by \\(<\\) and \\(>\\) symbols, and your task is to identify four types of relations between them (… more context here …)"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    581,
                    587,
                    680,
                    623
                ],
                "angle": 0,
                "content": "Text:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    581,
                    627,
                    1542,
                    742
                ],
                "angle": 0,
                "content": "This trade gradually \\(<\\) declined \\(>\\) during the early 19th century as demand for furs in Europe \\(<\\) diminished \\(>\\)."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    581,
                    778,
                    813,
                    815
                ],
                "angle": 0,
                "content": "Event Pairs:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    581,
                    818,
                    1180,
                    894
                ],
                "angle": 0,
                "content": "< diminished > and < declined >\n< declined > and < diminished >"
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    581,
                    930,
                    739,
                    970
                ],
                "angle": 0,
                "content": "Answers:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    578,
                    1032,
                    1578,
                    1115
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, NO_TEMPORAL, XCAUSE, SUBEVENT. X  \nNO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT."
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1649,
                    1019,
                    1746,
                    1118
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1848,
                    686,
                    2085,
                    739
                ],
                "angle": 0,
                "content": "Iteration 0"
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    456,
                    1178,
                    550,
                    1310
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    581,
                    1178,
                    1343,
                    1217
                ],
                "angle": 0,
                "content": "Your answers are logically inconsistent."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    581,
                    1217,
                    1575,
                    1293
                ],
                "angle": 0,
                "content": "There are some rules among the relations, you can select some of them to reason or check your answers:"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    581,
                    1293,
                    1563,
                    1405
                ],
                "angle": 0,
                "content": "(1) If event A CAUSES event B, then event A happens BEFORE or OVERLAP event B, and they won't have coreference and subevent relations ..."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    581,
                    1405,
                    1542,
                    1518
                ],
                "angle": 0,
                "content": "(2) If event A and event B do not have a temporal relation, then they won't have causal and subevent relations ..."
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    581,
                    1518,
                    1542,
                    1633
                ],
                "angle": 0,
                "content": "(3) If event B is a SUBEVENT of event A, then they won't have coreference and causal relations, and event A's time should CONTAINS event B's time ..."
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    581,
                    1293,
                    1563,
                    1633
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "image",
                "bbox": [
                    1649,
                    1666,
                    1746,
                    1762
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    1848,
                    1343,
                    2080,
                    1392
                ],
                "angle": 0,
                "content": "Iteration 1"
            },
            {
                "block_id": 24,
                "type": "image",
                "bbox": [
                    456,
                    1838,
                    550,
                    1973
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    581,
                    1838,
                    1346,
                    1877
                ],
                "angle": 0,
                "content": "Your answers are logically inconsistent."
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    581,
                    1877,
                    1575,
                    1956
                ],
                "angle": 0,
                "content": "There are some rules among the relations, you can select some of them to reason or check your answers:"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    581,
                    1956,
                    1563,
                    2069
                ],
                "angle": 0,
                "content": "(1) If event A CAUSES event B, then event A happens BEFORE or OVERLAP event B, and they won't have coreference and subevent relations ..."
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    581,
                    2069,
                    1575,
                    2148
                ],
                "angle": 0,
                "content": "(2) If event A's time CONTAINS event B's time, then they won't have coreference and causal relations ..."
            },
            {
                "block_id": 29,
                "type": "list",
                "bbox": [
                    581,
                    1956,
                    1575,
                    2148
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 30,
                "type": "image",
                "bbox": [
                    1649,
                    2161,
                    1746,
                    2260
                ],
                "angle": 0,
                "content": null,
                "caption": "ChatGPT"
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    1848,
                    2032,
                    2083,
                    2085
                ],
                "angle": 0,
                "content": "Iteration 2"
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    581,
                    2187,
                    1578,
                    2270
                ],
                "angle": 0,
                "content": "NO_COREREFERENCE, BEFORE, CAUSE, NO_SUBEVENT.  \nNO_COREREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT."
            },
            {
                "block_id": 34,
                "type": "image_caption",
                "bbox": [
                    242,
                    2389,
                    2295,
                    2438
                ],
                "angle": 0,
                "content": "Figure 15: Multi-turn conversation with ChatGPT. We retrieve relevant logical constraints and provide them to ChatGPT."
            },
            {
                "block_id": 35,
                "type": "title",
                "bbox": [
                    214,
                    2504,
                    736,
                    2547
                ],
                "angle": 0,
                "content": "H.3 Iterative Retrievals"
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    211,
                    2564,
                    1231,
                    2607
                ],
                "angle": 0,
                "content": "In this section, we present a prompt example used in Section 4.3.2."
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    211,
                    2610,
                    1229,
                    2702
                ],
                "angle": 0,
                "content": "As shown in Figure 15, with iterative prompting, ChatGPT finally outputs the correct answers."
            }
        ]
    },
    {
        "page_id": 22,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    214,
                    250,
                    963,
                    290
                ],
                "angle": 0,
                "content": "Learning To Teach Large Language Models Logical Reasoning"
            },
            {
                "block_id": 1,
                "type": "header",
                "bbox": [
                    1609,
                    250,
                    2330,
                    290
                ],
                "angle": 0,
                "content": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    214,
                    339,
                    782,
                    396
                ],
                "angle": 0,
                "content": "H.4 Deductive Reasoning"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    441,
                    402,
                    2103,
                    448
                ],
                "angle": 0,
                "content": "The highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    247,
                    471,
                    502,
                    518
                ],
                "angle": 0,
                "content": "Vanilla ICL"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    255,
                    551,
                    568,
                    587
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    252,
                    590,
                    1162,
                    735
                ],
                "angle": 0,
                "content": "Given a problem statement as contexts, the task is to answer a logical reasoning question. For each question, choose from the following options: A) True, B) False, or C) Unknown."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    255,
                    765,
                    405,
                    801
                ],
                "angle": 0,
                "content": "Context:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    252,
                    801,
                    1137,
                    877
                ],
                "angle": 0,
                "content": "The cow likes the lion. The cow visits the tiger. (… more context here …)"
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    255,
                    910,
                    420,
                    943
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    252,
                    943,
                    1127,
                    1059
                ],
                "angle": 0,
                "content": "Based on the above information, is the following statement true, false, or unknown? The tiger is not young."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    255,
                    1089,
                    657,
                    1164
                ],
                "angle": 0,
                "content": "The correct option is: C."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    247,
                    1260,
                    696,
                    1316
                ],
                "angle": 0,
                "content": "Vanilla ICL plus CoT"
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    255,
                    1339,
                    566,
                    1376
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    252,
                    1376,
                    1162,
                    1521
                ],
                "angle": 0,
                "content": "Given a problem statement as contexts, the task is to answer a logical reasoning question. For each question, choose from the following options: A) True, B) False, or C) Unknown."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    255,
                    1554,
                    405,
                    1590
                ],
                "angle": 0,
                "content": "Context:"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    252,
                    1590,
                    1137,
                    1666
                ],
                "angle": 0,
                "content": "The cow likes the lion. The cow visits the tiger. (… more context here …)"
            },
            {
                "block_id": 17,
                "type": "title",
                "bbox": [
                    255,
                    1696,
                    420,
                    1732
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    252,
                    1732,
                    1127,
                    1848
                ],
                "angle": 0,
                "content": "Based on the above information, is the following statement true, false, or unknown? The tiger is not young."
            },
            {
                "block_id": 19,
                "type": "title",
                "bbox": [
                    255,
                    1877,
                    441,
                    1913
                ],
                "angle": 0,
                "content": "Reasoning:"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    252,
                    1913,
                    1170,
                    2204
                ],
                "angle": 0,
                "content": "The tiger likes the cow. The tiger likes the squirrel. If something likes the squirrel and it likes the cow, then it visits the tiger. So the tiger visits the tiger. If something visits the tiger then it is nice. So the tiger is nice. If something is nice and it sees the tiger then it is young. So the tiger is young. The correct option is: B."
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    1377,
                    471,
                    2226,
                    528
                ],
                "angle": 0,
                "content": "CoT w. self-generated logical constraints"
            },
            {
                "block_id": 22,
                "type": "title",
                "bbox": [
                    1384,
                    551,
                    1693,
                    587
                ],
                "angle": 0,
                "content": "Task Description:"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    1382,
                    590,
                    2289,
                    735
                ],
                "angle": 0,
                "content": "Given a problem statement as contexts, the task is to answer a logical reasoning question. For each question, choose from the following options: A) True, B) False, or C) Unknown."
            },
            {
                "block_id": 24,
                "type": "title",
                "bbox": [
                    1384,
                    765,
                    1535,
                    801
                ],
                "angle": 0,
                "content": "Context:"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    1382,
                    801,
                    2269,
                    877
                ],
                "angle": 0,
                "content": "The cow likes the lion. The cow visits the tiger. (… more context here …)"
            },
            {
                "block_id": 26,
                "type": "title",
                "bbox": [
                    1384,
                    910,
                    1552,
                    943
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    1382,
                    943,
                    2254,
                    1059
                ],
                "angle": 0,
                "content": "Based on the above information, is the following statement true, false, or unknown? The tiger is not young."
            },
            {
                "block_id": 28,
                "type": "title",
                "bbox": [
                    1384,
                    1089,
                    2106,
                    1125
                ],
                "angle": 0,
                "content": "From the text, we could first get facts:"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    1384,
                    1125,
                    2289,
                    1633
                ],
                "angle": 0,
                "content": "The tiger likes the cow. The tiger likes the squirrel.   \nDue to the mentioned rules: if something likes the squirrel and it likes the cow, then it visits the tiger.   \nWe can deduce the facts: the tiger visits the tiger.   \nDue to the mentioned rules: if something visits the tiger then it is nice.   \nWe can deduce the facts: the tiger is nice.   \nDue to the mentioned rules: if something is nice and it sees the tiger then it is young.   \nWe can deduce the facts: the tiger is young.   \nTherefore, the correct option is: B."
            },
            {
                "block_id": 30,
                "type": "footer",
                "bbox": [
                    214,
                    2343,
                    1206,
                    2382
                ],
                "angle": 0,
                "content": "Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009"
            }
        ]
    }
]