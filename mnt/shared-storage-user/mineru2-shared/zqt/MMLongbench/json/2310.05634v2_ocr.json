[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    359,
                    301,
                    2116,
                    442
                ],
                "angle": 0,
                "content": "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    553,
                    480,
                    1930,
                    543
                ],
                "angle": 0,
                "content": "Xinze Li\\(^{1}\\), Yixin Cao\\(^{2\\dagger}\\), Liangming Pan\\(^{3}\\), Yubo Ma\\(^{1}\\), Aixin Sun\\(^{1\\dagger}\\)"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    796,
                    599,
                    1679,
                    655
                ],
                "angle": 0,
                "content": "<sup>1</sup> S-Lab, Nanyang Technological University"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    451,
                    655,
                    2024,
                    719
                ],
                "angle": 0,
                "content": "\\(^{2}\\) Singapore Management University \\(^{3}\\) University of California, Santa Barbara"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    451,
                    719,
                    2031,
                    775
                ],
                "angle": 0,
                "content": "{xinze002，yubo001}@e.ntu.edu.sg axsun@ntu.edu.sg"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    615,
                    778,
                    1860,
                    834
                ],
                "angle": 0,
                "content": "yxcao@smu.edu.sg liangmingpan@ucsb.edu"
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    647,
                    915,
                    846,
                    968
                ],
                "angle": 0,
                "content": "Abstract"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    352,
                    1024,
                    1143,
                    2578
                ],
                "angle": 0,
                "content": "Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. Although language attribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new \"Conscious Incompetence\" setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the \"Conscious Incompetence\" setting, and the critical role of retrieval accuracy."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    285,
                    2631,
                    645,
                    2683
                ],
                "angle": 0,
                "content": "1 Introduction"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    280,
                    2725,
                    1218,
                    3234
                ],
                "angle": 0,
                "content": "Recently, Large Language Models (Brown et al., 2020) (LLMs) have exhibited great capability in open-ended question answering (Yang et al., 2019). However, the generated answers may include factual errors and are not always reliable, and is commonly known as the \"hallucination\" (Shuster et al., 2021; Ji et al., 2023) problem. For instance, LLMs may give wrong diagnosis to patient's symptoms. Hallucination has severe harms especially on indus"
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1282,
                    1020,
                    2158,
                    1739
                ],
                "angle": 0,
                "content": null,
                "caption": "Output: Artemisia Gentileschi was an Italian painter born on July 8, 1596 [NA] in Rome [Q212657, citizen: Italy, occupation: painter, place of birth: Rome]. She was a member of ..."
            },
            {
                "block_id": 13,
                "type": "image_caption",
                "bbox": [
                    1262,
                    1866,
                    2188,
                    2118
                ],
                "angle": 0,
                "content": "Figure 1: A demonstration of our task set up. Given a question, the system generates answers attributed from a retrieved knowledge graph. The underlines in question are the retrieved entities, and the underlines in outputs are the citations. [NA] is the \"Not Applicable Citation\"."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1260,
                    2213,
                    2188,
                    2322
                ],
                "angle": 0,
                "content": "tries that require precision and factual knowledge like finance, law, and medical treatment."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1260,
                    2332,
                    2193,
                    3234
                ],
                "angle": 0,
                "content": "To minimize the negative impacts, researchers have proposed the task of language attribution (Bohnet et al., 2023), which not only enables users to verify the generated text flexibly but also contributes to many important applications, such as situation reports (Reddy et al., 2023), academic papers (Salvagno et al., 2023), medical diagnosis (Zuccon and Koopman, 2023). Existing works mainly attribute generated outputs to unstructured documents like web pages (Nakano et al., 2021; Menick et al., 2022) or passages (Gao et al., 2023). To verify the answer quality, they typically compare with a human annotated reference answer for automatic evaluation or conduct human evaluation. We argue that there are several concerns on such task definition. Firstly, are documents the only source"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1218,
                    862
                ],
                "angle": 0,
                "content": "for attribution? Many real-world applications have their own knowledge bases or semi-structured reports. Secondly, does the attribution source always include all the required knowledge? We consider the coverage issue since no perfect repository can contain all the information in this world. Thirdly, how to systematically evaluate the attributed content without references? For open-ended questions, there are unlimited number of answers and it is difficult to define a single ground truth."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    287,
                    869,
                    1220,
                    2452
                ],
                "angle": 0,
                "content": "To address the first challenge, we utilize knowledge graph (KG) as a reliable source for attribution, namely Knowledge-aware Language Model Attribution (KaLMA). We show a demonstration of task in Figure 1. KGs efficiently organize world knowledge in a structured manner and has the potential to unify various formats of data. For example, databases can be easily converted into KGs, or, passages and web pages can be represented as a node in KG like Wikipedia. KaLMA differs from entity linking (Sevgili et al., 2022) since the sentences or phrases are attributed to a knowledge triplet rather than a single entity. For the second challenge, we tackle the coverage problem by making the model aware of its limitations. We introduce a new setting \"Conscious Incompetence\" (Curtiss and Warren, 1974), which is the psychological stage that one is aware of the knowledge gap. During generation, LLMs identify sentences that require supporting knowledge absent in the knowledge graph. Our setting enables an attributed LM to recognize the knowledge gaps and allows users to verify uncertain claims, which enhances trustworthiness. For the third challenge, we propose a comprehensive automatic evaluation metric including text quality, citation quality, and text citation alignment. The entire evaluation process does not require human annotated ground truth."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    2466,
                    1218,
                    3030
                ],
                "angle": 0,
                "content": "To implement the above innovations, we first design an automatic dataset construction pipeline. Using this pipeline, we construct a dataset<sup>1</sup> in the biographical domain, namely BioKaLMA, for a benchmark with all-rounded automatic measurements. Biography forms a good test-set for attribution due to its practical application and convenient evaluation. The availability of high-quality knowledge graph like WikiData also benefits our dataset construction. Derived from the biograph-"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    1257,
                    294,
                    2195,
                    915
                ],
                "angle": 0,
                "content": "ical database2 (Plum et al., 2022) and WikiData, BioKaLMA contains 1,085 data entries. Each data entry includes question and knowledge required to answer the question. For evaluation, we separately evaluate the generated text, the generated citations, and the alignment between texts and citations. We use G-Eval (Liu et al., 2023b) to automatically evaluate the text quality. We also design measurement for correctness, precision, and recall for citations. Lastly, we determine the alignment between texts and citations employing NLI (Dagan et al., 2005)"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    1260,
                    919,
                    2195,
                    1880
                ],
                "angle": 0,
                "content": "We summarize our contributions as follows: 1) We define the task of Knowledge-aware Language Model Attribution (KaLMA) that attributes language models to structured knowledge. 2) We design a complete benchmarking pipeline, including dataset, baseline, and evaluation metrics. 3) We conduct extensive experiments and show room for improvement of the LLMs' ability to generate accurate and thorough citations based on provided knowledge graphs. Our experiments on \"Conscious Incompetence\" investigate the capability of current LLMs to identify if there are required knowledge not in knowledge graph. We highlight the necessity of incorporating this setting in future language attribution works. Furthermore, our ablation studies demonstrate the crucial role of retrieval accuracy in achieving desirable generation results."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    1262,
                    1929,
                    1721,
                    1982
                ],
                "angle": 0,
                "content": "2 Task and Dataset"
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1262,
                    2027,
                    1726,
                    2076
                ],
                "angle": 0,
                "content": "2.1 Task Formulation"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1257,
                    2104,
                    2195,
                    2609
                ],
                "angle": 0,
                "content": "We hereby define the task Knowledge-aware Language Model Attribution (KaLMA): Given a question \\( q \\) and the knowledge graph \\( G \\), the system generates an output text \\( t \\) that answers the question. The output text consists of a list of \\( m \\) sentences \\( s_1, \\ldots, s_m \\) grounded with a list of \\( n \\) grounded knowledge \\( k_1 \\ldots k_n \\) where \\( \\{k_1 \\ldots k_n\\} \\in G \\). Each knowledge \\( k \\) is a sub-graph of \\( G \\). Each sentence \\( s \\) may be grounded by zero up to multiple knowledge."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1260,
                    2652,
                    2193,
                    3101
                ],
                "angle": 0,
                "content": "Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence \\( s \\) in the output text \\( t \\) can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph \\( G \\). A sentence can map to both [NA] and a list of sub-graph knowledge if it can"
            },
            {
                "block_id": 9,
                "type": "page_footnote",
                "bbox": [
                    280,
                    3101,
                    1245,
                    3230
                ],
                "angle": 0,
                "content": "<sup>1</sup>The codes and dataset BioKaLMA are publicly available in https://github.com/lixinze777/Knowledge-aware-Language-Model-Attribution"
            },
            {
                "block_id": 10,
                "type": "page_footnote",
                "bbox": [
                    1260,
                    3136,
                    1905,
                    3230
                ],
                "angle": 0,
                "content": "2https://plumaj.github.io/biographical/"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    282,
                    298,
                    1215,
                    473
                ],
                "angle": 0,
                "content": "be partially verified by the knowledge graph \\( G \\). [NA] is not a citation on conventional means, but a indicator of knowledge gap."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    285,
                    522,
                    818,
                    571
                ],
                "angle": 0,
                "content": "2.2 Dataset Construction"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    603,
                    1215,
                    1280
                ],
                "angle": 0,
                "content": "Each entry of dataset bioKaLMA includes two questions and a minimum knowledge set. The two questions enquire about the same people on similar aspects of their life stories. The minimum knowledge set is the smallest set of knowledge that is required to answer each question. One question is a general version and the other is specific. The general questions are more concise and natural for human readers, and the specific version questions have a tighter bond to the minimum knowledge set, and is hence more accurate for evaluating LLMs. An example data piece is shown in Table 1."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    290,
                    1290,
                    1215,
                    2588
                ],
                "angle": 0,
                "content": "We construct the dataset using an automatic pipeline consisting of three steps: Person Selection, Name Disambiguation, and Evolutionary Question Generation. In the first two steps, we use SPARQL queries to select related people from human written sentences and identify their identity in WikiData. In the third step, we iteratively construct paragraph and question about the selected people. The first iteration starts with a human written sentence about the selected people. In each next iteration, we apply a data selection algorithm to select an appropriate knowledge from WikiData based on the existing paragraph, and extend the paragraph to include the additional knowledge using LLM. Then, LLM constructs the questions using the final paragraph as an answer. The general and specific questions are generated with different prompts and demonstrations. All the selected knowledge from each iteration form the \"minimum knowledge set\" for the question. While we use the human biography domain as an example, this method is applicable to all domains. We present the details of the data construction in Appendix A."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    285,
                    2641,
                    724,
                    2697
                ],
                "angle": 0,
                "content": "2.3 Dataset Analysis"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2725,
                    1215,
                    3230
                ],
                "angle": 0,
                "content": "Statistics There are 1,085 data entries in BioKalMA. On average, there are 6.8 pieces of knowledge in each \"minimum knowledge set\". BioKalMA demonstrates a good demographic variation. It includes a wide range of geographical distribution of people from 196 countries and 949 cities, taking 279 kinds of different occupations. The eras of people span from 1950 B.C. to 2001 A.D."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1285,
                    294,
                    2160,
                    694
                ],
                "angle": 0,
                "content": "General Question: Who were Oscar and Richard Hertwig, and what were their contributions to the fields of anatomy and biology? Specific Question: What were the career paths and significant contributions of Oscar and Richard Hertwig in the fields of anatomy and biology, and who were their notable mentors and students?"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1285,
                    733,
                    2133,
                    1115
                ],
                "angle": 0,
                "content": "Minimum Knowledge Set:  \n['Q85907', 'occupation', 'biologist']  \n['Q85907', 'doctoral student', 'Stanislaus von Prowazek']  \n['Q68753', 'doctoral advisor', 'Ernst Haeckel']  \n['Q68753', 'student of', 'Ernst Haeckel']  \n['Q68753', 'nominated for', 'Nobel Prize in Physiology or Medicine']"
            },
            {
                "block_id": 9,
                "type": "table",
                "bbox": [
                    1270,
                    1319,
                    2089,
                    1613
                ],
                "angle": 0,
                "content": "<table><tr><td>Metric (full score)</td><td>General</td><td>Specific</td></tr><tr><td>Authenticity (1)</td><td>1.00</td><td>1.00</td></tr><tr><td>Relevance (1)</td><td>0.73</td><td>0.84</td></tr><tr><td>Naturalness (5)</td><td>4.38</td><td>3.52</td></tr><tr><td>Significance (5)</td><td>3.94</td><td>3.68</td></tr></table>",
                "caption": "Table 1: An example for generated data entry in BioKaLMA. Q85907 and Q68753 are Richard Hertwig and Oscar Hertwig's QIDs in WikiData"
            },
            {
                "block_id": 10,
                "type": "table_footnote",
                "bbox": [
                    1285,
                    1613,
                    2158,
                    1655
                ],
                "angle": 0,
                "content": "Table 2: Human Evaluation on BioKaLMA dataset."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    1754,
                    2195,
                    2431
                ],
                "angle": 0,
                "content": "Evaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1) Authenticity: The generated questions should accurately reflect the objective facts. 2) Relevance: Each minimum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3) Naturalness: The generated question should be concise and understandable by human readers. 4) Significance: The generated question should be meaningful and helpful to users."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1260,
                    2438,
                    2193,
                    2662
                ],
                "angle": 0,
                "content": "To our best knowledge, there is no perfect automatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1260,
                    2666,
                    2193,
                    2950
                ],
                "angle": 0,
                "content": "We randomly sample 50 data entries from BioKaLMA and ask human annotators to evaluate the data entries based on the four metrics. The general and specific questions are evaluated separately. More details are given in Appendix C."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1260,
                    2953,
                    2195,
                    3234
                ],
                "angle": 0,
                "content": "The final result for each metric is taken average and reported in Table 2. For both general and specific settings, the questions from sample achieve a \\(100\\%\\) authenticity, which indicates that the overall authenticity of BioKaLMA dataset is high. The rel-"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1213,
                    743
                ],
                "angle": 0,
                "content": "evance on general and specific settings are \\(73\\%\\) and \\(84\\%\\) respectively. The specific question normally consists of more parts and include more details than its general version, and hence some knowledge are necessary to the specific version but not to the general version. However, the general version questions sacrifice relevance to achieve better naturalness and significance."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    290,
                    754,
                    1215,
                    1308
                ],
                "angle": 0,
                "content": "In practice, it is difficult to define a precise \"minimum knowledge set\" for a question unless it is very specific. However, a very specific question tends to be artificial. The relevance and naturalness of a question have a trade-off relationship. It is yet challenging to generate questions that have both high relevance and high naturalness, but our generation method allows for a control on the granularity of a question on whether it tends to be more natural or more relevant."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    282,
                    1357,
                    540,
                    1413
                ],
                "angle": 0,
                "content": "3 Method"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1448,
                    1215,
                    1789
                ],
                "angle": 0,
                "content": "We build a baseline to enable LLMs to generate knowledge-aware attributed answers. Following the approach of many retrieval augmented generation works (Lee et al., 2022; Izacard and Grave, 2021), we utilize a pipeline consisting of three components: retrieval, re-ranking, and generation."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    1831,
                    583,
                    1880
                ],
                "angle": 0,
                "content": "3.1 Retrieval"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    1904,
                    1215,
                    2634
                ],
                "angle": 0,
                "content": "Our baseline retrieval process consists of two parts: named entity recognition and graph retrieval. We utilize \\(\\mathsf{spaCy}^3\\) to identify the named entities mentioned in the question. Using these entities, we retrieve entity-centered sub-graphs using SPARQL. For each retrieved entity, we search for nodes in the graph that match the entity's name. We use the named entity recognition (NER) entity type as a simple filter (e.g., the NER category \"person\" matches the \"human\" entity type in WikiData). Taking each selected node as the center, we retrieve one-hop sub-graphs that contain properties associated with the entity."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    282,
                    2680,
                    627,
                    2736
                ],
                "angle": 0,
                "content": "3.2 Re-ranking"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    2753,
                    1213,
                    3146
                ],
                "angle": 0,
                "content": "The re-ranking component plays a crucial role in disambiguating retrieved entities, as multiple entities may share the same name in the WikiData graph. Two common scenarios are different individuals with the same name (e.g., Anne Hathaway the American actress and Anne Hathaway the wife of William Shakespeare) and different references"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1260,
                    298,
                    2190,
                    694
                ],
                "angle": 0,
                "content": "to the same word (e.g., “Chinese” the language and “Chinese” the ethnic group). When multiple entities are retrieved from the graph for a given entity name, we rank the graphs based on the Exact Match (EM) between the neighboring nodes and the question. We select the entity with the highest number of matched neighboring nodes."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1262,
                    729,
                    1602,
                    775
                ],
                "angle": 0,
                "content": "3.3 Generation"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    799,
                    2195,
                    2434
                ],
                "angle": 0,
                "content": "The generation component effectively prompt the LLMs with the retrieved knowledge graphs (KGs) to generate answers that attribute the KG. To adapt to the input format of the LLMs, we transform the structured KGs into flat texts. We preserve the information of the retrieved sub-graphs by mapping each sub-graph to a set of triples. Each triple consists of two nodes and one edge, where one node is the centered entity, the other node is its neighbor, and the edge represents the relationship between them. For example, [Q212657 - place of birth - Q220] can be translated to [Artemisia Gentileschi - place of birth - Rome]. In this translation, we use the names of the entities for better comprehension by both the models and humans, since WikiData utilizes QIDs (e.g., Q220) to represent unique entities. We construct a prompt (Table 13 in appendix D) which includes 1) instruction to the models to generate attributed answers. 2) retrieved knowledge graph, and 3) the question. We employ one-shot in-context learning (Brown et al., 2020) by preponding one human written demonstration. In the one-shot demonstration, we use the special token [NA] to represent the “Not Applicable Citations” for conscious incompetence. We deliberately omit some knowledge in the demonstration example knowledge graph, and we insert [NA] tokens in the corresponding sentences that use these knowledge within the example answer."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1262,
                    2476,
                    1761,
                    2529
                ],
                "angle": 0,
                "content": "4 Evaluation Metrics"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1260,
                    2560,
                    2193,
                    2841
                ],
                "angle": 0,
                "content": "Our benchmark includes evaluation metrics for both the generated text and citations. We also evaluate the alignment between the text and corresponding citations. We provide more discussions on the design of evaluation metrics in subsection 4.5."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1262,
                    2880,
                    1689,
                    2929
                ],
                "angle": 0,
                "content": "4.1 Text Evaluation"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1260,
                    2953,
                    2195,
                    3230
                ],
                "angle": 0,
                "content": "Since our test-set has no human-written gold answers as references, we do not utilize comparison-based metrics such as BERTScore (Zhang et al., 2019a) or MAUVE (Pillutla et al., 2021). Instead, we employ reference-free NLG evaluator G-"
            },
            {
                "block_id": 15,
                "type": "page_footnote",
                "bbox": [
                    337,
                    3178,
                    1193,
                    3230
                ],
                "angle": 0,
                "content": "3https://spacy.io/api-entityrecognizer"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    290,
                    287,
                    1205,
                    947
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 2: An illustration of how we evaluate the precision and recall for generated citations."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    1143,
                    1213,
                    1592
                ],
                "angle": 0,
                "content": "Eval (Liu et al., 2023b), which defines the following four metrics: 1) Coherence: whether the generated text is well-structured and well-organized. 2) Consistency: whether the generated text is consistent with the knowledge provided. 3) Fluency: whether the generated text is well-written and grammatical. 4) Relevance: how well is the generated text relevant to the question."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1596,
                    1215,
                    1876
                ],
                "angle": 0,
                "content": "We use the model text-davinci-003 for evaluation, which assigns an integer score of 1 to 5 for each metric. We follow the prompt provided in G-Eval (Liu et al., 2023b) and customize it based on our task. The full prompts are given in appendix D."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    1911,
                    781,
                    1960
                ],
                "angle": 0,
                "content": "4.2 Citation Evaluation"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    1985,
                    1213,
                    2550
                ],
                "angle": 0,
                "content": "We evaluate the citation qualities from three aspects: 1) Correctness, which measures whether the generated knowledge matches the given knowledge from the knowledge graph, 2) Precision, which determines how much of the generated citations are helpful to answer the question, and 3) Recall, which measures how much of the minimum knowledge set are covered by the generated citations. We also calculate the F1-Score based on the Precision and Recall to reflect the overall quality of citations."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    280,
                    2581,
                    1213,
                    2974
                ],
                "angle": 0,
                "content": "Correctness We calculate the citation correctness for each citation (0 or 1) and average over all citations. Each citation comprises a triplet of 1) center entity QID, 2) relation 3) neighbour entity value. If the generated citation is complete with all three parts, and exactly matches a triplet from the question's retrieved KG, correctness = 1."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    3006,
                    1213,
                    3234
                ],
                "angle": 0,
                "content": "Precision We calculate citation precision for each citation (0 or 1) and average over all citations to get micro precision. Precision = 1 for a citation if and only if 1) it is correct, and 2) it matches one"
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    1267,
                    287,
                    2188,
                    947
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 3: An illustration of how we evaluate the precision and recall for conscious incompetence ([NA])"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    1147,
                    2190,
                    1255
                ],
                "angle": 0,
                "content": "knowledge triplet from minimum knowledge set of the question. (See Figure 2.)"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    1297,
                    2193,
                    1578
                ],
                "angle": 0,
                "content": "Recall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall = 1 if and only if the knowledge is hit by a correct citation. (See Figure 2.)"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1260,
                    1582,
                    2193,
                    1859
                ],
                "angle": 0,
                "content": "We average over all citations/knowledge in an answer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding precision and recall."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1262,
                    1908,
                    1853,
                    1960
                ],
                "angle": 0,
                "content": "4.3 Text-Citation Alignment"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1257,
                    1985,
                    2193,
                    3002
                ],
                "angle": 0,
                "content": "Other than the text quality and citation quality, we measure whether the generated citations provide support for the corresponding sentences. A piece of useful knowledge is not an ideal citation if it is irrelevant to the sentence it links to. Therefore, we propose the metric \"Alignment\" which determines whether the generated citations are aligned to the sentences to which they belong. We use a state-of-the-art natural language inference (NLI) model TRUE (Honovich et al., 2022), which is a fine-tuned T5-11B (Raffel et al., 2020) model, to check whether the generated sentence entails the generated citation. Since one sentence could have multiple citations, we run NLI on all sentence-citation pairs and report the percentage of entailment. Additionally, we conduct human evaluation in § 5.4 to showcase if the automatic evaluation is correlated with human judgments."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1260,
                    3044,
                    2081,
                    3097
                ],
                "angle": 0,
                "content": "4.4 Conscious Incompetence Evaluation"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1260,
                    3122,
                    2188,
                    3234
                ],
                "angle": 0,
                "content": "Theoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1213,
                    1087
                ],
                "angle": 0,
                "content": "knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the \"absent knowledge ground truth\". In subsequent rounds, we each remove one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    280,
                    1090,
                    1213,
                    1371
                ],
                "angle": 0,
                "content": "We employ the NLI model TRUE (Honovich et al., 2022) to measure the alignment between sentences and knowledge. A sentence with [NA] should be aligned to an absent knowledge. We calculate precision and recall for [NA]."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    1406,
                    1213,
                    1746
                ],
                "angle": 0,
                "content": "[NA] precision We calculate [NA] precision for each sentence with [NA] (0 or 1) and average over all sentences with [NA]. Precision = 1 for a sentence if and only if it entails one knowledge triplet from absent knowledge set of the question. (See Figure 3.)"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1785,
                    1213,
                    2066
                ],
                "angle": 0,
                "content": "[NA] Recall We calculate [NA] recall for each knowledge (0 or 1) in absent knowledge set and average over all absent knowledge. Recall = 1 if and only if the knowledge is entailed by a sentence with [NA]. (See Figure 3.)"
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    282,
                    2108,
                    1044,
                    2164
                ],
                "angle": 0,
                "content": "4.5 Discussion on Evaluation metrics"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2185,
                    1213,
                    2466
                ],
                "angle": 0,
                "content": "In this section, we discuss on the evaluation metrics of benchmark BioBaLMA. We design the evaluation metrics from multiple dimensions to incorporate different understandings on what makes a high quality citation."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    337,
                    2511,
                    1210,
                    2904
                ],
                "angle": 0,
                "content": "• One understanding argues when the answer contains mistakes, even if the citation is correctly answering the questions, it cannot represent good LLM attribution performance. In this case, citation quality is considered as a measure of overall attribution performance, including the answer quality."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    337,
                    2950,
                    1210,
                    3234
                ],
                "angle": 0,
                "content": "- The other understanding argues for a complete decoupling of answer and citation quality. In this scenario, even if the answer is wrong, the citation is valuable as long as it provides reasonable support for the question. In such"
            },
            {
                "block_id": 8,
                "type": "list",
                "bbox": [
                    337,
                    2511,
                    1210,
                    3234
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1352,
                    298,
                    2193,
                    406
                ],
                "angle": 0,
                "content": "cases, citations do not give advice on the correctness of the answer."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    449,
                    2193,
                    954
                ],
                "angle": 0,
                "content": "Both understandings are plausible, and hence we have considered both of them when we design metrics. The alignment score is designed based on the first understanding, which measures whether the citations are closely linked to the answer. The precision and recall are designed for the second understanding, where the citations are completely decoupled from the answer, and are correct if they provide support for the question."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    957,
                    2193,
                    1294
                ],
                "angle": 0,
                "content": "In addition, we also incorporate an edge case for design of the [NA] precision calculation. If an NA-marked sentence does not answer the question at all, it is considered correct in the [NA] precision calculation. In this case, the LLM correctly identifies a sentence that requires further verification."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1262,
                    1392,
                    1625,
                    1452
                ],
                "angle": 0,
                "content": "5 Experiments"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1260,
                    1480,
                    2193,
                    2153
                ],
                "angle": 0,
                "content": "We run through the method pipeline described in § 3 on different LLMs and present the results in this section. Since we aim to obtain a more accurate evaluation, we conduct our main experiments on the specific questions setting, since the minimum knowledge set has a higher relevance on the specific questions. However, we will also provide evaluation results for the general questions in § 5.5 as ablation studies. The implementation details are reported in appendix B. We report five model baselines from both open and closed source model families:"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1260,
                    2188,
                    2193,
                    2473
                ],
                "angle": 0,
                "content": "OpenAI Models We use GPT4 (gpt-4-0314) and ChatGPT (gpt-3.5-turbo-0301) for our experiments. For ChatGPT, we experiment on temperature of 0.1, 0.5, and 0.9 to obtain different levels of randomness and creativity in generation."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1260,
                    2501,
                    2193,
                    2897
                ],
                "angle": 0,
                "content": "LLaMA We conduct experiments with LLaMA-7B (Touvron et al., 2023) and LLaMA-13B since they are powerful open-source models that are widely accessible. We have also conducted human instruction tuned LLaMA models, including Alpaca-7B (Taori et al., 2023) and Vicuna-13B (Chiang et al., 2023)."
            },
            {
                "block_id": 16,
                "type": "title",
                "bbox": [
                    1262,
                    2936,
                    1639,
                    2985
                ],
                "angle": 0,
                "content": "5.1 Main Results"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1260,
                    3006,
                    2193,
                    3234
                ],
                "angle": 0,
                "content": "Citation Quality Evaluation We present the main results in Table 3. For correctness, we report on a micro scale. For precision, recall, and F1-Score, we report on both micro and macro scales."
            }
        ]
    },
    {
        "page_id": 6,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    325,
                    287,
                    2141,
                    866
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"5\">Micro</td><td colspan=\"3\">Macro</td></tr><tr><td>Align.</td><td>Corr.</td><td>Prec.</td><td>Rec.</td><td>F1.</td><td>Prec.</td><td>Rec.</td><td>F1.</td></tr><tr><td>GPT-4 (0.5)</td><td>92.0(1.5)</td><td>97.6(0.1)</td><td>36.0(0.6)</td><td>43.6(1.0)</td><td>39.4</td><td>40.7(1.1)</td><td>43.9(1.0)</td><td>42.3</td></tr><tr><td>ChatGPT (0.1)</td><td>85.9(2.5)</td><td>96.1(0.4)</td><td>29.0(0.0)</td><td>50.8(0.3)</td><td>36.9</td><td>32.7(0.4)</td><td>51.2(0.3)</td><td>39.9</td></tr><tr><td>ChatGPT (0.5)</td><td>84.5(1.1)</td><td>94.8(0.2)</td><td>29.9(0.2)</td><td>49.0(0.8)</td><td>37.2</td><td>34.1(0.5)</td><td>49.4(0.9)</td><td>40.4</td></tr><tr><td>ChatGPT (0.9)</td><td>84.1(0.5)</td><td>94.2(0.4)</td><td>28.7(0.2)</td><td>49.0(0.3)</td><td>36.2</td><td>32.5(0.2)</td><td>49.4(0.3)</td><td>39.2</td></tr><tr><td>Alpaca-7B</td><td>46.9(0.9)</td><td>78.9(0.6)</td><td>14.9(1.4)</td><td>19.4(0.2)</td><td>16.8</td><td>19.8(0.4)</td><td>19.9(0.3)</td><td>19.8</td></tr><tr><td>LLaMA-7B</td><td>47.8(0.8)</td><td>70.2(0.2)</td><td>7.7(2.4)</td><td>41.1(0.7)</td><td>13.0</td><td>11.0(1.9)</td><td>41.4(0.7)</td><td>17.4</td></tr><tr><td>LLaMA-13B</td><td>62.1(0.4)</td><td>71.7(1.9)</td><td>10.5(3.3)</td><td>43.7(1.0)</td><td>16.9</td><td>13.8(2.2)</td><td>43.5(1.0)</td><td>20.9</td></tr><tr><td>Vicuna-13B</td><td>66.9(0.1)</td><td>59.0(0.6)</td><td>14.9(0.2)</td><td>16.8(0.0)</td><td>15.8</td><td>15.1(0.0)</td><td>17.0(0.0)</td><td>16.0</td></tr></table>"
            },
            {
                "block_id": 1,
                "type": "table_footnote",
                "bbox": [
                    280,
                    869,
                    2188,
                    968
                ],
                "angle": 0,
                "content": "Table 3: Citation Quality OpenAI models and LLaMA family models. The first five metrics are reported in Micro, and the last three metrics are reported in Macro. We also report text citation alignment."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    277,
                    1055,
                    1205,
                    1161
                ],
                "angle": 0,
                "content": "The experimental results are the mean of three runs, and the standard deviation is reported in brackets."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    277,
                    1171,
                    1213,
                    2129
                ],
                "angle": 0,
                "content": "In general, there is a room of improvement for all models since no model can achieve a micro F1 Score of higher than 40. The OpenAI models outperform the LLaMA family models in almost all metrics. The correctness is above 94 for OpenAI models, but around 70 for LLaMA based models. For ChatGPT, temperature does not play a significant role since it effect on F1 Score is at most 1.2. The GPT-4 model achieves the best performance across almost all metrics, except for recall, since GPT-4 models tend to generate shorter answers with fewer citations, resulting in higher precision. While LLaMA is better at Recall by generating long answers with many citations. The F1-Score of models from the same family are close to one another, showing that our automatic evaluation metric designed is reliable."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    2174,
                    1213,
                    2795
                ],
                "angle": 0,
                "content": "Text-Citation Alignment From Table 3, similar to citation quality, the OpenAI models also outperform the LLaMA based models on text-citation alignment. In addition, models with 7B, 13B, 175B (ChatGPT), and trillion level (GPT4) parameters have an alignment score of \\(40+\\), \\(60+\\), \\(80+\\), and 92 respectively. LLaMA-13B model has an improvement of 14.3 compared to LLaMA-7B model. This shows that parameter size may play an important role in generating sentences and citations with good alignment."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2837,
                    1213,
                    3234
                ],
                "angle": 0,
                "content": "Text Quality Evaluation We present the evaluation of generated text quality in Table 4. From the results, we find that OpenAI models, in general, have better text quality in all metrics compared to LLaMA family models, which corresponds to the citation evaluation results. All models exhibit rather high consistency, indicating that the LLMs"
            },
            {
                "block_id": 6,
                "type": "table",
                "bbox": [
                    1267,
                    1045,
                    2141,
                    1568
                ],
                "angle": 0,
                "content": "<table><tr><td>Model</td><td>Coh.</td><td>Con.</td><td>Flu.</td><td>Rel.</td></tr><tr><td>GPT-4 (0.5)</td><td>4.48</td><td>4.89</td><td>4.64</td><td>4.72</td></tr><tr><td>ChatGPT (0.1)</td><td>4.57</td><td>4.94</td><td>4.69</td><td>4.83</td></tr><tr><td>ChatGPT (0.5)</td><td>4.57</td><td>4.94</td><td>4.71</td><td>4.81</td></tr><tr><td>ChatGPT (0.9)</td><td>4.52</td><td>4.91</td><td>4.67</td><td>4.79</td></tr><tr><td>Alpaca-7B</td><td>4.10</td><td>4.46</td><td>4.23</td><td>3.76</td></tr><tr><td>LLaMa-7B</td><td>3.06</td><td>3.79</td><td>3.62</td><td>2.96</td></tr><tr><td>LLaMa-13B</td><td>3.60</td><td>4.23</td><td>3.94</td><td>3.56</td></tr><tr><td>Vicuna-13B</td><td>3.67</td><td>4.50</td><td>3.96</td><td>3.64</td></tr></table>"
            },
            {
                "block_id": 7,
                "type": "table_footnote",
                "bbox": [
                    1342,
                    1568,
                    2108,
                    1613
                ],
                "angle": 0,
                "content": "Table 4: Evaluation on generated text quality."
            },
            {
                "block_id": 8,
                "type": "table",
                "bbox": [
                    1267,
                    1662,
                    2074,
                    1953
                ],
                "angle": 0,
                "content": "<table><tr><td>Removed</td><td>Corr.</td><td>Prec.</td><td>Rec.</td><td>F1.</td></tr><tr><td>0 (gold)</td><td>95.5</td><td>30.1</td><td>57.1</td><td>39.4</td></tr><tr><td>1</td><td>94.1</td><td>26.1</td><td>42.5</td><td>32.3</td></tr><tr><td>2</td><td>94.0</td><td>21.0</td><td>31.4</td><td>25.2</td></tr><tr><td>3</td><td>93.9</td><td>16.3</td><td>20.4</td><td>18.1</td></tr></table>"
            },
            {
                "block_id": 9,
                "type": "table_footnote",
                "bbox": [
                    1260,
                    1957,
                    2185,
                    2059
                ],
                "angle": 0,
                "content": "Table 5: Citation quality evaluation for generated texts using a KG with N pieces of knowledge removed."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1257,
                    2150,
                    2190,
                    2487
                ],
                "angle": 0,
                "content": "are capable of generating answers that are not contradictory to the provided knowledge or self-contradictory. However, the relevance is relatively low for smaller models, indicating the difficulty these models face in generating answers that are relevant to the questions."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1260,
                    2532,
                    1860,
                    2588
                ],
                "angle": 0,
                "content": "5.2 Conscious Incompetence"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    2609,
                    2190,
                    3171
                ],
                "angle": 0,
                "content": "We first evaluate citation quality of the generated text with knowledge removed using method described in § 4.4. From Table 5, the removal of required knowledge has a minimal impact on correctness, but significantly affects citation precision and recall. With more knowledge absent from provided knowledge graph, both precision and recall drops drastically, demonstrating that the coverage issue poses a considerable challenge to generating answers with high quality citations."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1307,
                    3178,
                    2190,
                    3230
                ],
                "angle": 0,
                "content": "Next, we evaluate [NA] precision and recall."
            }
        ]
    },
    {
        "page_id": 7,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    334,
                    336,
                    1121,
                    961
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 4: Precision, Recall, and F1-Score for [NA]."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    1119,
                    1215,
                    1739
                ],
                "angle": 0,
                "content": "From Figure 4, The recall is stable at about 15 regardless of the number of absent knowledge. This indicates that the current LLMs have ability to identify absent knowledge to a limited extent. While precision and F1-Score exhibit a clear upward trend, which shows that with more absent knowledge in KG, [NA] enables generated outputs to locate absent knowledge more accurately. Therefore, the \"Conscious Incompetence\" setting plays an increasingly crucial role when the coverage problem of knowledge graph is more serious."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    282,
                    1792,
                    756,
                    1848
                ],
                "angle": 0,
                "content": "5.3 Retrieval Analysis"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    1873,
                    1213,
                    2494
                ],
                "angle": 0,
                "content": "We conduct an ablation study to examine the impact of retrieval accuracy on the model's output. The experiment simulates retrieval accuracy from 100 to 20 at intervals of 20. We start with the ground truth knowledge graphs that we used for question construction. In each subsequent rounds, we randomly replace additional \\(20\\%\\) knowledge graphs with irrelevant knowledge graphs to simulate retrieving wrong graphs. The results for citation quality are in Figure 5. Answers are generated using ChatGPT with a temperature of 0.5."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    280,
                    2497,
                    1215,
                    3234
                ],
                "angle": 0,
                "content": "The results show clear downward trends in all metrics as expected when retrieval accuracy dropped. Among precision and recall, the impact of poor retrieval quality on recall (green) is much more significant than on precision (yellow). This indicates that the model has the ability to filter out incorrect knowledge to a certain extent, resulting in less noticeable impact on precision compared to recall. The reduction in recall was nearly linear as retrieval accuracy decreased, which is understandable since a knowledge cannot be cited if it is not provided. The greatest drop in recall occurred between the ground truth (57.1) and 80 accuracy"
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    1312,
                    336,
                    2103,
                    964
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 5: Citation evaluation (Micro) of generated texts using knowledge graphs with retrieval accuracy 100 (gold), 80, 60,40, and 20."
            },
            {
                "block_id": 8,
                "type": "table",
                "bbox": [
                    1267,
                    1168,
                    2136,
                    1406
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>Alignment</td><td>Human Avg.</td></tr><tr><td>ChatGPT(0.5)</td><td>84.5</td><td>82.0</td></tr><tr><td>LLaMA-7B</td><td>47.8</td><td>45.5</td></tr><tr><td>Vicuna-13B</td><td>66.9</td><td>64.5</td></tr></table>",
                "caption": "Table 6: Result of Human Evaluation on text-citation alignment"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1257,
                    1596,
                    2193,
                    2045
                ],
                "angle": 0,
                "content": "(42.5), demonstrating the potential of the model to generate high-quality citations under perfect retrieval conditions. In practice, a retrieval accuracy of 80 is closest to the actual scenario of our experiment (our retrieval accuracy is 75.9). Therefore, when retrieval accuracy is reasonably high, the correctness of citations is not the most significant concern compared to recall."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1262,
                    2087,
                    1751,
                    2136
                ],
                "angle": 0,
                "content": "5.4 Human Evaluation"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1257,
                    2157,
                    2193,
                    2718
                ],
                "angle": 0,
                "content": "We conduct human evaluation to verify the correlation between automatic evaluation and human judgment. We randomly sample 100 sentence-citation pairs from each of the three baselines: ChatGPT (temperature 0.5), LLaMA-7B, and Vicuna-13B. We request two proficient English annotators for each baseline to determine if the citation aligns to the sentence and provides support for it. The reason we choose metric alignment here is in appendix C, with instruction to annotators and IAA."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1257,
                    2725,
                    2195,
                    3234
                ],
                "angle": 0,
                "content": "The comparison between automatically calculated Alignment and human evaluation results is shown in Table 6. For all three baselines, the automatic and human scores are close with a gap within 2.5, despite the significant differences among the baselines. This indicates a strong correlation between the automatically calculated alignment and human judgments. The experiment results demonstrate that the automatic evaluation serves as a reli-"
            }
        ]
    },
    {
        "page_id": 8,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    329,
                    287,
                    2136,
                    638
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\">Setting</td><td rowspan=\"2\">Model</td><td colspan=\"5\">Citation Eval.</td><td colspan=\"4\">Text Eval.</td></tr><tr><td>Align.</td><td>Corr.</td><td>Prec.</td><td>Rec.</td><td>F1.</td><td>Coh.</td><td>Con.</td><td>Flu.</td><td>Rel.</td></tr><tr><td rowspan=\"2\">General</td><td>GPT-4 (0.5)</td><td>90.9</td><td>97.6</td><td>30.8</td><td>42.1</td><td>35.6</td><td>4.38</td><td>4.77</td><td>4.48</td><td>4.48</td></tr><tr><td>ChatGPT (0.5)</td><td>82.7</td><td>94.5</td><td>25.2</td><td>47.4</td><td>32.9</td><td>4.64</td><td>4.89</td><td>4.45</td><td>4.70</td></tr><tr><td rowspan=\"2\">Specific</td><td>GPT-4 (0.5)</td><td>92.0</td><td>97.6</td><td>36.0</td><td>43.6</td><td>39.4</td><td>4.48</td><td>4.89</td><td>4.64</td><td>4.72</td></tr><tr><td>ChatGPT (0.5)</td><td>84.5</td><td>94.8</td><td>29.9</td><td>49.0</td><td>37.2</td><td>4.57</td><td>4.94</td><td>4.71</td><td>4.81</td></tr></table>",
                "caption": "Table 7: Comparison of evaluation results on General and Specific question setting"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    282,
                    778,
                    1213,
                    884
                ],
                "angle": 0,
                "content": "able measurement of the alignment between generated texts and citations."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    282,
                    929,
                    1007,
                    985
                ],
                "angle": 0,
                "content": "5.5 General and Specific Questions"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    1006,
                    1215,
                    2360
                ],
                "angle": 0,
                "content": "We compare experiments results of text, citation (micro), and alignment between the general and specific questions in Table 7. The results show that the same model's answers on specific questions outperform those on general questions in almost all metrics. The finding is not surprising because the specific questions provide clearer instructions to the models on which knowledge to use. In addition, the general questions in the dataset are inherently loosely bonded to the minimum knowledge set, and hence have impacts on the evaluation results. This experiment shows a trade-off between how explicitly the question context mentions the knowledge, and how irreplaceably the knowledge is required by the question. The specific questions target the knowledge more explicitly in the question context, and hence cover the scope of the paragraph better. It stands for an upper bound for knowledge coverage and a lower bound for question naturalness. The general questions implicitly target the knowledge in the question context, and there loosely cover the scope of the paragraph. It stands for an upper bound for question naturalness and a lower bound for knowledge coverage."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    282,
                    2406,
                    669,
                    2459
                ],
                "angle": 0,
                "content": "6 Related Work"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    280,
                    2497,
                    1215,
                    3230
                ],
                "angle": 0,
                "content": "Retrieval-augmented LLMs KiC (Pan et al., 2022) empower models with external memory of multiple formats including knowledge graph but does not explore attribution. WebGPT (Nakano et al., 2021) outsources document retrieval to Microsoft Bing and fine-tunes GPT3 to answer questions. GopherCite (Menick et al., 2022) fine-tunes a Gopher (Rae et al., 2021) model to generate text alongside quotes extracted from Google search. ALCE (Gao et al., 2023) retrieves top-k passages from Wikipedia and asks LLMs to generate outputs with citations to corresponding supporting documents. These works attribute LLMs to unstructured"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1262,
                    778,
                    1947,
                    831
                ],
                "angle": 0,
                "content": "documents but not knowledge graph."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1260,
                    894,
                    2195,
                    1852
                ],
                "angle": 0,
                "content": "Evaluation (Rashkin et al., 2021) define the \"Attributable to Identified Sources\" (AIS) to measure whether model-generated statements are supported by underlying sources. (Bohnet et al., 2022) study an automatic metric (AutoAIS) that formulates evaluation of automated question answering as a NLI task. (Yue et al., 2023) investigate the automatic evaluation of attribution by prompting LLMs and fine-tuning smaller LMs. (Liu et al., 2023a) conduct human evaluation to audit generative search engines for their citation qualities. ALCE (Gao et al., 2023) evaluates generated answers by comparing with gold answers using MAUVE, and calculates precision and recall for citations using NLI. To the best of our knowledge, our evaluation methods are the first framework that requires no human annotated data."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1265,
                    1925,
                    1592,
                    1982
                ],
                "angle": 0,
                "content": "7 Conclusion"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1260,
                    2038,
                    2195,
                    2767
                ],
                "angle": 0,
                "content": "We propose KaLMA that comprises a new dataset BioKaLMA, a pipeline for generating attributed answers by retrieving from KGs, and a set of automatic evaluation metrics to assess text quality, citation quality, and text-citation alignment. We introduce the \"Conscious Incompetence\" setting, enabling LLMs to identify the knowledge required to support the answers but is absent from the KG. Through this benchmark, we address three challenges: incorporating diverse attribution sources, limited attribution source coverage, and the absence of human annotated ground truth for automatic evaluation."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1257,
                    2781,
                    2195,
                    3234
                ],
                "angle": 0,
                "content": "Our extensive experimental results demonstrate that current LLMs still have room for improvement when utilizing KGs as attribution sources. We also highlight the increasing effectiveness of \"Conscious Incompetence\" setting as the coverage of attribution source becomes worse. Lastly, we prove the crucial role of retrieval accuracy in generating high-quality attributed texts."
            }
        ]
    },
    {
        "page_id": 9,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    285,
                    298,
                    548,
                    347
                ],
                "angle": 0,
                "content": "Limitations"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    280,
                    385,
                    1210,
                    722
                ],
                "angle": 0,
                "content": "One limitation is that our work only investigates a simple form of knowledge graph, where each node is an entity, and each sub-graph is a knowledge triple. There are more complicated forms of knowledge graph, where each node is a document. We will explore this setting in future works."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    290,
                    729,
                    1215,
                    1283
                ],
                "angle": 0,
                "content": "Another limitation lies within the text quality evaluation. We uses ChatGPT as the model to evaluate texts, which could potentially have a bias if the model prefers the text style generated by itself. Such bias can be observed from the abnormal phenomenon that the scores of ChatGPT generated answers are higher than that of the GPT4 generated answers for all four dimensions. Due to cost considerations, we do not repeat the text quality evaluation with GPT-4."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    285,
                    1329,
                    786,
                    1385
                ],
                "angle": 0,
                "content": "Ethical Considerations"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    1420,
                    1213,
                    1925
                ],
                "angle": 0,
                "content": "The potential risk is when users leverage the automatic dataset construction pipeline to generate massive hazardous datasets. This can only happen when a structured knowledge of harmful content is available. Otherwise there is no risk as long as the benchmark is used correctly. All data are collected from WikiData which is publicly available. Hence there is no privacy issue. We also conduct human check to ensure there is no offensive content."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    285,
                    2024,
                    530,
                    2076
                ],
                "angle": 0,
                "content": "References"
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    290,
                    2104,
                    1215,
                    2522
                ],
                "angle": 0,
                "content": "Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2023. Attributed question answering: Evaluation and modeling for attributed large language models."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    290,
                    2553,
                    1213,
                    2834
                ],
                "angle": 0,
                "content": "Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    290,
                    2869,
                    1213,
                    3013
                ],
                "angle": 0,
                "content": "Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    290,
                    3044,
                    1213,
                    3230
                ],
                "angle": 0,
                "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot"
            },
            {
                "block_id": 10,
                "type": "list",
                "bbox": [
                    290,
                    2104,
                    1215,
                    3230
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1312,
                    301,
                    2188,
                    396
                ],
                "angle": 0,
                "content": "learners. Advances in neural information processing systems, 33:1877-1901."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1270,
                    434,
                    2193,
                    715
                ],
                "angle": 0,
                "content": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with \\(90\\%\\) * chatgpt quality."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1270,
                    750,
                    2195,
                    1168
                ],
                "angle": 0,
                "content": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Hahuong, Suchin Gururangan, and Noah A. Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1203,
                    2193,
                    1301
                ],
                "angle": 0,
                "content": "Paul R Curtiss and Phillip W Warren. 1974. The dynamics of life skills coaching. life skills series."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1336,
                    2193,
                    1525
                ],
                "angle": 0,
                "content": "Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In *Machine learning challenges* workshop, pages 177-190. Springer."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1561,
                    2190,
                    1701
                ],
                "angle": 0,
                "content": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1739,
                    2193,
                    2017
                ],
                "angle": 0,
                "content": "Or Honovich, Roee Aharoni, Jonathan Herzig, Hāgai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2055,
                    2195,
                    2378
                ],
                "angle": 0,
                "content": "Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2413,
                    2193,
                    2648
                ],
                "angle": 0,
                "content": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2687,
                    2190,
                    2830
                ],
                "angle": 0,
                "content": "Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering. volume 32."
            },
            {
                "block_id": 21,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2866,
                    2193,
                    3051
                ],
                "angle": 0,
                "content": "Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D. Manning, and Kyoung-Gu Woo. 2022. You only need one model for open-domain question answering."
            },
            {
                "block_id": 22,
                "type": "ref_text",
                "bbox": [
                    1270,
                    3087,
                    2193,
                    3230
                ],
                "angle": 0,
                "content": "Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848."
            },
            {
                "block_id": 23,
                "type": "list",
                "bbox": [
                    1270,
                    301,
                    2195,
                    3230
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 10,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    290,
                    301,
                    1215,
                    484
                ],
                "angle": 0,
                "content": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human alignment."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    290,
                    519,
                    1210,
                    799
                ],
                "angle": 0,
                "content": "Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    290,
                    831,
                    1210,
                    1108
                ],
                "angle": 0,
                "content": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    290,
                    1143,
                    1210,
                    1329
                ],
                "angle": 0,
                "content": "Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. 2022. Knowledge-in-context: Towards knowledgeable semi-parametric language models. arXiv preprint arXiv:2210.16433."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    290,
                    1361,
                    1210,
                    1543
                ],
                "angle": 0,
                "content": "Jorge Pérez, Marcelo Arenas, and Claudio Gutierrez. 2009. Semantics and complexity of sparql. ACM Transactions on Database Systems (TODS), 34(3):1-45."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    290,
                    1582,
                    1210,
                    1859
                ],
                "angle": 0,
                "content": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:4816-4828."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    290,
                    1890,
                    1210,
                    2171
                ],
                "angle": 0,
                "content": "Alistair Plum, Tharindu Ranasinghe, Spencer Jones, Constantin Orasan, and Ruslan Mitkov. 2022. Biographical semi-supervised relation extraction dataset. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3121-3130."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    290,
                    2203,
                    1210,
                    2431
                ],
                "angle": 0,
                "content": "Hongjin Qian, Zhicheng Dou, Jiejun Tan, Haonan Chen, Haoqi Gu, Ruofei Lai, Xinyu Zhang, Zhao Cao, and Ji-Rong Wen. 2023. Optimizing factual accuracy in text generation through dynamic knowledge selection."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    290,
                    2466,
                    1210,
                    2746
                ],
                "angle": 0,
                "content": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    290,
                    2778,
                    1210,
                    3055
                ],
                "angle": 0,
                "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    290,
                    3087,
                    1210,
                    3227
                ],
                "angle": 0,
                "content": "Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David"
            },
            {
                "block_id": 11,
                "type": "list",
                "bbox": [
                    290,
                    301,
                    1215,
                    3227
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1314,
                    301,
                    2188,
                    438
                ],
                "angle": 0,
                "content": "Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1270,
                    491,
                    2188,
                    670
                ],
                "angle": 0,
                "content": "Revanth Gangi Reddy, Yi R Fung, Qi Zeng, Manling Li, Ziqi Wang, Paul Sullivan, et al. 2023. Smartbook: AI-assisted situation report generation. arXiv preprint arXiv:2303.14337."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1270,
                    722,
                    2188,
                    862
                ],
                "angle": 0,
                "content": "Michele Salvagno, Fabio Silvio Taccone, Alberto Giovanni Gerli, et al. 2023. Can artificial intelligence help for scientific writing? Critical care, 27(1):1-5."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1270,
                    908,
                    2188,
                    1231
                ],
                "angle": 0,
                "content": "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624-643, Online. Association for Computational Linguistics."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1276,
                    2188,
                    1466
                ],
                "angle": 0,
                "content": "Özge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko, and Chris Biemann. 2022. Neural entity linking: A survey of models based on deep learning. Semantic Web, (Preprint):1-44."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1511,
                    2188,
                    1697
                ],
                "angle": 0,
                "content": "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1270,
                    1746,
                    2188,
                    2066
                ],
                "angle": 0,
                "content": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 3(6):7."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2115,
                    2188,
                    2532
                ],
                "angle": 0,
                "content": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERIFICATION. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2578,
                    2188,
                    2855
                ],
                "angle": 0,
                "content": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models."
            },
            {
                "block_id": 21,
                "type": "ref_text",
                "bbox": [
                    1270,
                    2901,
                    2188,
                    3041
                ],
                "angle": 0,
                "content": "Denny Vrandecić and Markus Krötzsch. 2014. Wiki-data: a free collaborative knowledgebase. Communications of the ACM, 57(10):78-85."
            },
            {
                "block_id": 22,
                "type": "ref_text",
                "bbox": [
                    1270,
                    3087,
                    2188,
                    3227
                ],
                "angle": 0,
                "content": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference."
            },
            {
                "block_id": 23,
                "type": "list",
                "bbox": [
                    1270,
                    301,
                    2188,
                    3227
                ],
                "angle": 0,
                "content": null
            }
        ]
    },
    {
        "page_id": 11,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    301,
                    1210,
                    487
                ],
                "angle": 0,
                "content": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    519,
                    1210,
                    708
                ],
                "angle": 0,
                "content": "Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    740,
                    1210,
                    922
                ],
                "angle": 0,
                "content": "Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    961,
                    1210,
                    1143
                ],
                "angle": 0,
                "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019a. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1178,
                    1210,
                    1550
                ],
                "angle": 0,
                "content": "Yuan Zhang, Jason Baldridge, and Luheng He. 2019b. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308, Minneapolis, Minnesota. Association for Computational Linguistics."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1585,
                    1210,
                    1768
                ],
                "angle": 0,
                "content": "Guido Zucco and Bevan Koopman. 2023. Dr chatgpt, tell me what i want to hear: How prompt knowledge impacts health answer correctness. arXiv preprint arXiv:2302.13793."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1265,
                    294,
                    1818,
                    347
                ],
                "angle": 0,
                "content": "A Dataset Construction"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1260,
                    385,
                    2193,
                    663
                ],
                "angle": 0,
                "content": "In this section, we will explain the detailed process and algorithms for the automatic dataset construction pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1265,
                    712,
                    1714,
                    761
                ],
                "angle": 0,
                "content": "A.1 Person Selection"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1260,
                    785,
                    2195,
                    1915
                ],
                "angle": 0,
                "content": "To improve the complexity of the questions and difficulty to LLMs, we involve more than one person in each question. In addition, we need high quality paragraphs for subsequent dataset generation steps. Therefore, we utilize name pairs and paragraphs from the biographical database, which is a database specifically designed for the relation extraction (RE) task. Each piece of data from the biographical database includes a short paragraph, and a relation triple extracted from the paragraph. The relation triple consists of two people and their relationship such as <William Shakespeare, Spouse, Anne Hathaway>. The biographical database includes an automatically extracted set and a human annotated set. We specifically choose the human annotated set from the database to ensure high-quality name pairs. To avoid potential ambiguities, we filter out data if any name in the triple is incomplete. In practice, we consider a name complete if it has at least a family name and a surname."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1265,
                    1960,
                    1826,
                    2010
                ],
                "angle": 0,
                "content": "A.2 Name Disambiguation"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    2034,
                    2193,
                    3108
                ],
                "angle": 0,
                "content": "Due to the presence of duplicate names (e.g., Anne Hathaway: the actress, or the wife of William Shakespeare), we perform name disambiguation to map each name in the triple to a unique entity from the knowledge graph. We utilize WikiData\\(^4\\) (Vrandečić and Krötzsch, 2014) as the knowledge base and employ SPARQL (Pérez et al., 2009) queries to retrieve all entities associated with the name. WikiData assigns a unique QID to each entity which distinguishes between entities with the same name. In WikiData, each entity represents a node in the knowledge graph. Since each triple consists of two names and one relation, we select the two entities obtained from the query if they are connected to each other on WikiData. Additionally, the connecting edge should align with the relation specified in the triple. Subsequently, we extract the one-hop sub-graph centered around each person node, which provides properties related to the person,"
            },
            {
                "block_id": 12,
                "type": "page_footnote",
                "bbox": [
                    1262,
                    3136,
                    2017,
                    3230
                ],
                "angle": 0,
                "content": "4https://www.wikidata.org/wiki/Wikidata:Main_Page"
            }
        ]
    },
    {
        "page_id": 12,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    280,
                    298,
                    1210,
                    638
                ],
                "angle": 0,
                "content": "such as gender, birth date, occupation, and more. We convert ambiguous person names from previous steps to unique QID from WikiData. The extracted sub-graphs contain all knowledge from WikiData about the selected people. We call the extracted graphs \"knowledge pool\"."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    282,
                    677,
                    1081,
                    729
                ],
                "angle": 0,
                "content": "A.3 Evolutionary Question Generation"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    280,
                    747,
                    1215,
                    1873
                ],
                "angle": 0,
                "content": "We employ an \"evolutionary question generation\" approach inspired by WizardLM (Xu et al., 2023) and DKGen (Qian et al., 2023), where we gradually increase the set of knowledge required by injecting knowledge through iterations. In each iteration, LLMs extend the paragraph with one sentence by incorporating the additional knowledge. After the last iteration, LLMs propose two questions according to the extended paragraph, one is a general version, and the other is a specific version. The general question is more concise, and the specific question is more detailed. Both questions target the same set of knowledge. All injected knowledge form a \"minimum knowledge set\", which includes the least knowledge required to answer the proposed question (Table 1). We do not throw all knowledge to LLM at once to form a paragraph because extending the paragraph and knowledge set incrementally allows us to select the appropriate knowledge after each iteration."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    280,
                    1880,
                    1215,
                    3230
                ],
                "angle": 0,
                "content": "In the first iteration, LLMs annotate the original paragraph from Biographical Database with the knowledge from the \"knowledge pool\". For instance, the sentence \"Artemisia was born in Rome.\" is annotated with knowledge [Artemisia, place of birth, Rome]. In each subsequent iteration, we select a piece of appropriate knowledge according to the existing paragraph. A sentence with appropriate knowledge should have good specificity and coherence. Specificity refers to the significance of the knowledge, such that it is not too general or trivial. Coherence refers to the naturalness of the additional knowledge. The added knowledge should not deviate from the existing paragraph and should be coherent when reading. During knowledge selection, each piece of knowledge is assigned a score by adding the specificity score and coherence score. The specificity score measures the uniqueness of the knowledge. We discourage the system from selecting too frequent relation types like \"gender\" or \"date of birth\" which may be less informative. A less frequent relation tend to provide a knowledge specific to the person. Derived from IDF, we calculate the number of occurrences \\(Count_{r}\\) for"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    1260,
                    298,
                    2195,
                    971
                ],
                "angle": 0,
                "content": "each relation \\( r \\) in the dataset with size \\( N \\). The coherence score is calculated through perplexity. We convert each piece of knowledge to a simple sentence by applying a template. For instance, the knowledge [Artemisia, place of birth, Rome] is converted to \"Artemisia's place of birth is Rome\". There are three templates depending on the POS of the relation. We append each sentence to the original paragraph and calculate normalized inverse perplexity to obtain coherence score. The overall score is a weighted sum of specificity score and coherence score:"
            },
            {
                "block_id": 5,
                "type": "equation",
                "bbox": [
                    1322,
                    1013,
                    2076,
                    1143
                ],
                "angle": 0,
                "content": "\\[\n\\begin{array}{l} S c o r e _ {r} = \\alpha \\cdot \\log (2 \\cdot N / C o u n t _ {r}) \\\\ + (1 - \\alpha) \\cdot \\operatorname {s o f t m a x} \\left(1 / p e r p _ {r}\\right) \\\\ \\end{array}\n\\]"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1260,
                    1192,
                    2195,
                    1810
                ],
                "angle": 0,
                "content": "In each iteration, we leverage the \"text-davinci-003\" model for annotation or generation with incontext learning. We provide separate instructions and demonstrations for general and specific questions. The detailed prompt templates used is provided in the appendix D. We provide one human written demonstration. Some examples of full question evolution process are provided in appendix E. In practice, we employ five iterations to ensure sufficient complexity in the questions without making them overly tedious."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    1262,
                    1862,
                    1773,
                    1918
                ],
                "angle": 0,
                "content": "B Experiment Details"
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1262,
                    1960,
                    1734,
                    2013
                ],
                "angle": 0,
                "content": "B.1 Main Experiment"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1260,
                    2038,
                    2193,
                    2711
                ],
                "angle": 0,
                "content": "For the main experiments, we run reach model with different seeds for three times. The OpenAI family models are implemented using OpenAI APIs. Running one round of experiment with ChatGPT model takes approximately 1 hour, and costs about 3 USD. Running one round of experiment with GPT4 model takes approximately 1.5 to 2 hours, and costs about 60 USD. Each LLaMA family model is run on one TESLA V100 GPU, where each run takes about 6 to 8 hours for Alpaca-7B and Vicuna-13B, and about 12-16 hours for LLaMA-7B and LLaMA-13B."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1262,
                    2760,
                    1853,
                    2813
                ],
                "angle": 0,
                "content": "B.2 Text Quality Evaluation"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    2837,
                    2195,
                    3234
                ],
                "angle": 0,
                "content": "For text quality evaluation, we use the model \"text-davinci-003\" with temperature 0 to ensure stability and reproducibility of the results. We randomly sample 100 outputs from each baseline and take three runs to report mean. We do not report standard deviation since most of them are mostly insignificantly small (below 0.1)."
            }
        ]
    },
    {
        "page_id": 13,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    282,
                    298,
                    491,
                    347
                ],
                "angle": 0,
                "content": "B.3 NLI"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    277,
                    371,
                    1223,
                    1220
                ],
                "angle": 0,
                "content": "For the automatic evaluation of text citation alignment and evaluation of the known unknown citations, we implement the TRUE model from HuggingFace\\(^5\\), which was trained on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), Fever (Thorne et al., 2018), Scitail (Khot et al., 2018), PAWS (Zhang et al., 2019b), and VitaminC (Schuster et al., 2021). The model uses the prompt of “premise: {PREMISE} hypothesis: {HYPOTHESIS}\\(’\\)). For each sentence citation pair, we place the sentence in the “PREMISE”, and the citation to the “HYPOTHESIS”, like the following: “premise: {Hertwig served as a professor at the University of Jena for the last 40 years of his career.} hypothesis: {employer: University of Jena}\\(’\\)"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    282,
                    1255,
                    788,
                    1308
                ],
                "angle": 0,
                "content": "C Human Evaluation"
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    282,
                    1343,
                    781,
                    1392
                ],
                "angle": 0,
                "content": "C.1 Dataset Evaluation"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    1413,
                    1215,
                    1641
                ],
                "angle": 0,
                "content": "To evaluate the dataset quality, we have two individual annotators who are proficient in the English language. Below are the exact method for evaluating each metric:"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    337,
                    1666,
                    1213,
                    2059
                ],
                "angle": 0,
                "content": "- Authenticity. We ask the annotators to check from Wikipedia and understand the background stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    337,
                    2090,
                    1215,
                    2602
                ],
                "angle": 0,
                "content": "- Relevance. After understanding the background stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is labeled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redundant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    337,
                    2634,
                    1215,
                    2974
                ],
                "angle": 0,
                "content": "- Naturalness. We ask the annotators to give an integer score 1 to 5 to label each question. 5 means the question can be easily understandable, and is concise. 1 means the question is not written in natural English language or is extremely tedious."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    337,
                    3006,
                    1215,
                    3118
                ],
                "angle": 0,
                "content": "- Significance. We ask the annotators to give an integer score 1 to 5 to label each question."
            },
            {
                "block_id": 9,
                "type": "list",
                "bbox": [
                    337,
                    1666,
                    1215,
                    3118
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1352,
                    298,
                    2188,
                    466
                ],
                "angle": 0,
                "content": "5 means the annotator feels that he or she may be interested in this question under some circumstances, and 1 means the opposite."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1260,
                    547,
                    2195,
                    943
                ],
                "angle": 0,
                "content": "The agreement between the two annotators are as follow: the agreement between them is \\(100\\%\\) for authenticity and \\(86\\%\\) for relevance. Since the evaluation for naturalness and significance are score based, in \\(92\\%\\) and \\(90\\%\\) of the evaluated datasets respectively, the score difference between the two annotators is no larger than 1."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1262,
                    1017,
                    1912,
                    1069
                ],
                "angle": 0,
                "content": "C.2 Generated Text Evaluation"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1260,
                    1112,
                    2195,
                    2013
                ],
                "angle": 0,
                "content": "Among text quality evaluation, citation quality evaluation, and text-citation alignment, we conduct human evaluation on text-citation alignment. Text quality evaluation is conducted using G-Eval. We acknowledge this is not a perfect metric, but the human evaluation is conducted in (Liu et al., 2023b). The focus is this paper is not to improve G-Eval. Citation quality evaluation is conducted with looking for exact match between generated citations and minimum knowledge set, which is an objective evaluation. The text-citation alignment evaluation is conducted using NLI, which we are not certain if entailment means providing support. In addition, whether a knowledge supports a sentence can be subjective. Therefore, we conduct human evaluation on alignment."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1260,
                    2024,
                    2200,
                    2648
                ],
                "angle": 0,
                "content": "We present the Human Evaluation Instructions provided to the annotators in Table 8. We follow the implementation from (Clark et al., 2021), and provide detailed instructions and examples to improve evaluation accuracy. For this human evaluation, there are four individual annotators in total. We arrange different annotators for different baselines, and each baseline has two annotators. The Inter-Annotator Agreement for ChatGPT, LLaMA-7B, and Vicuna-13B are reported as follows: \\(90\\%\\), \\(97\\%\\), and \\(89\\%\\) respectively."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1262,
                    2722,
                    1550,
                    2781
                ],
                "angle": 0,
                "content": "D Prompts"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1260,
                    2837,
                    2195,
                    3234
                ],
                "angle": 0,
                "content": "We present the prompts and instructions we used in this section. We present the prompts for the evolutionary question construction in Table 9, 10, 11, and 12. We present the prompt for the answer generation in Table 13. We present the prompts we use for text evaluation with G-Eval in Table 14, 15, 16, and 17."
            },
            {
                "block_id": 17,
                "type": "page_footnote",
                "bbox": [
                    282,
                    3136,
                    1198,
                    3234
                ],
                "angle": 0,
                "content": "5https://huggingface.co/google/t5_xx1_true_nli_mixture"
            }
        ]
    },
    {
        "page_id": 14,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    317,
                    294,
                    726,
                    333
                ],
                "angle": 0,
                "content": "Annotation Method:"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    315,
                    375,
                    1171,
                    919
                ],
                "angle": 0,
                "content": "Each evaluation content includes a sentence and a piece of knowledge. Our task is to determine whether this sentence contains the given knowledge, i.e., whether this knowledge provides support for the sentence. If the sentence does not mention the given knowledge or if the content of the sentence does not align with the knowledge, it is considered unsupported. We use 1 to indicate support and 0 to indicate lack of support."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    317,
                    957,
                    838,
                    999
                ],
                "angle": 0,
                "content": "Here are some examples:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    315,
                    1038,
                    1156,
                    1413
                ],
                "angle": 0,
                "content": "Sentence: Stephen Crane was an American writer born on November 1, 1871, in Newark, and died on June 5, 1900, in Badenweiler.  \nKnowledge: date of birth: 1871-11-01  \nResult: 1, because the sentence's date of birth matches the knowledge's date of birth."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    315,
                    1455,
                    1171,
                    1701
                ],
                "angle": 0,
                "content": "Sentence: Merton died on December 10, 1968, in Bangkok, Thailand.  \nKnowledge: country of citizenship: United States of America  \nResult: 0, because the sentence does not mention Merton's nationality."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1708,
                    1205,
                    1754
                ],
                "angle": 0,
                "content": "Table 8: Instruction we provide to the human annotators."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    285,
                    1841,
                    1118,
                    1901
                ],
                "angle": 0,
                "content": "E Evolutionary Question Generation"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    282,
                    1932,
                    1205,
                    2045
                ],
                "angle": 0,
                "content": "We provide an example of evolutionary question generation in Table 18."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    285,
                    2087,
                    590,
                    2146
                ],
                "angle": 0,
                "content": "F Examples"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    282,
                    2174,
                    1215,
                    2287
                ],
                "angle": 0,
                "content": "We show examples of the attributed answers generated by the LLMs in Table 19 and 20."
            }
        ]
    },
    {
        "page_id": 15,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    312,
                    554,
                    2049,
                    666
                ],
                "angle": 0,
                "content": "Instruction: Your objective is to select relevant knowledge to label the sentence and generate a question"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    310,
                    722,
                    2160,
                    996
                ],
                "angle": 0,
                "content": "sentence: Artemisia Gentileschi was born Artemisia Gentileschi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentileschi and Prudenzia di Ottaviano Montoni."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    310,
                    1003,
                    2158,
                    2241
                ],
                "angle": 0,
                "content": "knowledge: {qid: Q367360, name: Orazio Gentileschi, sex or gender: male, place of birth: Pisa, place of death: London, instance of: human, occupation: painter, child: Artemisia Gentileschi, described by source: The Great Theatre of Dutch Painters, notable works: Diana the Huntress, given name: Orazio, topic's main category: Category:Orazio Gentileschi, surname: Gentileschi, genre: portrait, languages spoken: Italian, movement: mannerism, work location: Rome, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07} {qid: Q212657, name: Artemisia Gentileschi, sex or gender: female, place of birth: Rome, place of death: Naples, instance of: human, occupation: painter, member of: Accademia delle Arti del Disegno, father: Orazio Gentileschi, described by source: The Great Theatre of Dutch Painters, notable works: Judith Slaying Holofernes, topic's main category: Category:Artemisia Gentileschi, movement: Caravaggisti, ethnic group: Italians, work location: Florence, depicted by: Artemisia, field of work: painting, surname: Gentileschi, genre: portrait, languages spoken: Italian, position held: court painter, student of: Orazio Gentileschi, spouse: Pierantonio Stiattesi, given name: Artemisia, mother: Prudenzia di Ottaviano Montoni, date of birth: 1596-07-08, date of death: 1654-01-01}"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    310,
                    2297,
                    2160,
                    2802
                ],
                "angle": 0,
                "content": "Generated Answer: Artemisia Gentileschi [qid: Q212657, name: Artemisia Gentileschi] was born Artemisia Gentileschi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentileschi [qid: Q212657, father: Orazio Gentileschi] [qid: Q367360, name: Orazio Gentileschi, occupation: painter] and Prudenzia di Ottaviano Montoni."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    282,
                    2859,
                    2190,
                    2964
                ],
                "angle": 0,
                "content": "Table 9: Instruction and demonstration for initial round of evolutionary question construction. We use brown color for the knowledge, and blue color for the demonstration of expected output."
            }
        ]
    },
    {
        "page_id": 16,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    312,
                    971,
                    2158,
                    1083
                ],
                "angle": 0,
                "content": "Instruction: Your objective is to extend the original paragraph by adding one sentence that includes the given knowledge"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    312,
                    1140,
                    2158,
                    1701
                ],
                "angle": 0,
                "content": "answer: Artemisia Gentileschi [qid: Q212657, name: Artemisia Gentileschi] was born Artemisia Gentileschi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentileschi [qid: Q212657, father: Orazio Gentileschi] [qid: Q367360, name: Orazio Gentileschi, occupation: painter] and Prudenzia di Ottaviano Montoni. knowledge: {qid: Q212657, name: Artemisia Gentileschi, notable works: Judith Slaying Holofernes}"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    312,
                    1761,
                    2158,
                    2431
                ],
                "angle": 0,
                "content": "Generated Answer: Artemisia Gentileschi [qid: Q212657, name: Artemisia Gentileschi] was born Artemisia Gentileschi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentileschi [qid: Q212657, father: Orazio Gentileschi] [qid: Q367360, name: Orazio Gentileschi, occupation: painter] and Prudenzia di Ottaviano Montoni. Under the influence of her father, Artemisia Gentileschi created her iconic painting Judith Slaying Holofernes [qid: Q212657, notable works: Judith Slaying Holofernes] when she was around twenty years old."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    2438,
                    2188,
                    2543
                ],
                "angle": 0,
                "content": "Table 10: Instruction and demonstration for evolutionary question construction for round 2 to 5. We use brown color for the knowledge, and blue color for the demonstration of expected output."
            }
        ]
    },
    {
        "page_id": 17,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    310,
                    333,
                    2151,
                    501
                ],
                "angle": 0,
                "content": "Instruction: Your objective is to ask a question whose answer is the given paragraph. There should be only one question when possible, if not, make sure the question is as concise as possible."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    310,
                    557,
                    2160,
                    1403
                ],
                "angle": 0,
                "content": "Paragraph: Artemisia Gentileschi was born Artemisia Gentileschi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentileschi and Prudenzia di Ottaviano Montoni. Her life and work were later depicted in the film Artemisia, which brought her story to a wider audience. Her father, Orazio, was a prominent figure in the Mannerism art movement, which likely influenced Artemisia's own artistic style. However, Artemisia herself was a part of the Caravaggisti movement, a group of artists who followed the style of Caravaggio. She was also a student of her father, Orazio Gentileschi, which further shaped her artistic development. Orazio's influence on Artemisia's development as a prominent Baroque painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    310,
                    1462,
                    1935,
                    1571
                ],
                "angle": 0,
                "content": "Generated Question: Who was Artemisia Gentileschi and what influences shaped her artistic style?"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    282,
                    1575,
                    2193,
                    1673
                ],
                "angle": 0,
                "content": "Table 11: Instruction and demonstration for general question generation. We use blue color for the demonstration of expected output."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    310,
                    1782,
                    2106,
                    1894
                ],
                "angle": 0,
                "content": "Instruction: Your objective is to ask a question whose answer is the given paragraph. The question should not be too tedious."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    310,
                    1950,
                    2160,
                    2799
                ],
                "angle": 0,
                "content": "Paragraph: Artemisia Gentileschi was born Artemisia Gentileschi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentileschi and Prudenzia di Ottaviano Montoni. Her life and work were later depicted in the film Artemisia, which brought her story to a wider audience. Her father, Orazio, was a prominent figure in the Mannerism art movement, which likely influenced Artemisia's own artistic style. However, Artemisia herself was a part of the Caravaggisti movement, a group of artists who followed the style of Caravaggio. She was also a student of her father, Orazio Gentileschi, which further shaped her artistic development. Orazio's influence on Artemisia's development as a prominent Baroque painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    310,
                    2855,
                    2128,
                    3076
                ],
                "angle": 0,
                "content": "Generated Question: What were the key artistic influences and characteristics that shaped Artemisia Gentileschi's unique Baroque style, and how did her relationship with her father, Orazio Gentileschi, impact her artistic development?"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    282,
                    3080,
                    2193,
                    3181
                ],
                "angle": 0,
                "content": "Table 12: Instruction and demonstration for specific question generation. We use blue color for the demonstration of expected output."
            }
        ]
    },
    {
        "page_id": 18,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    310,
                    526,
                    2158,
                    694
                ],
                "angle": 0,
                "content": "Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    312,
                    754,
                    1384,
                    799
                ],
                "angle": 0,
                "content": "Question: Considering the information:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    310,
                    806,
                    2096,
                    1806
                ],
                "angle": 0,
                "content": "{name: Orazio Gentileschi, place of death: London, child: Artemisia Gentileschi, notable works: Diana the Huntress, given name: Orazio, surname: Gentileschi, languages spoken: Italian, movement: mannerism, work location: Rome, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07, qid: Q367360} {name: Artemisia Gentileschi, place of birth: Rome, place of death: Naples, occupation: painter, member of: Accademia delle Arti del Disegno, father: Orazio Gentileschi, notable works: Judith Slaying Holofernes, movement: Caravaggisti, ethnic group: Italians, work location: Florence, depicted by: Artemisia, surname: Gentileschi, languages spoken: Italian, student of: Orazio Gentileschi, given name: Artemisia, mother: Prudenzia di Ottaviano Montoni, date of death: 1654-01-01, qid: Q212657} How did Orazio Gentileschi's influence on Artemisia's life and career shape her development as a prominent Baroque painter, despite facing significant challenges as a female artist in a male-dominated field?"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    310,
                    1869,
                    2155,
                    2880
                ],
                "angle": 0,
                "content": "Answer: Artemisia Gentileschi was an Italian painter born on July 8, 1596 [NA] in Rome [Q212657, ethnic group: Italians, occupation: painter, place of birth: Rome]. She was a member of the Accademia delle Arti del Disegno and is best known for her work Judith Slaying Holofernes [Q212657, member of: Accademia delle Arti del Disegno, notable works: Judith Slaying Holofernes]. She was the eldest child of the Tuscan painter Orazio Gentileschi [Q212657, father: Orazio Gentileschi]. Orazio Gentileschi was an Italian painter [NA] born in 1563 and died in 1639 [Q367360, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07]. He was born in Pisa and died in London [Q367360, place of death: London]. Orazio's influence on Artemisia's development as a prominent Baroque [NA] painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique [NA]. He also provided her with the opportunity to study with him and learn from his experience and expertise. She became an important second-generation proponent of Caravaggio's dramatic realism [Q212657, movement: Caravaggisti]."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    280,
                    2887,
                    2188,
                    2985
                ],
                "angle": 0,
                "content": "Table 13: Full instruction and demonstration for answer generation with cetaion. We use brown color for the knowledge pool, and blue color for the expected outcome provided by the demonstration."
            }
        ]
    },
    {
        "page_id": 19,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    310,
                    470,
                    2160,
                    694
                ],
                "angle": 0,
                "content": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    310,
                    754,
                    868,
                    799
                ],
                "angle": 0,
                "content": "Evaluation Criteria:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    307,
                    806,
                    2155,
                    1143
                ],
                "angle": 0,
                "content": "Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby the answer should be well-structured and well-organized. The answer should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    310,
                    1206,
                    783,
                    1255
                ],
                "angle": 0,
                "content": "Evaluation Steps:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    310,
                    1259,
                    2071,
                    1368
                ],
                "angle": 0,
                "content": "1. Read the questions carefully and identify the main topic and key points."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    310,
                    1375,
                    2151,
                    1532
                ],
                "angle": 0,
                "content": "2. Read the answer and compare it to the question. Check if the answer covers the main topic and key points of the question, and if it presents them in a clear and logical order."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    310,
                    1543,
                    2096,
                    1648
                ],
                "angle": 0,
                "content": "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria."
            },
            {
                "block_id": 7,
                "type": "list",
                "bbox": [
                    310,
                    1259,
                    2151,
                    1648
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "table_caption",
                "bbox": [
                    645,
                    1655,
                    1823,
                    1701
                ],
                "angle": 0,
                "content": "Table 14: Instruction for text evaluation with GPT-EVAL - Coherence"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    310,
                    2090,
                    2160,
                    2315
                ],
                "angle": 0,
                "content": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    310,
                    2374,
                    863,
                    2417
                ],
                "angle": 0,
                "content": "Evaluation Criteria:"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    307,
                    2431,
                    2123,
                    2592
                ],
                "angle": 0,
                "content": "Consistency (1-5) - the answer should be consistent with the given knowledge. The answer should also be self-consistent, without any contradiction to itself."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    310,
                    2655,
                    783,
                    2704
                ],
                "angle": 0,
                "content": "Evaluation Steps:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    310,
                    2711,
                    1570,
                    2760
                ],
                "angle": 0,
                "content": "1. Read the question and knowledge carefully."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    310,
                    2767,
                    2121,
                    2873
                ],
                "angle": 0,
                "content": "2. Read the answer and compare it to the knowledge. Check if the answer is consistent with the give knowledge."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    310,
                    2883,
                    2146,
                    2985
                ],
                "angle": 0,
                "content": "3. Assign a score for consistency on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria."
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    310,
                    2711,
                    2146,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "table_caption",
                "bbox": [
                    635,
                    2992,
                    1835,
                    3041
                ],
                "angle": 0,
                "content": "Table 15: Instruction for text evaluation with GPT-EVAL - Consistency"
            }
        ]
    },
    {
        "page_id": 20,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    310,
                    526,
                    2160,
                    750
                ],
                "angle": 0,
                "content": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    310,
                    810,
                    868,
                    855
                ],
                "angle": 0,
                "content": "Evaluation Criteria:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    310,
                    862,
                    2151,
                    1087
                ],
                "angle": 0,
                "content": "Fluency (1- 5) - the answer should be written in fluent language. The answer should use appropriate vocabulary, grammar, and sentence structures that enable readers or listeners to comprehend the content effortlessly."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    312,
                    1150,
                    783,
                    1199
                ],
                "angle": 0,
                "content": "Evaluation Steps:"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    312,
                    1206,
                    1193,
                    1255
                ],
                "angle": 0,
                "content": "1. Read the question carefully."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    312,
                    1259,
                    2014,
                    1364
                ],
                "angle": 0,
                "content": "2. Read the answer and check if the language in the answer is fluent."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    312,
                    1371,
                    2148,
                    1480
                ],
                "angle": 0,
                "content": "3. Assign a score for fluency on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria."
            },
            {
                "block_id": 7,
                "type": "list",
                "bbox": [
                    312,
                    1206,
                    2148,
                    1480
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "table_caption",
                "bbox": [
                    667,
                    1487,
                    1801,
                    1536
                ],
                "angle": 0,
                "content": "Table 16: Instruction for text evaluation with GPT-EVAL - Fluency"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    310,
                    2034,
                    2160,
                    2259
                ],
                "angle": 0,
                "content": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    312,
                    2318,
                    868,
                    2364
                ],
                "angle": 0,
                "content": "Evaluation Criteria:"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    310,
                    2371,
                    2123,
                    2536
                ],
                "angle": 0,
                "content": "Relevance (1- 5) - the answer should be relevant to the question. The answer should directly answers the question, without providing any irrelevant information."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    312,
                    2599,
                    783,
                    2648
                ],
                "angle": 0,
                "content": "Evaluation Steps:"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    312,
                    2655,
                    1193,
                    2704
                ],
                "angle": 0,
                "content": "1. Read the question carefully."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    312,
                    2711,
                    2069,
                    2816
                ],
                "angle": 0,
                "content": "2. Read the answer and compare with the question to check if it fully answers the question and have no redundancies."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    312,
                    2823,
                    2096,
                    2929
                ],
                "angle": 0,
                "content": "3. Assign a score for relevance on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria."
            },
            {
                "block_id": 16,
                "type": "list",
                "bbox": [
                    312,
                    2655,
                    2096,
                    2929
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "table_caption",
                "bbox": [
                    652,
                    2936,
                    1818,
                    2981
                ],
                "angle": 0,
                "content": "Table 17: Instruction for text evaluation with GPT-EVAL - Relevance"
            }
        ]
    },
    {
        "page_id": 21,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    317,
                    666,
                    498,
                    701
                ],
                "angle": 0,
                "content": "Round 1:"
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    317,
                    708,
                    766,
                    747
                ],
                "angle": 0,
                "content": "Annotated Knowledge:"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    320,
                    743,
                    2089,
                    782
                ],
                "angle": 0,
                "content": "[qid: Q258115, name: Diego Simeone, occupation: association football player]"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    322,
                    782,
                    1776,
                    820
                ],
                "angle": 0,
                "content": "[qid: Q6439494, name: Giovanni Simeone, father: Diego Simeone]"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    322,
                    824,
                    1798,
                    862
                ],
                "angle": 0,
                "content": "[qid: Q6439494, name: Giovanni Simeone, place of birth: Madrid]"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    322,
                    866,
                    2066,
                    905
                ],
                "angle": 0,
                "content": "[qid: Q258115, name: Diego Simeone, member of sports team: Atletico Madrid]"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    320,
                    908,
                    2071,
                    1031
                ],
                "angle": 0,
                "content": "Paragraph: The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing for Atletico Madrid."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    317,
                    1076,
                    498,
                    1112
                ],
                "angle": 0,
                "content": "Round 2:"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    317,
                    1115,
                    2111,
                    1189
                ],
                "angle": 0,
                "content": "Input Knowledge: [qid: Q258115, name: Diego Simeone, position played on team: midfielder]"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    317,
                    1199,
                    2074,
                    1322
                ],
                "angle": 0,
                "content": "Paragraph: The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    317,
                    1368,
                    498,
                    1403
                ],
                "angle": 0,
                "content": "Round 3:"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    317,
                    1406,
                    2044,
                    1445
                ],
                "angle": 0,
                "content": "Input Knowledge: [qid: Q258115, Diego Simeone, member of sports team: S.S."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    320,
                    1448,
                    451,
                    1483
                ],
                "angle": 0,
                "content": "Lazio"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    317,
                    1490,
                    2155,
                    1655
                ],
                "angle": 0,
                "content": "Paragraph: The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio."
            },
            {
                "block_id": 14,
                "type": "title",
                "bbox": [
                    317,
                    1701,
                    498,
                    1739
                ],
                "angle": 0,
                "content": "Round 4:"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    317,
                    1743,
                    2066,
                    1782
                ],
                "angle": 0,
                "content": "Input Knowledge: [qid: Q258115, Diego Simeone, medical condition: COVID-19]"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    317,
                    1785,
                    2155,
                    1992
                ],
                "angle": 0,
                "content": "Paragraph: The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone contracted the COVID-19 virus, but managed to recover."
            },
            {
                "block_id": 17,
                "type": "title",
                "bbox": [
                    317,
                    2034,
                    498,
                    2073
                ],
                "angle": 0,
                "content": "Round 5:"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    317,
                    2076,
                    2044,
                    2157
                ],
                "angle": 0,
                "content": "Input Knowledge: [qid: Q6439494, Giovanni Simeone, position played on team: forward]"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    317,
                    2160,
                    2155,
                    2406
                ],
                "angle": 0,
                "content": "Paragraph: The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone contracted the COVID-19 virus, but managed to recover. Following in his father's footsteps but with a different role on the pitch,"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    317,
                    2409,
                    1776,
                    2452
                ],
                "angle": 0,
                "content": "Giovanni Simeone grew up to be a forward in the game of football."
            },
            {
                "block_id": 21,
                "type": "title",
                "bbox": [
                    317,
                    2494,
                    699,
                    2529
                ],
                "angle": 0,
                "content": "General Question:"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    317,
                    2529,
                    1959,
                    2567
                ],
                "angle": 0,
                "content": "How did the career of Diego Simeone influence his son's path in football?"
            },
            {
                "block_id": 23,
                "type": "title",
                "bbox": [
                    317,
                    2578,
                    719,
                    2616
                ],
                "angle": 0,
                "content": "Specific Question:"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    317,
                    2616,
                    2026,
                    2690
                ],
                "angle": 0,
                "content": "Who are Diego Simeone and Giovanni Simeone, and how are there career path in football different from each other?"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    290,
                    2697,
                    2188,
                    2844
                ],
                "angle": 0,
                "content": "Table 18: An example of the generated questions after each round in evolutionary question construction. We use brown color for the knowledge, and blue color for the generated questions. In each round we underline the texts to the newly injected knowledge."
            }
        ]
    },
    {
        "page_id": 22,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    312,
                    477,
                    565,
                    522
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    310,
                    526,
                    2151,
                    799
                ],
                "angle": 0,
                "content": "What were some of Stephen Crane's notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University?"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    312,
                    862,
                    863,
                    912
                ],
                "angle": 0,
                "content": "Retrieved Knowledge:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    310,
                    915,
                    2153,
                    1697
                ],
                "angle": 0,
                "content": "[qid: Q206534, name: Stephen Crane, sex or gender: male, place of birth: Newark, place of death: Badenweiler, country of citizenship: United States of America, cause of death: Tuberculosis, place of burial: Evergreen Cemetery, alma mater: Syracuse University, instance of: human, given name: Stephen, medical condition: Tuberculosis, occupation: writer, described by source: Great Soviet Encyclopedia (1969a€\"1978), manner of death: natural causes, movement: literary realism, notable works: The Red Badge of Courage, surname: Crane', \"topic's main category\": 'Category: Stephen Crane, religion: atheism, languages spoken: English, mother: Mary Helen Peck Crane, sport: baseball, member of sports team: Syracuse Orange baseball, father: Jonathan Townley Crane, name: Stephen Crane, date of birth: 1871-11-01, date of death: 1900-06-05]"
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    312,
                    1761,
                    783,
                    1810
                ],
                "angle": 0,
                "content": "Generated Output :"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    310,
                    1813,
                    2123,
                    2487
                ],
                "angle": 0,
                "content": "Stephen Crane was a male writer born on November 1, 1871, in Newark, United States of America, and died on June 5, 1900, in Badenweiler due to tuberculosis [Q206534, sex or gender: male, place of birth: Newark, place of death: Badenweiler, country of citizenship: United States of America, cause of death: Tuberculosis, medical condition: Tuberculosis, date of birth: 1871-11-01, date of death: 1900-06-05]. Crane was a prominent literary realist writer [Q206534, movement: literary realism] who wrote about the harsh realities of life and his atheistic views on religion [Q206534, religion: atheism]. His notable works include \"The Red Badge of Courage\" [Q206534, notable works: The Red Badge of Courage]."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    310,
                    2546,
                    2153,
                    2936
                ],
                "angle": 0,
                "content": "Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including \"The Eighty-Yard Run,\" which was based on his own experiences as a football player at Syracuse [NA]."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    280,
                    2939,
                    2188,
                    3041
                ],
                "angle": 0,
                "content": "Table 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge."
            }
        ]
    },
    {
        "page_id": 23,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    310,
                    505,
                    565,
                    550
                ],
                "angle": 0,
                "content": "Question:"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    307,
                    554,
                    2151,
                    831
                ],
                "angle": 0,
                "content": "What were some of Stephen Crane's notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University?"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    310,
                    891,
                    863,
                    943
                ],
                "angle": 0,
                "content": "Retrieved Knowledge:"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    307,
                    943,
                    2153,
                    1729
                ],
                "angle": 0,
                "content": "[qid: Q206534, name: Stephen Crane, sex or gender: male, place of birth: Newark, place of death: Badenweiler, country of citizenship: United States of America, cause of death: Tuberculosis, place of burial: Evergreen Cemetery, alma mater: Syracuse University, instance of: human, given name: Stephen, medical condition: Tuberculosis, occupation: writer, described by source: Great Soviet Encyclopedia (1969a€\"1978), manner of death: natural causes, movement: literary realism, notable works: The Red Badge of Courage, surname: Crane', \"topic's main category\": 'Category: Stephen Crane, religion: atheism, languages spoken: English, mother: Mary Helen Peck Crane, sport: baseball, member of sports team: Syracuse Orange baseball, father: Jonathan Townley Crane, name: Stephen Crane, date of birth: 1871-11-01, date of death: 1900-06-05]"
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    310,
                    1789,
                    783,
                    1838
                ],
                "angle": 0,
                "content": "Generated Output :"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    307,
                    1838,
                    2128,
                    2402
                ],
                "angle": 0,
                "content": "Stephen Crane was an American writer born on November 1, 1871, in Newark, and died on June 5, 1900, in Badenweiler [Q206534, date of birth: 1871-11-01, date of death: 1900-06-05, place of birth: Newark, place of death: Badenweiler]. He was a part of the literary realism movement and is best known for his notable work, The Red Badge of Courage [Q206534, movement: literary realism, notable works: The Red Badge of Courage]. Crane's views on religion were atheistic [Q206534, religion: atheism]. His works often reflected the harsh realities of life, such as war, poverty, and human suffering [NA]."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    307,
                    2459,
                    2131,
                    2687
                ],
                "angle": 0,
                "content": "Crane attended Syracuse University [Q206534, alma mater: Syracuse University], where his experiences likely influenced his writing. He was exposed to a diverse range of ideas and perspectives, which may have contributed to his realistic approach to literature [NA]."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    307,
                    2743,
                    2046,
                    2904
                ],
                "angle": 0,
                "content": "While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    280,
                    2911,
                    2188,
                    3013
                ],
                "angle": 0,
                "content": "Table 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge."
            }
        ]
    }
]