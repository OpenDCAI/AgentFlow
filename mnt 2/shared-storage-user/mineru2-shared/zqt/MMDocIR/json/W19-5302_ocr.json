[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    342,
                    263,
                    2146,
                    406
                ],
                "angle": 0,
                "content": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    466,
                    438,
                    1210,
                    620
                ],
                "angle": 0,
                "content": "Qingsong Ma  \nTencent-CSIG, AI Evaluation Lab  \nqingsong.mqs@gmail.com"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    483,
                    670,
                    1173,
                    852
                ],
                "angle": 0,
                "content": "Ondrej Bojar  \nCharles University, MFF UFAL  \nbojar@ufal.mff.cuni.cz"
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    640,
                    926,
                    870,
                    982
                ],
                "angle": 0,
                "content": "Abstract"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    354,
                    1062,
                    1146,
                    2210
                ],
                "angle": 0,
                "content": "This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less \"metrics\" and constitute submissions to the joint task with WMT19 Quality Estimation Task, \"QE as a Metric\". In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 implementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    290,
                    2294,
                    692,
                    2350
                ],
                "angle": 0,
                "content": "1 Introduction"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2402,
                    1218,
                    3195
                ],
                "angle": 0,
                "content": "To determine system performance in machine translation (MT), it is often more practical to use an automatic evaluation, rather than a manual one. Manual/human evaluation can be costly and time consuming, and so an automatic evaluation metric, given that it sufficiently correlates with manual evaluation, can be useful in developmental cycles. In studies involving hyperparameter tuning or architecture search, automatic metrics are necessary as the amount of human effort implicated in manual evaluation is generally prohibitively large. As objective, reproducible quantities, metrics can also facilitate cross-paper compar-"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1354,
                    438,
                    1962,
                    613
                ],
                "angle": 0,
                "content": "Johnny Tian-Zheng Wei  \nUMass Amherst, CICS  \njwei@umass.edu"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1305,
                    670,
                    2009,
                    845
                ],
                "angle": 0,
                "content": "Yvette Graham  \nDublin City University, ADAPT  \ngraham.yvette@gmail.com"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1262,
                    929,
                    2195,
                    1210
                ],
                "angle": 0,
                "content": "isons. The WMT Metrics Shared Task<sup>1</sup> annually serves as a venue to validate the use of existing metrics (including baselines such as BLEU), and to develop new ones; see Koehn and Monz (2006) through Ma et al. (2018)."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1262,
                    1213,
                    2195,
                    1655
                ],
                "angle": 0,
                "content": "In the setup of our Metrics Shared Task, an automatic metric compares an MT system's output translations with manual reference translations to produce: either (a) system-level score, i.e. a single overall score for the given MT system, or (b) segment-level scores for each of the output translations, or both."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1262,
                    1666,
                    2193,
                    2732
                ],
                "angle": 0,
                "content": "This year we teamed up with the organizers of the QE Task and hosted \"QE as a Metric\" as a joint task. In the setup of the Quality Estimation Task (Fonseca et al., 2019), no human-produced translations are provided to estimate the quality of output translations. Quality estimation (QE) methods are built to assess MT output based on the source or based on the translation itself. In this task, QE developers were invited to perform the same scoring as standard metrics participants, with the exception that they refrain from using a reference translation in production of their scores. We then evaluate the QE submissions in exactly the same way as regular metrics are evaluated, see below. From the point of view of correlation with manual judgements, there is no difference in metrics using or not using references."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1262,
                    2736,
                    2193,
                    3076
                ],
                "angle": 0,
                "content": "The source, reference texts, and MT system outputs for the Metrics task come from the News Translation Task (Barrault et al., 2019, which we denote as Findings 2019). The texts were drawn from the news domain and involve translations of English (en) to/from"
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    3097,
                    2153,
                    3185
                ],
                "angle": 0,
                "content": "http://www.statmt.org/wmt19/metrics-task.html"
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1270,
                    3279
                ],
                "angle": 0,
                "content": "62"
            },
            {
                "block_id": 15,
                "type": "footer",
                "bbox": [
                    305,
                    3301,
                    2170,
                    3399
                ],
                "angle": 0,
                "content": "Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 62-90 Florence, Italy, August 1-2, 2019. ©2019 Association for Computational Linguistics"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    282,
                    263,
                    1210,
                    768
                ],
                "angle": 0,
                "content": "Czech (cs), German (de), Finnish (fi), Gujarati (gu), Kazakh (kk), Lithuanian (lt), Russian (ru), and Chinese (zh), but excluding csen (15 language pairs). Three other language pairs not including English were also manually evaluated as part of the News Translation Task: German \\(\\rightarrow\\) Czech and German \\(\\leftrightarrow\\) French. In total, metrics could participate in 18 language pairs, with 10 target languages."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    775,
                    1215,
                    1164
                ],
                "angle": 0,
                "content": "In the following, we first give an overview of the task (Section 2) and summarize the baseline (Section 3) and submitted (Section 4) metrics. The results for system- and segment-level evaluation are provided in Sections 5.1 and 5.2, respectively, followed by a joint discussion Section 6."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    287,
                    1220,
                    654,
                    1280
                ],
                "angle": 0,
                "content": "2 Task Setup"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1311,
                    1213,
                    1704
                ],
                "angle": 0,
                "content": "This year, we provided task participants with one test set for each examined language pair, i.e. a set of source texts (which are commonly ignored by MT metrics), corresponding MT outputs (these are the key inputs to be scored) and a reference translation (held out for the participants of “QE as a Metric” track)."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1715,
                    1215,
                    2445
                ],
                "angle": 0,
                "content": "In the system-level, metrics aim to correlate with a system's score which is an average over many human judgments of segment translation quality produced by the given system. In the segment-level, metrics aim to produce scores that correlate best with a human ranking judgment of two output translations for a given source segment (more on the manual quality assessment in Section 2.3). Participants were free to choose which language pairs and tracks (system/segment and reference-based/reference-free) they wanted to take part in."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    287,
                    2490,
                    1051,
                    2543
                ],
                "angle": 0,
                "content": "2.1 Source and Reference Texts"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2567,
                    1213,
                    3016
                ],
                "angle": 0,
                "content": "The source and reference texts we use are newstest2019 from this year's WMT News Translation Task (see Findings 2019). This set contains approximately 2,000 sentences for each translation direction (except Gujarati, Kazakh and Lithuanian which have approximately 1,000 sentences each, and German to/from French which has 1701 sentences)."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    287,
                    3023,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "The reference translations provided in newstest2019 were created in the same direction as the MT systems were translating."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    263,
                    2195,
                    712
                ],
                "angle": 0,
                "content": "The exceptions are German \\(\\rightarrow\\) Czech where both sides are translations from English and German \\(\\leftrightarrow\\) French which followed last years' practice. Last year and the years before, the dataset consisted of two halves, one originating in the source language and one in the target language. This however lead to adverse artifacts in MT evaluation."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1267,
                    750,
                    1771,
                    803
                ],
                "angle": 0,
                "content": "2.2 System Outputs"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1262,
                    820,
                    2193,
                    1666
                ],
                "angle": 0,
                "content": "The results of the Metrics Task are affected by the actual set of MT systems participating in a given translation direction. On one hand, if all systems are very close in their translation quality, then even humans will struggle to rank them. This in turn will make the task for MT metrics very hard. On the other hand, if the task includes a wide range of systems of varying quality, correlating with humans should be generally easier, see Section 6.1 for a discussion on this. One can also expect that if the evaluated systems are of different types, they will exhibit different error patterns and various MT metrics can be differently sensitive to these patterns."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    1669,
                    2193,
                    1950
                ],
                "angle": 0,
                "content": "This year, all MT systems included in the Metrics Task come from the News Translation Task (see Findings 2019). There are however still noticeable differences among the various language pairs."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1312,
                    1989,
                    2195,
                    2725
                ],
                "angle": 0,
                "content": "- Unsupervised MT Systems. The German \\(\\rightarrow\\)Czech research systems were trained in an unsupervised fashion, i.e. without the access to parallel Czech-German texts (except for a couple of thousand sentences used primarily for validation). We thus expect the research German-Czech systems to be \"more creative\" and depart further away from the references. The online systems in this language directions are however standard MT systems so the German-Czech evaluation could be to some extent bimodal."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1312,
                    2760,
                    2200,
                    3097
                ],
                "angle": 0,
                "content": "- EU Election. The French \\(\\leftrightarrow\\) German translation was focused on a sub-domain of news, namely texts related EU Election. Various MT system developers may have invested more or less time to the domain adaptation."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1312,
                    3136,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "- Regular News Tasks Systems. These"
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    1312,
                    1989,
                    2200,
                    3192
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "63"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    374,
                    263,
                    1223,
                    663
                ],
                "angle": 0,
                "content": "are all the other MT systems in the evaluation; differing in whether they are trained only on WMT provided data (\"Constrained\", or \"Unconstrained\") as in the previous years. All the freely available web services (online MT systems) are deemed unconstrained."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    287,
                    708,
                    1215,
                    820
                ],
                "angle": 0,
                "content": "Overall, the results are based on 233 systems across 18 language pairs.\\(^{2}\\)"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    287,
                    862,
                    1049,
                    915
                ],
                "angle": 0,
                "content": "2.3 Manual Quality Assessment"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    936,
                    1215,
                    1213
                ],
                "angle": 0,
                "content": "Direct Assessment (DA, Graham et al., 2013, 2014a, 2016) was employed as the source of the \"golden truth\" to evaluate metrics again this year. The details of this method of human evaluation are provided in Findings 2019."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1220,
                    1218,
                    1501
                ],
                "angle": 0,
                "content": "The basis of DA is to collect a large number of quality assessments (a number on a scale of 1-100, i.e. effectively a continuous scale) for the outputs of all MT systems. These scores are then standardized per annotator."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1508,
                    1215,
                    1897
                ],
                "angle": 0,
                "content": "In the past years, the underlying manual scores were reference-based (human judges had access to the same reference translation as the MT quality metric). This year, the official WMT19 scores are reference-based (or \"monolingual\") for some language pairs and reference-free (or \"bilingual\") for others.3"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    1901,
                    1215,
                    2231
                ],
                "angle": 0,
                "content": "Due to these different types of golden truth collection, reference-based language pairs are in a closer match with the standard reference-based metrics, while the reference-free language pairs are better fit for the “QE as a metric” subtask."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2241,
                    1215,
                    2525
                ],
                "angle": 0,
                "content": "Note that system-level manual scores are different than those of the segment-level. Since for segment-level evaluation, collecting enough DA judgements for each segment is infeasible, so we resort to converting DA judgements to"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    266,
                    2195,
                    371
                ],
                "angle": 0,
                "content": "golden truth expressed as relative rankings, see Section 2.3.2."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    378,
                    2195,
                    663
                ],
                "angle": 0,
                "content": "The exact methods used to calculate correlations of participating metrics with the golden truth are described below, in the two sections for system-level evaluation (Section 5.1) and segment-level evaluation (Section 5.2)."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1265,
                    701,
                    2168,
                    754
                ],
                "angle": 0,
                "content": "2.3.1 System-level Golden Truth: DA"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    771,
                    2193,
                    1048
                ],
                "angle": 0,
                "content": "For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed segments for each MT system to produce a scalar rating for the system's performance."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    1055,
                    2193,
                    1445
                ],
                "angle": 0,
                "content": "The underlying set of assessed segments is different for each system. Thanks to the fact that the system-level DA score is an average over many judgments, mean scores are consistent and have been found to be reproducible (Graham et al., 2013). For more details see Findings 2019."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1267,
                    1487,
                    2093,
                    1592
                ],
                "angle": 0,
                "content": "2.3.2 Segment-level Golden Truth: daRR"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1265,
                    1613,
                    2193,
                    1943
                ],
                "angle": 0,
                "content": "Starting from Bojar et al. (2017), when WMT fully switched to DA, we had to come up with a solid golden standard for segment-level judgements. Standard DA scores are reliable only when averaged over sufficient number of judgments.[4]"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1265,
                    1957,
                    2193,
                    2518
                ],
                "angle": 0,
                "content": "Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as \"DARR\", to distinguish it clearly from the relative ranking (\"RR\") golden truth used in the past years.[5]"
            },
            {
                "block_id": 16,
                "type": "page_footnote",
                "bbox": [
                    285,
                    2553,
                    1215,
                    2725
                ],
                "angle": 0,
                "content": "2This year, we do not use the artificially constructed \"hybrid systems\" (Graham and Liu, 2016) because the confidence on the ranking of system-level metrics is sufficient even without hybrids."
            },
            {
                "block_id": 17,
                "type": "page_footnote",
                "bbox": [
                    285,
                    2729,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "<sup>3</sup>Specifically, the reference-based language pairs were those where the anticipated translation quality was lower or where the manual judgements were obtained with the help of anonymous crowdsourcing. Most of these cases were translations into English (fien, gu-en, kk-en, lt-en, ru-en and zh-en) and then the language pairs not involving English (de-cs, de-fr and fr-de). The reference-less (bilingual) evaluations were those where mainly MT researchers themselves were involved in the annotations: en-cs, en-de, en-fi, en-gu, en-kk, en-lt, en-ru, en-zh."
            },
            {
                "block_id": 18,
                "type": "list",
                "bbox": [
                    285,
                    2553,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 19,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    2553,
                    2195,
                    3020
                ],
                "angle": 0,
                "content": "<sup>4</sup>For segment-level evaluation, one would need to collect many manual evaluations of the exact same segment as produced by each MT system. Such a sampling would be however wasteful for the evaluation needed by WMT, so only some MT systems happen to be evaluated for a given input sentence. In principle, we would like to return to DA's standard segment-level evaluation in future, where a minimum of 15 human judgements of translation quality are collected per translation and combined to get highly accurate scores for translations, but this would increase annotation costs."
            },
            {
                "block_id": 20,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    3020,
                    2193,
                    3188
                ],
                "angle": 0,
                "content": "<sup>5</sup>Since the analogue rating scale employed by DA is marked at the 0-25-50-75-100 points, we use 25 points as the minimum required difference between two system scores to produce DARR judgements. Note that we"
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    1265,
                    2553,
                    2195,
                    3188
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "64"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    305,
                    256,
                    1208,
                    1504
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>DA&gt;1</td><td>Ave</td><td>DA pairs</td><td>DARR</td></tr><tr><td>de-en</td><td>2,000</td><td>16.0</td><td>239,220</td><td>85,365</td></tr><tr><td>fi-en</td><td>1,996</td><td>9.5</td><td>83,168</td><td>38,307</td></tr><tr><td>gu-en</td><td>1,016</td><td>11.0</td><td>55,880</td><td>31,139</td></tr><tr><td>kk-en</td><td>1,000</td><td>11.0</td><td>55,000</td><td>27,094</td></tr><tr><td>lt-en</td><td>1,000</td><td>11.0</td><td>55,000</td><td>21,862</td></tr><tr><td>ru-en</td><td>1,999</td><td>11.9</td><td>131,766</td><td>46,172</td></tr><tr><td>zh-en</td><td>2,000</td><td>10.1</td><td>95,174</td><td>31,070</td></tr><tr><td>en-cs</td><td>1,997</td><td>9.1</td><td>75,560</td><td>27,178</td></tr><tr><td>en-de</td><td>1,997</td><td>19.1</td><td>347,109</td><td>99,840</td></tr><tr><td>en-fi</td><td>1,997</td><td>8.1</td><td>59,129</td><td>31,820</td></tr><tr><td>en-gu</td><td>998</td><td>6.9</td><td>21,854</td><td>11,355</td></tr><tr><td>en-kk</td><td>998</td><td>9.0</td><td>37,032</td><td>18,172</td></tr><tr><td>en-lt</td><td>998</td><td>9.0</td><td>36,435</td><td>17,401</td></tr><tr><td>en-ru</td><td>1,997</td><td>8.7</td><td>69,503</td><td>24,334</td></tr><tr><td>en-zh</td><td>1,997</td><td>9.8</td><td>87,501</td><td>18,658</td></tr><tr><td>de-cs</td><td>1,997</td><td>8.5</td><td>65,039</td><td>35,793</td></tr><tr><td>de-fr</td><td>1,605</td><td>4.1</td><td>12,055</td><td>4,862</td></tr><tr><td>fr-de</td><td>1,224</td><td>3.0</td><td>4,258</td><td>1,369</td></tr><tr><td colspan=\"5\">newstest2019</td></tr></table>"
            },
            {
                "block_id": 1,
                "type": "table_footnote",
                "bbox": [
                    285,
                    1536,
                    1215,
                    2192
                ],
                "angle": 0,
                "content": "Table 1: Number of judgements for DA converted to DARR data; “DA>1” is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; “Ave” is the average number of translations with at least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “DARR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    2273,
                    1215,
                    2953
                ],
                "angle": 0,
                "content": "From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into DARR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conversion of scores in this way produced a large set of DARR judgements for all language pairs,"
            },
            {
                "block_id": 3,
                "type": "page_footnote",
                "bbox": [
                    285,
                    2974,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "rely on judgements collected from known-reliable volunteers and crowd-sourced workers who passed DA's quality control mechanism. Any inconsistency that could arise from reliance on DA judgements collected from low quality crowd-sourcing is thus prevented."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    1262,
                    263,
                    2195,
                    655
                ],
                "angle": 0,
                "content": "shown in Table 1 due to combinatorial advantage of extracting DARR judgements from all possible pairs of translations of the same source input. We see that only German-French and esp. French-German can suffer from insufficient number of these simulated pairwise comparisons."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1262,
                    659,
                    2195,
                    824
                ],
                "angle": 0,
                "content": "The DARR judgements serve as the golden standard for segment-level evaluation in WMT19."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1265,
                    866,
                    1771,
                    922
                ],
                "angle": 0,
                "content": "3 Baseline Metrics"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1262,
                    954,
                    2193,
                    1178
                ],
                "angle": 0,
                "content": "In addition to validating popular metrics, including baselines metrics serves as comparison and prevents \"loss of knowledge\" as mentioned by Bojar et al. (2016)."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1262,
                    1178,
                    2193,
                    1571
                ],
                "angle": 0,
                "content": "Moses scorer<sup>6</sup> is one of the MT evaluation tools that aggregated several useful metrics over the time. Since Macháček and Bojar (2013), we have been using Moses scorer to provide most of the baseline metrics and kept encouraging authors of well-performing MT metrics to include them in Moses scorer.<sup>7</sup>"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1312,
                    1575,
                    1890,
                    1631
                ],
                "angle": 0,
                "content": "The baselines we report are:"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1292,
                    1666,
                    2200,
                    2118
                ],
                "angle": 0,
                "content": "BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1292,
                    2157,
                    2195,
                    2606
                ],
                "angle": 0,
                "content": "TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1290,
                    2645,
                    2195,
                    2813
                ],
                "angle": 0,
                "content": "sentBLEU. The metric SENTBLEU is computed using the script sentence-bleu, a part of the Moses toolkit. It is a"
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    1267,
                    2841,
                    2185,
                    2929
                ],
                "angle": 0,
                "content": "\\(^{6}\\)https://github.com/moses-smt/mosesdecoder/ blob/master/mert/evaluator.cpp"
            },
            {
                "block_id": 14,
                "type": "page_footnote",
                "bbox": [
                    1270,
                    2929,
                    2190,
                    3055
                ],
                "angle": 0,
                "content": "<sup>7</sup>If you prefer standard BLEU, we recommend sacreBLEU (Post, 2018a), found at https://github.com/mjpost/sacreBLEU."
            },
            {
                "block_id": 15,
                "type": "page_footnote",
                "bbox": [
                    1324,
                    3055,
                    2096,
                    3101
                ],
                "angle": 0,
                "content": "<sup>8</sup>http://www.itl.nist.gov/iaid/mig/tools/"
            },
            {
                "block_id": 16,
                "type": "page_footnote",
                "bbox": [
                    1272,
                    3101,
                    2190,
                    3192
                ],
                "angle": 0,
                "content": "\\(^{9}\\)International tokenization is found to perform slightly better (Macháček and Bojar, 2013)."
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    1267,
                    2841,
                    2190,
                    3192
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "65"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    369,
                    259,
                    1803,
                    3178
                ],
                "angle": 270,
                "content": "<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Features</td><td rowspan=\"2\">Learned?</td><td colspan=\"2\">Scoring Level</td><td rowspan=\"2\">Citation/Participant</td><td rowspan=\"2\">Availability</td></tr><tr><td>Seg</td><td>Sys</td></tr><tr><td rowspan=\"11\">Baselines</td><td>SENTBLEU</td><td>n-grams</td><td></td><td>●</td><td>-</td><td></td><td>(mosesdecoder) mert/sentence-bleu</td></tr><tr><td>BLEU</td><td>n-grams</td><td></td><td>-</td><td>●</td><td>Papineni et al. (2002)</td><td>(mosesdecoder) scripts/generic/mteval-v13a.pl</td></tr><tr><td>NIST</td><td>n-grams</td><td></td><td>-</td><td>●</td><td>Doddington (2002)</td><td>(mosesdecoder) scripts/generic/mteval-v13a.pl</td></tr><tr><td>WER</td><td>Levenshtein distance</td><td></td><td>-</td><td>●</td><td>Leusch et al. (2006)</td><td>(mosesdecoder) mert/evaluator</td></tr><tr><td>TER</td><td>edit distance, edit types</td><td></td><td>-</td><td>●</td><td>Snover et al. (2006)</td><td>(mosesdecoder) mert/evaluator</td></tr><tr><td>PER</td><td>edit distance, edit types</td><td></td><td>-</td><td>●</td><td>Leusch et al. (2003)</td><td>(mosesdecoder) mert/evaluator</td></tr><tr><td>CDER</td><td>edit distance, edit types</td><td></td><td>-</td><td>●</td><td>Leusch et al. (2006)</td><td>(mosesdecoder) mert/evaluator</td></tr><tr><td>CHRF</td><td>character n-grams</td><td></td><td>●</td><td>∅</td><td>Popović (2015)</td><td>http://github.com/m-popovic/chrf</td></tr><tr><td>CHRF+</td><td>character n-grams</td><td></td><td>●</td><td>∅</td><td>Popović (2017)</td><td>http://github.com/m-popovic/chrf</td></tr><tr><td>SACREBLEU-BLEU</td><td>n-grams</td><td></td><td>-</td><td>●</td><td>Post (2018a)</td><td>http://github.com/mjpost/sacreBLEU</td></tr><tr><td>SACREBLEU-CHRF</td><td>n-grams</td><td></td><td>-</td><td>●</td><td>Post (2018a)</td><td>http://github.com/mjpost/sacreBLEU</td></tr><tr><td rowspan=\"14\">Metrics</td><td>BEER</td><td>char. n-grams, permutation trees</td><td>yes</td><td>●</td><td>∅</td><td>Univ. of Amsterdam, ILCC (Stanojević and Sima&#x27;an, 2015)</td><td>http://github.com/stanojevic/beer</td></tr><tr><td>BERTR</td><td>contextual word embeddings</td><td></td><td>●</td><td>∅</td><td>Univ. of Melbourne (Mathur et al., 2019)</td><td>http://github.com/nitikam/mteval-in-context</td></tr><tr><td>CHARACTER</td><td>char. edit distance, edit types</td><td></td><td>●</td><td>∅</td><td>RWTH Aachen Univ. (Wang et al., 2016a)</td><td>http://github.com/ruth-i6/CharACTER</td></tr><tr><td>EED</td><td>char. edit distance, edit types</td><td></td><td>●</td><td>∅</td><td>RWTH Aachen Univ. (Stanchev et al., 2019)</td><td>http://github.com/ruth-i6/ExtendedEditDistance</td></tr><tr><td>ESIM</td><td>learned neural representations</td><td>yes</td><td>●</td><td>∅</td><td>Univ. of Melbourne (Mathur et al., 2019)</td><td>http://github.com/nitikam/mteval-in-context</td></tr><tr><td>LEPORA</td><td>surface linguistic features</td><td></td><td>●</td><td>∅</td><td>Dublin City University, ADAPT (Han et al., 2012, 2013)</td><td>http://github.com/poethan/LEPOR</td></tr><tr><td>LEPORB</td><td>surface linguistic features</td><td></td><td>●</td><td>∅</td><td>Dublin City University, ADAPT (Han et al., 2012, 2013)</td><td>http://github.com/poethan/LEPOR</td></tr><tr><td>METEOR++_2.0 (SYNTAX)</td><td>word alignments</td><td></td><td>●</td><td>∅</td><td>Peking University (Guo and Hu, 2019)</td><td>-</td></tr><tr><td>METEOR++_2.0 (SYNTAX+COPY)</td><td>word alignments</td><td></td><td>●</td><td>∅</td><td>Peking University (Guo and Hu, 2019)</td><td>-</td></tr><tr><td>PREP</td><td>psuedo-references, paraphrases</td><td></td><td>●</td><td>∅</td><td>Tokyo Metropolitan Univ. (Yoshimura et al., 2019)</td><td>http://github.com/kokeman/PReP</td></tr><tr><td>WMDO</td><td>word mover distance</td><td></td><td>●</td><td>∅</td><td>Imperial College London (Chow et al., 2019a)</td><td>-</td></tr><tr><td>YiSI-0</td><td>semantic similarity</td><td></td><td>●</td><td>∅</td><td>NRC (Lo, 2019)</td><td>http://github.com/chikiulo/YiSi</td></tr><tr><td>YiSI-1</td><td>semantic similarity</td><td></td><td>●</td><td>∅</td><td>NRC (Lo, 2019)</td><td>http://github.com/chikiulo/YiSi</td></tr><tr><td>YiSI-1_SRL</td><td>semantic similarity</td><td></td><td>●</td><td>∅</td><td>NRC (Lo, 2019)</td><td>http://github.com/chikiulo/YiSi</td></tr><tr><td rowspan=\"10\">QE Systems</td><td>IBM1-MORPHEME</td><td>LM log pros., IBM1 lexicon</td><td></td><td>●</td><td>∅</td><td>Dublin City University, ADAPT (Popovic, 2012)</td><td>-</td></tr><tr><td>IBM1-POS4GRAM</td><td>LM log pros., IBM1 lexicon</td><td></td><td>●</td><td>∅</td><td>Dublin City University, ADAPT (Popovic, 2012)</td><td>-</td></tr><tr><td>LP</td><td>contextual word emb., MT log prob.</td><td>yes</td><td>●</td><td>∅</td><td>Univ. of Tartu (Yankovskaya et al., 2019)</td><td>-</td></tr><tr><td>LASIM</td><td>contextual word embeddings</td><td>yes</td><td>●</td><td>∅</td><td>Univ. of Tartu (Yankovskaya et al., 2019)</td><td>-</td></tr><tr><td>UNI</td><td>?</td><td>?</td><td>●</td><td>∅</td><td>?</td><td>?</td></tr><tr><td>UNI+</td><td>?</td><td>?</td><td>●</td><td>∅</td><td>?</td><td>?</td></tr><tr><td>USFD</td><td>?</td><td>?</td><td>●</td><td>∅</td><td>Univ. of Sheffield</td><td>?</td></tr><tr><td>USFD-TL</td><td>?</td><td>?</td><td>●</td><td>∅</td><td>Univ. of Sheffield</td><td>?</td></tr><tr><td>YiSI-2</td><td>semantic similarity</td><td></td><td>●</td><td>∅</td><td>NRC (Lo, 2019)</td><td>http://github.com/chikiulo/YiSi</td></tr><tr><td>YiSI-2_SRL</td><td>semantic similarity</td><td></td><td>●</td><td>∅</td><td>NRC (Lo, 2019)</td><td>http://github.com/chikiulo/YiSi</td></tr></table>"
            },
            {
                "block_id": 1,
                "type": "table_footnote",
                "bbox": [
                    1843,
                    256,
                    2086,
                    3181
                ],
                "angle": 270,
                "content": "Table 2: Participants of WMT19 Metrics Shared Task. “●” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation. “○” indicates that the system-level scores are implied, simply taking arithmetic (macro-)average of segment-level scores. “—” indicates that the metric didn’t participate the track (Seg/Sys-level). A metric is learned if it is trained on a QE or metric evaluation dataset (i.e. pretraining or parsers don’t count, but training on WMT 2017 metrics task data does). For the baseline metrics available in the Moses toolkit, paths are relative to http://github.com/moses-smt/mosesdecoder/."
            },
            {
                "block_id": 2,
                "type": "page_number",
                "bbox": [
                    1218,
                    3237,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "66"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    374,
                    263,
                    1215,
                    484
                ],
                "angle": 0,
                "content": "smoothed version of BLEU for scoring at the segment-level. We used the standard tokenizer script as available in Moses toolkit for tokenization."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    310,
                    568,
                    1215,
                    1189
                ],
                "angle": 0,
                "content": "chrF and chrF+. The metrics CHRF and CHRF+ (Popovic, 2015, 2017) are computed using their original Python implementation, see Table 2. We ran chrF++.py with the parameters -nw 0 -b 3 to obtain the CHRF score and with -nw 1 -b 3 to obtain the CHRF+ score. Note that CHRF intentionally removes all spaces before matching the \\( n \\)-grams, detokenizing the segments but also concatenating words.\\(^{10}\\)"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    310,
                    1266,
                    1215,
                    2059
                ],
                "angle": 0,
                "content": "sacreBLEU-BLEU and sacreBLEU-chrF. The metrics SACREBLEU-BLEU and SACREBLEU-CHRF (Post, 2018a) are re-implementation of BLEU and chrF respectively. We ran SACREBLEU-CHRF with the same parameters as CHRF, but their scores are slightly different. The signature strings produced by sacreBLEU for BLEU and chrF respectively are BLEU+case.mc+lang.de-en+numrefs.1+ smooth.exp+tok.intl+version.1.3.6 and chrF3+case.mixed+lang.de-en +numchars.6+numrefs.1+space.False+ tok.13a+version.1.3.6."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    2146,
                    1215,
                    2480
                ],
                "angle": 0,
                "content": "The baselines serve in system and segment-level evaluations as customary: BLEU, TER, WER, PER, CDER, SACREBLEU-BLEU and SACREBLEU-CHRF for system-level only; SENTBLEU for segment-level only and CHRF for both."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    2494,
                    1215,
                    2946
                ],
                "angle": 0,
                "content": "Chinese word segmentation is unfortunately not supported by the tokenization scripts mentioned above. For scoring Chinese with baseline metrics, we thus pre-processed MT outputs and reference translations with the script tokenizeChinese.py<sup>11</sup> by Shujian Huang, which separates Chinese characters from each other and also from non-Chinese parts."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    1267,
                    263,
                    1821,
                    312
                ],
                "angle": 0,
                "content": "4 Submitted Metrics"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1262,
                    354,
                    2195,
                    747
                ],
                "angle": 0,
                "content": "Table 2 lists the participants of the WMT19 Shared Metrics Task, along with their metrics and links to the source code where available. We have collected 24 metrics from a total of 13 research groups, with 10 reference-less “metrics” submitted to the joint task “QE as a Metric” with WMT19 Quality Estimation Task."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1265,
                    754,
                    2193,
                    862
                ],
                "angle": 0,
                "content": "The rest of this section provides a brief summary of all the metrics that participated."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1267,
                    905,
                    1540,
                    954
                ],
                "angle": 0,
                "content": "4.1 BEER"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    982,
                    2195,
                    1427
                ],
                "angle": 0,
                "content": "BEER (Stanojevic and Sima'an, 2015) is a trained evaluation metric with a linear model that combines sub-word feature indicators (character n-grams) and global word order features (skip bigrams) to achieve a language agnostic and fast to compute evaluation metric. BEER has participated in previous years of the evaluation task."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1267,
                    1473,
                    1555,
                    1522
                ],
                "angle": 0,
                "content": "4.2 BERTr"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    1550,
                    2193,
                    1711
                ],
                "angle": 0,
                "content": "BERT (Mathur et al., 2019) uses contextual word embeddings to compare the MT output with the reference translation."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1262,
                    1718,
                    2193,
                    2343
                ],
                "angle": 0,
                "content": "The BERTr score of a translation is the average recall score over all tokens, using a relaxed version of token matching based on BERT embeddings: namely, computing the maximum cosine similarity between the embedding of a reference token against any token in the MT output. BERTr uses bert_base_uncased embeddings for the to-English language pairs, and bert_baseMULTILINGCA sided embeddings for all other language pairs."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1267,
                    2381,
                    1664,
                    2431
                ],
                "angle": 0,
                "content": "4.3 CharacTER"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1262,
                    2459,
                    2195,
                    3192
                ],
                "angle": 0,
                "content": "CHARACTER (Wang et al., 2016b,a), identical to the 2016 setup, is a character-level metric inspired by the commonly applied translation edit rate (TER). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence. CHARACTER calculates the character-level edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis word is considered to match a reference word and could be shifted, if the edit dis"
            },
            {
                "block_id": 15,
                "type": "page_footnote",
                "bbox": [
                    287,
                    3009,
                    1210,
                    3188
                ],
                "angle": 0,
                "content": "<sup>10</sup>We originally planned to use the CHRF implementation which was recently made available in Moses Scorer but it mishandles Unicode characters for now. <sup>11</sup> http://hdl.handle.net/11346/WMT17-TVXH"
            },
            {
                "block_id": 16,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "67"
            }
        ]
    },
    {
        "page_id": 6,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    263,
                    1215,
                    768
                ],
                "angle": 0,
                "content": "tance between them is below a threshold value. The Levenshtein distance between the reference and the shifted hypothesis sequence is computed on the character level. In addition, the lengths of hypothesis sequences instead of reference sequences are used for normalizing the edit distance, which effectively counters the issue that shorter translations normally achieve lower TER."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    778,
                    1215,
                    1340
                ],
                "angle": 0,
                "content": "Similarly to other character-level metrics, CHARACTER is generally applied to non-tokensized outputs and references, which also holds for this year's submission with one exception. This year tokenization was carried out for en-ru hypotheses and references before calculating the scores, since this results in large improvements in terms of correlations. For other language pairs, no tokenizer was used for pre-processing."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    287,
                    1396,
                    525,
                    1445
                ],
                "angle": 0,
                "content": "4.4 EED"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1473,
                    1215,
                    2262
                ],
                "angle": 0,
                "content": "EED (Stanchev et al., 2019) is a character-based metric, which builds upon CDER. It is defined as the minimum number of operations of an extension to the conventional edit distance containing a \"jump\" operation. The edit distance operations (insertions, deletions and substitutions) are performed at the character level and jumps are performed when a blank space is reached. Furthermore, the coverage of multiple characters in the hypothesis is penalised by the introduction of a coverage penalty. The sum of the length of the reference and the coverage penalty is used as the normalisation term."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    287,
                    2318,
                    548,
                    2367
                ],
                "angle": 0,
                "content": "4.5 ESIM"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2399,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "Enhanced Sequential Inference Model (ESIM; Chen et al., 2017; Mathur et al., 2019) is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation. It uses cross-sentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 to-English data for the to-English language pairs, and all WMT 2018 data for all other language pairs."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1270,
                    263,
                    1868,
                    375
                ],
                "angle": 0,
                "content": "4.6 hLEPORb_baseline, hLEPORa_baseline"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1262,
                    399,
                    2195,
                    957
                ],
                "angle": 0,
                "content": "The submitted metric HLEPOR_BASELINE is a metric based on the factor combination of length penalty, precision, recall, and position difference penalty. The weighted harmonic mean is applied to group the factors together with tunable weight parameters. The system-level score is calculated with the same formula but with each factor weighted using weight estimated at system-level and not at segment-level."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1262,
                    968,
                    2195,
                    1757
                ],
                "angle": 0,
                "content": "In this submitted baseline version, HLEPOR_BASELINE was not tuned for each language pair separately but the default weights were applied across all submitted language pairs. Further improvements can be achieved by tuning the weights according to the development data, adding morphological information and applying n-gram factor scores into it (e.g. part-of-speech, n-gram precision and n-gram recall that were added into LEPOR in WMT13.). The basic model factors and further development with parameters setting were described in the paper (Han et al., 2012) and (Han et al., 2013)."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1262,
                    1764,
                    2193,
                    2038
                ],
                "angle": 0,
                "content": "For sentence-level score, only HLE-PORA_BASELINE was submitted with scores calculated as the weighted harmonic mean of all the designed factors using default parameters."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    2048,
                    2193,
                    2557
                ],
                "angle": 0,
                "content": "For system-level score, both HLEPORA_BASELINE and HLEPORB_BASELINE were submitted, where HLEPORA_BASELINE is the the average score of all sentence-level scores, and HLEPORB_BASELINE is calculated via the same sentence-level hLEPOR equation but replacing each factor value with its system-level counterpart."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1267,
                    2602,
                    2096,
                    2718
                ],
                "angle": 0,
                "content": "4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy)"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1262,
                    2739,
                    2195,
                    3195
                ],
                "angle": 0,
                "content": "METEOR++ 2.0 (Guo and Hu, 2019) is a metric based on Meteor (Denkowski and Lavie, 2014) that takes syntactic-level paraphrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching,"
            },
            {
                "block_id": 13,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "68"
            }
        ]
    },
    {
        "page_id": 7,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    263,
                    1215,
                    599
                ],
                "angle": 0,
                "content": "they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases. ME-TEOR++ 2.0 extracts the knowledge from the Paraphrase Database (PPDB; Bannard and Callison-Burch, 2005) and integrates it into Meteor-based metrics."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    638,
                    548,
                    687
                ],
                "angle": 0,
                "content": "4.8 PReP"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    712,
                    1215,
                    877
                ],
                "angle": 0,
                "content": "PREP (Yoshimura et al., 2019) is a method for filtering pseudo-references to achieve a good match with a gold reference."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    884,
                    1215,
                    1613
                ],
                "angle": 0,
                "content": "At the beginning, the source sentence is translated with some off-the-shelf MT systems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudoreferences are then filtered using BERT (Devlin et al., 2019) fine-tuned on the MPRC corpus (Dolan and Brockett, 2005), estimating the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT systems, a large portion of their outputs is indeed considered as a valid paraphrase."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1617,
                    1213,
                    1778
                ],
                "angle": 0,
                "content": "The final metric score is calculated simply with SentBLEU with these multiple references."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    287,
                    1824,
                    600,
                    1876
                ],
                "angle": 0,
                "content": "4.9 WMDO"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    1894,
                    1215,
                    2631
                ],
                "angle": 0,
                "content": "WMDO (Chow et al., 2019b) is a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation's word order have not been fully explored. Building on the Word Mover's Distance metric and various word embeddings, WMDO introduces a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    287,
                    2666,
                    1213,
                    2781
                ],
                "angle": 0,
                "content": "4.10 YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2, YiSi-2_srl"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    285,
                    2795,
                    1213,
                    3016
                ],
                "angle": 0,
                "content": "YiSi (Lo, 2019) is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    285,
                    3023,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "YiSi-1 is a MT evaluation metric that measures the semantic similarity between a machine translation and human references by"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1262,
                    263,
                    2195,
                    540
                ],
                "angle": 0,
                "content": "aggregating theidf-weighted lexical semantic similarities based on the contextual embeddings extracted from BERT and optionally incorporating shallow semantic structures (denoted as YiSi-1_srl)."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1262,
                    547,
                    2193,
                    771
                ],
                "angle": 0,
                "content": "YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any language. It uses longest common character substring to measure the lexical similarity."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1262,
                    775,
                    2195,
                    1171
                ],
                "angle": 0,
                "content": "YiSi-2 is the bilingual, reference-less version for MT quality estimation, which uses the contextual embeddings extracted from BERT to evaluate the crosslingual lexical semantic similarity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl)."
            },
            {
                "block_id": 13,
                "type": "title",
                "bbox": [
                    1265,
                    1210,
                    1701,
                    1266
                ],
                "angle": 0,
                "content": "4.11 QE Systems"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1262,
                    1283,
                    2193,
                    1736
                ],
                "angle": 0,
                "content": "In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the \"QE as a Metric\" track. The submitted QE systems are evaluated in the same settings as metrics to facilitate comparison. Their descriptions can be found in the Findings of the WMT 2019 Shared Task on Quality Estimation (Fonseca et al., 2019)."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1267,
                    1778,
                    1545,
                    1827
                ],
                "angle": 0,
                "content": "5 Results"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1262,
                    1869,
                    2193,
                    2034
                ],
                "angle": 0,
                "content": "We discuss system-level results for news task systems in Section 5.1. The segment-level results are in Section 5.2."
            },
            {
                "block_id": 17,
                "type": "title",
                "bbox": [
                    1265,
                    2076,
                    1962,
                    2129
                ],
                "angle": 0,
                "content": "5.1 System-Level Evaluation"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1262,
                    2150,
                    2193,
                    2371
                ],
                "angle": 0,
                "content": "As in previous years, we employ the Pearson correlation \\((r)\\) as the main evaluation measure for system-level metrics. The Pearson correlation is as follows:"
            },
            {
                "block_id": 19,
                "type": "equation",
                "bbox": [
                    1381,
                    2452,
                    2188,
                    2592
                ],
                "angle": 0,
                "content": "\\[\nr = \\frac {\\sum_ {i = 1} ^ {n} (H _ {i} - \\overline {{H}}) (M _ {i} - \\overline {{M}})}{\\sqrt {\\sum_ {i = 1} ^ {n} (H _ {i} - \\overline {{H}}) ^ {2}} \\sqrt {\\sum_ {i = 1} ^ {n} (M _ {i} - \\overline {{M}}) ^ {2}}} \\quad (1)\n\\]"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1262,
                    2627,
                    2190,
                    2904
                ],
                "angle": 0,
                "content": "where \\( H_{i} \\) are human assessment scores of all systems in a given translation direction, \\( M_{i} \\) are the corresponding scores as predicted by a given metric. \\( \\overline{H} \\) and \\( \\overline{M} \\) are their means, respectively."
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1262,
                    2911,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "Since some metrics, such as BLEU, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER, aim for a strong negative correlation we compare metrics via the absolute value \\( |r| \\) of a"
            },
            {
                "block_id": 22,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "69"
            }
        ]
    },
    {
        "page_id": 8,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    379,
                    775,
                    2096,
                    2445
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>de-en</td><td>fi-en</td><td>gu-en</td><td>kk-en</td><td>lt-en</td><td>ru-en</td><td>zh-en</td></tr><tr><td>n</td><td>16</td><td>12</td><td>11</td><td>11</td><td>11</td><td>14</td><td>15</td></tr><tr><td>Correlation</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td></tr><tr><td>BEER</td><td>0.906</td><td>0.993</td><td>0.952</td><td>0.986</td><td>0.947</td><td>0.915</td><td>0.942</td></tr><tr><td>BERTR</td><td>0.926</td><td>0.984</td><td>0.938</td><td>0.990</td><td>0.948</td><td>0.971</td><td>0.974</td></tr><tr><td>BLEU</td><td>0.849</td><td>0.982</td><td>0.834</td><td>0.946</td><td>0.961</td><td>0.879</td><td>0.899</td></tr><tr><td>CDER</td><td>0.890</td><td>0.988</td><td>0.876</td><td>0.967</td><td>0.975</td><td>0.892</td><td>0.917</td></tr><tr><td>CHARACTER</td><td>0.898</td><td>0.990</td><td>0.922</td><td>0.953</td><td>0.955</td><td>0.923</td><td>0.943</td></tr><tr><td>CHRF</td><td>0.917</td><td>0.992</td><td>0.955</td><td>0.978</td><td>0.940</td><td>0.945</td><td>0.956</td></tr><tr><td>CHRF+</td><td>0.916</td><td>0.992</td><td>0.947</td><td>0.976</td><td>0.940</td><td>0.945</td><td>0.956</td></tr><tr><td>EED</td><td>0.903</td><td>0.994</td><td>0.976</td><td>0.980</td><td>0.929</td><td>0.950</td><td>0.949</td></tr><tr><td>ESIM</td><td>0.941</td><td>0.971</td><td>0.885</td><td>0.986</td><td>0.989</td><td>0.968</td><td>0.988</td></tr><tr><td>HLEPORA_BASELINE</td><td>-</td><td>-</td><td>-</td><td>0.975</td><td>-</td><td>-</td><td>0.947</td></tr><tr><td>HLEPORB_BASELINE</td><td>-</td><td>-</td><td>-</td><td>0.975</td><td>0.906</td><td>-</td><td>0.947</td></tr><tr><td>METEOR++_2.0(SYNTAX)</td><td>0.887</td><td>0.995</td><td>0.909</td><td>0.974</td><td>0.928</td><td>0.950</td><td>0.948</td></tr><tr><td>METEOR++_2.0(SYNTAX+COPY)</td><td>0.896</td><td>0.995</td><td>0.900</td><td>0.971</td><td>0.927</td><td>0.952</td><td>0.952</td></tr><tr><td>NIST</td><td>0.813</td><td>0.986</td><td>0.930</td><td>0.942</td><td>0.944</td><td>0.925</td><td>0.921</td></tr><tr><td>PER</td><td>0.883</td><td>0.991</td><td>0.910</td><td>0.737</td><td>0.947</td><td>0.922</td><td>0.952</td></tr><tr><td>PREP</td><td>0.575</td><td>0.614</td><td>0.773</td><td>0.776</td><td>0.494</td><td>0.782</td><td>0.592</td></tr><tr><td>SACREBLEU.BLEU</td><td>0.813</td><td>0.985</td><td>0.834</td><td>0.946</td><td>0.955</td><td>0.873</td><td>0.903</td></tr><tr><td>SACREBLEU.CHRF</td><td>0.910</td><td>0.990</td><td>0.952</td><td>0.969</td><td>0.935</td><td>0.919</td><td>0.955</td></tr><tr><td>TER</td><td>0.874</td><td>0.984</td><td>0.890</td><td>0.799</td><td>0.960</td><td>0.917</td><td>0.840</td></tr><tr><td>WER</td><td>0.863</td><td>0.983</td><td>0.861</td><td>0.793</td><td>0.961</td><td>0.911</td><td>0.820</td></tr><tr><td>WMDO</td><td>0.872</td><td>0.987</td><td>0.983</td><td>0.998</td><td>0.900</td><td>0.942</td><td>0.943</td></tr><tr><td>YISI-0</td><td>0.902</td><td>0.993</td><td>0.993</td><td>0.991</td><td>0.927</td><td>0.958</td><td>0.937</td></tr><tr><td>YISI-1</td><td>0.949</td><td>0.989</td><td>0.924</td><td>0.994</td><td>0.981</td><td>0.979</td><td>0.979</td></tr><tr><td>YISI-1_SRL</td><td>0.950</td><td>0.989</td><td>0.918</td><td>0.994</td><td>0.983</td><td>0.978</td><td>0.977</td></tr><tr><td colspan=\"8\">QE as a Metric:</td></tr><tr><td>IBM1-MORPHEME</td><td>0.345</td><td>0.740</td><td>-</td><td>-</td><td>0.487</td><td>-</td><td>-</td></tr><tr><td>IBM1-POS4GRAM</td><td>0.339</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LASIM</td><td>0.247</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.310</td><td>-</td></tr><tr><td>LP</td><td>0.474</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.488</td><td>-</td></tr><tr><td>UNI</td><td>0.846</td><td>0.930</td><td>-</td><td>-</td><td>-</td><td>0.805</td><td>-</td></tr><tr><td>UNI+</td><td>0.850</td><td>0.924</td><td>-</td><td>-</td><td>-</td><td>0.808</td><td>-</td></tr><tr><td>YISI-2</td><td>0.796</td><td>0.642</td><td>0.566</td><td>0.324</td><td>0.442</td><td>0.339</td><td>0.940</td></tr><tr><td>YISI-2_SRL</td><td>0.804</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.947</td></tr><tr><td colspan=\"8\">newtest2019</td></tr></table>",
                "caption": "Table 3: Absolute Pearson correlation of to-English system-level metrics with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold."
            },
            {
                "block_id": 2,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "70"
            }
        ]
    },
    {
        "page_id": 9,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    414,
                    834,
                    2074,
                    2388
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>en-cs</td><td>en-de</td><td>en-fi</td><td>en-gu</td><td>en-kk</td><td>en-lt</td><td>en-ru</td><td>en-zh</td></tr><tr><td>n</td><td>11</td><td>22</td><td>12</td><td>11</td><td>11</td><td>12</td><td>12</td><td>12</td></tr><tr><td>Correlation</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td><td>|r|</td></tr><tr><td>BEER</td><td>0.990</td><td>0.983</td><td>0.989</td><td>0.829</td><td>0.971</td><td>0.982</td><td>0.977</td><td>0.803</td></tr><tr><td>BLEU</td><td>0.897</td><td>0.921</td><td>0.969</td><td>0.737</td><td>0.852</td><td>0.989</td><td>0.986</td><td>0.901</td></tr><tr><td>CDER</td><td>0.985</td><td>0.973</td><td>0.978</td><td>0.840</td><td>0.927</td><td>0.985</td><td>0.993</td><td>0.905</td></tr><tr><td>CHARACTER</td><td>0.994</td><td>0.986</td><td>0.968</td><td>0.910</td><td>0.936</td><td>0.954</td><td>0.985</td><td>0.862</td></tr><tr><td>CHRF</td><td>0.990</td><td>0.979</td><td>0.986</td><td>0.841</td><td>0.972</td><td>0.981</td><td>0.943</td><td>0.880</td></tr><tr><td>CHRF+</td><td>0.991</td><td>0.981</td><td>0.986</td><td>0.848</td><td>0.974</td><td>0.982</td><td>0.950</td><td>0.879</td></tr><tr><td>EED</td><td>0.993</td><td>0.985</td><td>0.987</td><td>0.897</td><td>0.979</td><td>0.975</td><td>0.967</td><td>0.856</td></tr><tr><td>ESIM</td><td>-</td><td>0.991</td><td>0.957</td><td>-</td><td>0.980</td><td>0.989</td><td>0.989</td><td>0.931</td></tr><tr><td>HLEPORA_BASELINE</td><td>-</td><td>-</td><td>-</td><td>0.841</td><td>0.968</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HLEPORB_BASELINE</td><td>-</td><td>-</td><td>-</td><td>0.841</td><td>0.968</td><td>0.980</td><td>-</td><td>-</td></tr><tr><td>NIST</td><td>0.896</td><td>0.321</td><td>0.971</td><td>0.786</td><td>0.930</td><td>0.993</td><td>0.988</td><td>0.884</td></tr><tr><td>PER</td><td>0.976</td><td>0.970</td><td>0.982</td><td>0.839</td><td>0.921</td><td>0.985</td><td>0.981</td><td>0.895</td></tr><tr><td>SACREBLEU.BLEU</td><td>0.994</td><td>0.969</td><td>0.966</td><td>0.736</td><td>0.852</td><td>0.986</td><td>0.977</td><td>0.801</td></tr><tr><td>SACREBLEU.CHRF</td><td>0.983</td><td>0.976</td><td>0.980</td><td>0.841</td><td>0.967</td><td>0.966</td><td>0.985</td><td>0.796</td></tr><tr><td>TER</td><td>0.980</td><td>0.969</td><td>0.981</td><td>0.865</td><td>0.940</td><td>0.994</td><td>0.995</td><td>0.856</td></tr><tr><td>WER</td><td>0.982</td><td>0.966</td><td>0.980</td><td>0.861</td><td>0.939</td><td>0.991</td><td>0.994</td><td>0.875</td></tr><tr><td>YISI-0</td><td>0.992</td><td>0.985</td><td>0.987</td><td>0.863</td><td>0.974</td><td>0.974</td><td>0.953</td><td>0.861</td></tr><tr><td>YISI-1</td><td>0.962</td><td>0.991</td><td>0.971</td><td>0.909</td><td>0.985</td><td>0.963</td><td>0.992</td><td>0.951</td></tr><tr><td>YISI-1_SRL</td><td>-</td><td>0.991</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.948</td></tr><tr><td colspan=\"9\">QE as a Metric:</td></tr><tr><td>IBM1-MORPHEME</td><td>0.871</td><td>0.870</td><td>0.084</td><td>-</td><td>-</td><td>0.810</td><td>-</td><td>-</td></tr><tr><td>IBM1-POS4GRAM</td><td>-</td><td>0.393</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LASIM</td><td>-</td><td>0.871</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.823</td><td>-</td></tr><tr><td>LP</td><td>-</td><td>0.569</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.661</td><td>-</td></tr><tr><td>UNI</td><td>0.028</td><td>0.841</td><td>0.907</td><td>-</td><td>-</td><td>-</td><td>0.919</td><td>-</td></tr><tr><td>UNI+</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.918</td><td>-</td></tr><tr><td>USFD</td><td>-</td><td>0.224</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.857</td><td>-</td></tr><tr><td>USFD-TL</td><td>-</td><td>0.091</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.771</td><td>-</td></tr><tr><td>YISI-2</td><td>0.324</td><td>0.924</td><td>0.696</td><td>0.314</td><td>0.339</td><td>0.055</td><td>0.766</td><td>0.097</td></tr><tr><td>YISI-2_SRL</td><td>-</td><td>0.936</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.118</td></tr><tr><td colspan=\"9\">newtest2019</td></tr></table>",
                "caption": "Table 4: Absolute Pearson correlation of out-of-English system-level metrics with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold."
            },
            {
                "block_id": 2,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "71"
            }
        ]
    },
    {
        "page_id": 10,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    468,
                    284,
                    2012,
                    2894
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 1: System-level metric significance test results for DA human assessment for into English and out-of-English language pairs (newstest2019): Green cells denote a statistically significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to Williams test."
            },
            {
                "block_id": 2,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "72"
            }
        ]
    },
    {
        "page_id": 11,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    287,
                    266,
                    1210,
                    368
                ],
                "angle": 0,
                "content": "given metric's correlation with human assessment."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    410,
                    945,
                    463
                ],
                "angle": 0,
                "content": "5.1.1 System-Level Results"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    477,
                    1215,
                    869
                ],
                "angle": 0,
                "content": "Tables 3, 4 and 5 provide the system-level correlations of metrics evaluating translation of newstest2019. The underlying texts are part of the WMT19 News Translation test set (newstest2019) and the underlying MT systems are all MT systems participating in the WMT19 News Translation Task."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    873,
                    1215,
                    1438
                ],
                "angle": 0,
                "content": "As recommended by Graham and Baldwin (2014), we employ Williams significance test (Williams, 1959) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables 3, 4 and 5."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1441,
                    1215,
                    1831
                ],
                "angle": 0,
                "content": "Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics significantly outperform the most widely employed metric BLEU, we include significance test results for every competing pair of metrics including our baseline metrics in Figure 1 and Figure 2."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1838,
                    1215,
                    2283
                ],
                "angle": 0,
                "content": "This year, the increased number of systems participating in the news tasks has provided a larger sample of system scores for testing metrics. Since we already have sufficiently conclusive results on genuine MT systems, we do not need to generate hybrid system results as in Graham and Liu (2016) and past metrics tasks."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    287,
                    2325,
                    1009,
                    2381
                ],
                "angle": 0,
                "content": "5.2 Segment-Level Evaluation"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2402,
                    1215,
                    3016
                ],
                "angle": 0,
                "content": "Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, again we were unable to follow the methodology outlined in Graham et al. (2015) for evaluation of segment-level metrics because the sampling of sentences did not provide sufficient number of assessments of the same segment. We therefore convert pairs of DA scores for competing translations to DARR better/worse preferences as described in Section 2.3.2."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    285,
                    3023,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "We measure the quality of metrics' segment-level scores against the DARR golden truth using a Kendall's Tau-like formulation, which is"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    266,
                    2195,
                    547
                ],
                "angle": 0,
                "content": "an adaptation of the conventional Kendall's Tau coefficient. Since we do not have a total order ranking of all translations, it is not possible to apply conventional Kendall's Tau (Graham et al., 2015)."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    547,
                    2193,
                    655
                ],
                "angle": 0,
                "content": "Our Kendall's Tau-like formulation, \\(\\tau\\), is as follows:"
            },
            {
                "block_id": 11,
                "type": "equation",
                "bbox": [
                    1396,
                    694,
                    2188,
                    813
                ],
                "angle": 0,
                "content": "\\[\n\\tau = \\frac {| C o n c o r d a n t | - | D i s c o r d a n t |}{| C o n c o r d a n t | + | D i s c o r d a n t |} (2)\n\\]"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1262,
                    848,
                    2193,
                    1241
                ],
                "angle": 0,
                "content": "where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons for which a given metric disagrees. The formula is not specific with respect to ties, i.e. cases where the annotation says that the two outputs are equally good."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1262,
                    1245,
                    2193,
                    1638
                ],
                "angle": 0,
                "content": "The way in which ties (both in human and metric judgement) were incorporated in computing Kendall \\(\\tau\\) has changed across the years of WMT Metrics Tasks. Here we adopt the version used in WMT17 DARR evaluation. For a detailed discussion on other options, see also Macháček and Bojar (2014)."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1265,
                    1641,
                    2193,
                    1918
                ],
                "angle": 0,
                "content": "Whether or not a given comparison of a pair of distinct translations of the same source input, \\( s_1 \\) and \\( s_2 \\), is counted as a concordant (Conc) or disconcertant (Disc) pair is defined by the following matrix:"
            },
            {
                "block_id": 15,
                "type": "table",
                "bbox": [
                    1305,
                    1943,
                    2165,
                    2234
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\" colspan=\"2\"></td><td colspan=\"3\">Metric</td></tr><tr><td>s1 &lt; s2</td><td>s1 = s2</td><td>s1 &gt; s2</td></tr><tr><td rowspan=\"3\">Human</td><td>s1 &lt; s2</td><td>Conc</td><td>Disc</td><td>Disc</td></tr><tr><td>s1 = s2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>s1 &gt; s2</td><td>Disc</td><td>Disc</td><td>Conc</td></tr></table>"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1265,
                    2248,
                    2193,
                    2473
                ],
                "angle": 0,
                "content": "In the notation of Macháček and Bojar (2014), this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR):"
            },
            {
                "block_id": 17,
                "type": "table",
                "bbox": [
                    1488,
                    2494,
                    1982,
                    2781
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\">WMT12</td><td colspan=\"3\">Metric</td></tr><tr><td>&lt;</td><td>=</td><td>&gt;</td></tr><tr><td rowspan=\"3\">Human</td><td>&lt;</td><td>1</td><td>-1</td></tr><tr><td>=</td><td>X</td><td>X</td></tr><tr><td>&gt;</td><td>-1</td><td>-1</td></tr></table>"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1265,
                    2799,
                    2195,
                    3192
                ],
                "angle": 0,
                "content": "The key differences between the evaluation used in WMT14-WMT16 and evaluation used in WMT17-WMT19 were (1) the move from RR to daRR and (2) the treatment of ties. In the years 2014-2016, ties in metrics scores were not penalized. With the move to daRR, where the quality of the two candidate translations"
            },
            {
                "block_id": 19,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "73"
            }
        ]
    },
    {
        "page_id": 12,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    798,
                    413,
                    1689,
                    1669
                ],
                "angle": 0,
                "content": "<table><tr><td></td><td>de-cs</td><td>de-fr</td><td>fr-de</td></tr><tr><td>n</td><td>11</td><td>11</td><td>10</td></tr><tr><td>Correlation</td><td>|r|</td><td>|r|</td><td>|r|</td></tr><tr><td>BEER</td><td>0.978</td><td>0.941</td><td>0.848</td></tr><tr><td>BLEU</td><td>0.941</td><td>0.891</td><td>0.864</td></tr><tr><td>CDER</td><td>0.864</td><td>0.949</td><td>0.852</td></tr><tr><td>CHARACTER</td><td>0.965</td><td>0.928</td><td>0.849</td></tr><tr><td>CHRF</td><td>0.974</td><td>0.931</td><td>0.864</td></tr><tr><td>CHRF+</td><td>0.972</td><td>0.936</td><td>0.848</td></tr><tr><td>EED</td><td>0.982</td><td>0.940</td><td>0.851</td></tr><tr><td>ESIM</td><td>0.980</td><td>0.950</td><td>0.942</td></tr><tr><td>HLEPORA_BASELINE</td><td>0.941</td><td>0.814</td><td>-</td></tr><tr><td>HLEPORB_BASELINE</td><td>0.959</td><td>0.814</td><td>-</td></tr><tr><td>NIST</td><td>0.954</td><td>0.916</td><td>0.862</td></tr><tr><td>PER</td><td>0.875</td><td>0.857</td><td>0.899</td></tr><tr><td>SACREBLEU-BLEU</td><td>0.869</td><td>0.891</td><td>0.869</td></tr><tr><td>SACREBLEU-CHRF</td><td>0.975</td><td>0.952</td><td>0.882</td></tr><tr><td>TER</td><td>0.890</td><td>0.956</td><td>0.895</td></tr><tr><td>WER</td><td>0.872</td><td>0.956</td><td>0.894</td></tr><tr><td>YISI-0</td><td>0.978</td><td>0.952</td><td>0.820</td></tr><tr><td>YISI-1</td><td>0.973</td><td>0.969</td><td>0.908</td></tr><tr><td>YISI-1_SRL</td><td>-</td><td>-</td><td>0.912</td></tr><tr><td colspan=\"4\">QE as a Metric:</td></tr><tr><td>IBM1-MORPHEME</td><td>0.355</td><td>0.509</td><td>0.625</td></tr><tr><td>IBM1-POS4GRAM</td><td>-</td><td>0.085</td><td>0.478</td></tr><tr><td>YISI-2</td><td>0.606</td><td>0.721</td><td>0.530</td></tr><tr><td colspan=\"4\">newtest2019</td></tr></table>",
                "caption": "Table 5: Absolute Pearson correlation of system-level metrics for language pairs not involving English with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold."
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    471,
                    2224,
                    962,
                    2757
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    999,
                    2224,
                    1481,
                    2757
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1518,
                    2224,
                    2004,
                    2757
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 2: System-level metric significance test results for DA human assessment in newstest2019 for German to Czech, German to French and French to German; green cells denote a statistically significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to Williams test."
            },
            {
                "block_id": 6,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "74"
            }
        ]
    },
    {
        "page_id": 13,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    349,
                    961,
                    2123,
                    2266
                ],
                "angle": 0,
                "content": "<table><tr><td>Human Evaluationn</td><td>de-enDARR85,365</td><td>fi-enDARR38,307</td><td>gu-enDARR31,139</td><td>kk-enDARR27,094</td><td>lt-enDARR21,862</td><td>ru-enDARR46,172</td><td>zh-enDARR31,070</td></tr><tr><td>BEER</td><td>0.128</td><td>0.283</td><td>0.260</td><td>0.421</td><td>0.315</td><td>0.189</td><td>0.371</td></tr><tr><td>BERTR</td><td>0.142</td><td>0.331</td><td>0.291</td><td>0.421</td><td>0.353</td><td>0.195</td><td>0.399</td></tr><tr><td>CHARACTER</td><td>0.101</td><td>0.253</td><td>0.190</td><td>0.340</td><td>0.254</td><td>0.155</td><td>0.337</td></tr><tr><td>CHRF</td><td>0.122</td><td>0.286</td><td>0.256</td><td>0.389</td><td>0.301</td><td>0.180</td><td>0.371</td></tr><tr><td>CHRF+</td><td>0.125</td><td>0.289</td><td>0.257</td><td>0.394</td><td>0.303</td><td>0.182</td><td>0.374</td></tr><tr><td>EED</td><td>0.120</td><td>0.281</td><td>0.264</td><td>0.392</td><td>0.298</td><td>0.176</td><td>0.376</td></tr><tr><td>ESIM</td><td>0.167</td><td>0.337</td><td>0.303</td><td>0.435</td><td>0.359</td><td>0.201</td><td>0.396</td></tr><tr><td>HLEPORA_BASELINE</td><td>-</td><td>-</td><td>-</td><td>0.372</td><td>-</td><td>-</td><td>0.339</td></tr><tr><td>METEOR++_2.0(SYNTAX)</td><td>0.084</td><td>0.274</td><td>0.237</td><td>0.395</td><td>0.291</td><td>0.156</td><td>0.370</td></tr><tr><td>METEOR++_2.0(SYNTAX+COPY)</td><td>0.094</td><td>0.273</td><td>0.244</td><td>0.402</td><td>0.287</td><td>0.163</td><td>0.367</td></tr><tr><td>PREP</td><td>0.030</td><td>0.197</td><td>0.192</td><td>0.386</td><td>0.193</td><td>0.124</td><td>0.267</td></tr><tr><td>SENTBLEU</td><td>0.056</td><td>0.233</td><td>0.188</td><td>0.377</td><td>0.262</td><td>0.125</td><td>0.323</td></tr><tr><td>WMDO</td><td>0.096</td><td>0.281</td><td>0.260</td><td>0.420</td><td>0.300</td><td>0.162</td><td>0.362</td></tr><tr><td>YISI-0</td><td>0.117</td><td>0.271</td><td>0.263</td><td>0.402</td><td>0.289</td><td>0.178</td><td>0.355</td></tr><tr><td>YISI-1</td><td>0.164</td><td>0.347</td><td>0.312</td><td>0.440</td><td>0.376</td><td>0.217</td><td>0.426</td></tr><tr><td>YISI-1_SRL</td><td>0.199</td><td>0.346</td><td>0.306</td><td>0.442</td><td>0.380</td><td>0.222</td><td>0.431</td></tr><tr><td colspan=\"8\">QE as a Metric:</td></tr><tr><td>IBM1-MORPHHEME</td><td>-0.074</td><td>0.009</td><td>-</td><td>-</td><td>0.069</td><td>-</td><td>-</td></tr><tr><td>IBM1-POS4GRAM</td><td>-0.153</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LASIM</td><td>-0.024</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.022</td><td>-</td></tr><tr><td>LP</td><td>-0.096</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-0.035</td><td>-</td></tr><tr><td>UNI</td><td>0.022</td><td>0.202</td><td>-</td><td>-</td><td>-</td><td>0.084</td><td>-</td></tr><tr><td>UNI+</td><td>0.015</td><td>0.211</td><td>-</td><td>-</td><td>-</td><td>0.089</td><td>-</td></tr><tr><td>YISI-2</td><td>0.068</td><td>0.126</td><td>-0.001</td><td>0.096</td><td>0.075</td><td>0.053</td><td>0.253</td></tr><tr><td>YISI-2_SRL</td><td>0.068</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.246</td></tr><tr><td colspan=\"8\">newstest2019</td></tr></table>",
                "caption": "Table 6: Segment-level metric results for to-English language pairs in newstest2019: absolute Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold."
            },
            {
                "block_id": 2,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "75"
            }
        ]
    },
    {
        "page_id": 14,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    362,
                    252,
                    2111,
                    1452
                ],
                "angle": 0,
                "content": "<table><tr><td>Human Evaluation\nn</td><td>en-cs\nDARR\n27,178</td><td>en-de\nDARR\n99,840</td><td>en-fi\nDARR\n31,820</td><td>en-gu\nDARR\n11,355</td><td>en-kk\nDARR\n18,172</td><td>en-lt\nDARR\n17,401</td><td>en-ru\nDARR\n24,334</td><td>en-zh\nDARR\n18,658</td></tr><tr><td>BEER</td><td>0.443</td><td>0.316</td><td>0.514</td><td>0.537</td><td>0.516</td><td>0.441</td><td>0.542</td><td>0.232</td></tr><tr><td>CHARACTER</td><td>0.349</td><td>0.264</td><td>0.404</td><td>0.500</td><td>0.351</td><td>0.311</td><td>0.432</td><td>0.094</td></tr><tr><td>CHRF</td><td>0.455</td><td>0.326</td><td>0.514</td><td>0.534</td><td>0.479</td><td>0.446</td><td>0.539</td><td>0.301</td></tr><tr><td>CHRF+</td><td>0.458</td><td>0.327</td><td>0.514</td><td>0.538</td><td>0.491</td><td>0.448</td><td>0.543</td><td>0.296</td></tr><tr><td>EED</td><td>0.431</td><td>0.315</td><td>0.508</td><td>0.568</td><td>0.518</td><td>0.425</td><td>0.546</td><td>0.257</td></tr><tr><td>ESIM</td><td>-</td><td>0.329</td><td>0.511</td><td>-</td><td>0.510</td><td>0.428</td><td>0.572</td><td>0.339</td></tr><tr><td>HLEPORA_BASELINE</td><td>-</td><td>-</td><td>-</td><td>0.463</td><td>0.390</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SENTBLEU</td><td>0.367</td><td>0.248</td><td>0.396</td><td>0.465</td><td>0.392</td><td>0.334</td><td>0.469</td><td>0.270</td></tr><tr><td>YiSi-0</td><td>0.406</td><td>0.304</td><td>0.483</td><td>0.539</td><td>0.494</td><td>0.402</td><td>0.535</td><td>0.266</td></tr><tr><td>YiSi-1</td><td>0.475</td><td>0.351</td><td>0.537</td><td>0.551</td><td>0.546</td><td>0.470</td><td>0.585</td><td>0.355</td></tr><tr><td>YiSi-1_SRL</td><td>-</td><td>0.368</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.361</td></tr><tr><td colspan=\"9\">QE as a Metric:</td></tr><tr><td>IBM1-MORPHEME</td><td>-0.135</td><td>-0.003</td><td>-0.005</td><td>-</td><td>-</td><td>-0.165</td><td>-</td><td>-</td></tr><tr><td>IBM1-POS4GRAM</td><td>-</td><td>-0.123</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LASIM</td><td>-</td><td>0.147</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-0.24</td><td>-</td></tr><tr><td>LP</td><td>-</td><td>-0.119</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-0.158</td><td>-</td></tr><tr><td>UNI</td><td>0.060</td><td>0.129</td><td>0.351</td><td>-</td><td>-</td><td>-</td><td>0.226</td><td>-</td></tr><tr><td>UNI+</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.222</td><td>-</td></tr><tr><td>USFD</td><td>-</td><td>-0.029</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.136</td><td>-</td></tr><tr><td>USFD-TL</td><td>-</td><td>-0.037</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.191</td><td>-</td></tr><tr><td>YiSi-2</td><td>0.069</td><td>0.212</td><td>0.239</td><td>0.147</td><td>0.187</td><td>0.003</td><td>-0.155</td><td>0.044</td></tr><tr><td>YiSi-2_SRL</td><td>-</td><td>0.236</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.034</td></tr><tr><td colspan=\"9\">newstest2019</td></tr></table>",
                "caption": "Table 7: Segment-level metric results for out-of-English language pairs in newstest2019: absolute Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    290,
                    1824,
                    1220,
                    2732
                ],
                "angle": 0,
                "content": "<table><tr><td>Human Evaluation\nn</td><td>de-cs\nDARR\n35,793</td><td>de-fr\nDARR\n4,862</td><td>fr-de\nDARR\n1,369</td></tr><tr><td>BEER</td><td>0.337</td><td>0.293</td><td>0.265</td></tr><tr><td>CHARACTER</td><td>0.232</td><td>0.251</td><td>0.224</td></tr><tr><td>CHRF</td><td>0.326</td><td>0.284</td><td>0.275</td></tr><tr><td>CHRF+</td><td>0.326</td><td>0.284</td><td>0.278</td></tr><tr><td>EED</td><td>0.345</td><td>0.301</td><td>0.267</td></tr><tr><td>ESIM</td><td>0.331</td><td>0.290</td><td>0.289</td></tr><tr><td>HLEPORA_BASELINE</td><td>0.207</td><td>0.239</td><td>-</td></tr><tr><td>SENTBLEU</td><td>0.203</td><td>0.235</td><td>0.179</td></tr><tr><td>YISI-0</td><td>0.331</td><td>0.296</td><td>0.277</td></tr><tr><td>YISI-1</td><td>0.376</td><td>0.349</td><td>0.310</td></tr><tr><td>YISI-1_SRL</td><td>-</td><td>-</td><td>0.299</td></tr><tr><td colspan=\"4\">QE as a Metric:</td></tr><tr><td>IBM1-MORPHEME</td><td>0.048</td><td>-0.013</td><td>-0.053</td></tr><tr><td>IBM1-POS4GRAM</td><td>-</td><td>-0.074</td><td>-0.097</td></tr><tr><td>YISI-2</td><td>0.199</td><td>0.186</td><td>0.066</td></tr><tr><td colspan=\"4\">newstest2019</td></tr></table>"
            },
            {
                "block_id": 3,
                "type": "table_footnote",
                "bbox": [
                    285,
                    2764,
                    1215,
                    3069
                ],
                "angle": 0,
                "content": "Table 8: Segment-level metric results for language pairs not involving English in newstest2019: absolute Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    1265,
                    1743,
                    2193,
                    1964
                ],
                "angle": 0,
                "content": "is deemed substantially different and no ties in human judgements arise, it makes sense to penalize ties in metrics' predictions in order to promote discerning metrics."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1262,
                    1971,
                    2193,
                    2585
                ],
                "angle": 0,
                "content": "Note that the penalization of ties makes our evaluation asymmetric, dependent on whether the metric predicted the tie for a pair where humans predicted \\(<\\), or \\(>\\). It is now important to interpret the meaning of the comparison identically for humans and metrics. For error metrics, we thus reverse the sign of the metric score prior to the comparison with human scores: higher scores have to indicate better translation quality. In WMT19, the original authors did this for CharacTER."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1265,
                    2592,
                    2190,
                    2704
                ],
                "angle": 0,
                "content": "To summarize, the WMT19 Metrics Task for segment-level evaluation:"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1312,
                    2753,
                    2190,
                    2978
                ],
                "angle": 0,
                "content": "- ensures that error metrics are first converted to the same orientation as the human judgements, i.e. higher score indicating higher translation quality,"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1312,
                    3023,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "- excludes all human ties (this is already implied by the construction of DARR from DA judgements),"
            },
            {
                "block_id": 9,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "76"
            }
        ]
    },
    {
        "page_id": 15,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    439,
                    291,
                    937,
                    806
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    989,
                    291,
                    1488,
                    806
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    1538,
                    291,
                    2039,
                    806
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    439,
                    806,
                    935,
                    1311
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    989,
                    806,
                    1488,
                    1315
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1543,
                    806,
                    2034,
                    1315
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    439,
                    1311,
                    935,
                    1820
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    989,
                    1315,
                    1488,
                    1834
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    1543,
                    1315,
                    2039,
                    1831
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    439,
                    1824,
                    935,
                    2346
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    989,
                    1827,
                    1488,
                    2378
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1543,
                    1827,
                    2034,
                    2378
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "image",
                "bbox": [
                    439,
                    2346,
                    935,
                    2880
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    965,
                    2385,
                    1510,
                    2932
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    1543,
                    2385,
                    2039,
                    2880
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 3: DARR segment-level metric significance test results for into English and out-of-English language pairs (newstest2019): Green cells denote a significant win for the metric in a given row over the metric in a given column according to bootstrap resampling."
            },
            {
                "block_id": 16,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "77"
            }
        ]
    },
    {
        "page_id": 16,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    468,
                    249,
                    957,
                    785
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    989,
                    252,
                    1481,
                    782
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    1513,
                    252,
                    2004,
                    778
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 4: DARR segment-level metric significance test results for German to Czech, German to French and French to German (newstest2019): Green cells denote a significant win for the metric in a given row over the metric in a given column according bootstrap resampling."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    332,
                    1083,
                    1205,
                    1136
                ],
                "angle": 0,
                "content": "- counts metric's ties as a Discordant pairs."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1171,
                    1213,
                    1564
                ],
                "angle": 0,
                "content": "We employ bootstrap resampling (Koehn, 2004; Graham et al., 2014b) to estimate confidence intervals for our Kendall's Tau formulation, and metrics with non-overlapping \\(95\\%\\) confidence intervals are identified as having statistically significant difference in performance."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    287,
                    1596,
                    975,
                    1648
                ],
                "angle": 0,
                "content": "5.2.1 Segment-Level Results"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    1662,
                    1213,
                    2115
                ],
                "angle": 0,
                "content": "Results of the segment-level human evaluation for translations sampled from the News Translation Task are shown in Tables 6, 7 and 8, where metric correlations not significantly outperformed by any other metric are highlighted in bold. Head-to-head significance test results for differences in metric performance are included in Figures 3 and 4."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    287,
                    2150,
                    637,
                    2206
                ],
                "angle": 0,
                "content": "6 Discussion"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    285,
                    2238,
                    1215,
                    2802
                ],
                "angle": 0,
                "content": "This year, human data was collected from reference-based evaluations (or \"monolingual\") and reference-free evaluations (or \"bilingual\"). The reference-based (monolingual) evaluations were obtained with the help of anonymous crowdsourcing, while the reference-less (bilingual) evaluations were mainly from MT researchers who committed their time contribution to the manual evaluation for each submitted system."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    287,
                    2837,
                    1069,
                    2894
                ],
                "angle": 0,
                "content": "6.1 Stability across MT Systems"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    285,
                    2908,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "The observed performance of metrics depends on the underlying texts and systems that participate in the News Translation Task (see Section 2). For the strongest MT systems, distinguishing which system outputs are better is"
            },
            {
                "block_id": 12,
                "type": "image",
                "bbox": [
                    1448,
                    1066,
                    2012,
                    1448
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 5: Pearson correlations of SACREBLEU-BLEU for English-German system-level evaluation for all systems (left) down to only top 4 systems (right). The y-axis spans from -1 to +1, baseline metrics for the language pair in grey."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1262,
                    1834,
                    2193,
                    2059
                ],
                "angle": 0,
                "content": "hard, even for human assessors. On the other hand, if the systems are spread across a wide performance range, it will be easier for metrics to correlate with human judgements."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1262,
                    2062,
                    2193,
                    2395
                ],
                "angle": 0,
                "content": "To provide a more reliable view, we created plots of Pearson correlation when the underlying set of MT systems is reduced to top \\( n \\) ones. One sample such plot is in Figure 5, all language pairs and most of the metrics are in Appendix A."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1262,
                    2402,
                    2195,
                    3132
                ],
                "angle": 0,
                "content": "As the plot documents, the official correlations reported in Tables 3 to 5 can lead to wrong conclusions. SACREBLEU-BLEU correlates at .969 when all systems are considered, but as we start considering only the top \\( n \\) systems, the correlation falls relatively quickly. With 10 systems, we are below .5 and when only the top 6 or 4 systems are considered, the correlation falls even to the negative values. Note that correlations point estimates (the value in the y-axis) become noisier with the decreasing number of the underlying MT systems."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1312,
                    3136,
                    2190,
                    3192
                ],
                "angle": 0,
                "content": "Figure 6 explains the situation and illus"
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "78"
            }
        ]
    },
    {
        "page_id": 17,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    287,
                    252,
                    1215,
                    957
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 6"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    1154,
                    1215,
                    1711
                ],
                "angle": 0,
                "content": "trates the sensitivity of the observed correlations to the exact set of systems. On the full set of systems, the single outlier (the worst-performing system called EN_DE_TASK) helps to achieve a great positive correlation. The majority of MT systems however form a cloud with Pearson correlation around .5 and the top 4 systems actually exhibit a negative correlation of the human score and SACREBLEU-BLEU."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1722,
                    1215,
                    2511
                ],
                "angle": 0,
                "content": "In Appendix A, baseline metrics are plotted in grey in all the plots, so that their trends can be observed jointly. In general, most baselines have similar correlations, as most baselines use similar features (n-gram or word-level features, with the exception of CHRF). In a number of language pairs (de-en, de-fr, en-de, en-kk, lt-en, ru-en, zh-en), baseline correlations tend towards 0 (no correlation) or even negative Pearson correlation. For a widely applied metric such as SACREBLEU-BLEU, our analysis reveals weak correlation in comparing top state-of-the-art systems in these language pairs, especially in en-de, de-en, ru-en, and zh-en."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    2515,
                    1215,
                    3195
                ],
                "angle": 0,
                "content": "We will restrict our analysis to those language pairs where the baseline metrics have an obvious downward trend (de-en, de-fr, en-de, en-kk, lt-en, ru-en, zh-en). Examining the top-n correlation in the submitted metrics (not including QE systems), most metrics show the same degradation in correlation as the baselines. We note BERTR as the one exception consistently degrading less and retaining positive correlation compared to other submitted metrics and baselines, in the language pairs where it participated."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1265,
                    266,
                    2195,
                    884
                ],
                "angle": 0,
                "content": "For QE systems, we noticed that in some instances, QE systems have upward correlation trends when other metrics and baselines have downward trends. For instance, LP, UNI, and \\(\\mathrm{UNI + }\\) in the de-en language pair, YiSi-2 in en-kk, and UNI and \\(\\mathrm{UNI + }\\) in ru-en. These results suggest that QE systems such as UNI and \\(\\mathrm{UNI + }\\) perform worse on judging systems of wide ranging quality, but better for top performing systems, or perhaps for systems closer in quality."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1265,
                    887,
                    2195,
                    1280
                ],
                "angle": 0,
                "content": "If our method of human assessment is sound, we should believe that BLEU, a widely applied metric, is no longer a reliable metric for judging our best systems. Future investigations are needed to understand when BLEU applies well, and why BLEU is not effective for output from our state of the art models."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1265,
                    1287,
                    2195,
                    1848
                ],
                "angle": 0,
                "content": "Metrics and QE systems such as BERTR, ESIM, YiSi that perform well at judging our best systems often use more semantic features compared to our n-gram/char-gram based baselines. Future metrics may want to explore a) whether semantic features such as contextual word embeddings are achieving semantic understanding and b) whether semantic understanding is the true source of a metric's performance gains."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    1855,
                    2193,
                    2301
                ],
                "angle": 0,
                "content": "It should be noted that some language pairs do not show the strong degrading pattern with top-\\(n\\) systems this year, for instance en-cs, en-gu, en-ru, or kk-en. English-Chinese is particularly interesting because we see a clear trend towards better correlations as we reduce the set of underlying systems to the top scoring ones."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1267,
                    2357,
                    2039,
                    2409
                ],
                "angle": 0,
                "content": "6.2 Overall Metric Performance"
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1267,
                    2438,
                    2002,
                    2490
                ],
                "angle": 0,
                "content": "6.2.1 System-Level Evaluation"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    2504,
                    2193,
                    2844
                ],
                "angle": 0,
                "content": "In system-level evaluation, the series of YIsI metrics achieve the highest correlations in several language pairs and it is not significantly outperformed by any other metrics (denoted as a “win” in the following) for almost all language pairs."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    2848,
                    2193,
                    3073
                ],
                "angle": 0,
                "content": "The new metric ESIM performs best on 5 language languages (18 language pairs) and obtains 11 \"wins\" out of 16 language pairs in which ESIM participated."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1267,
                    3076,
                    2190,
                    3195
                ],
                "angle": 0,
                "content": "The metric EED performs better for language pairs out-of-English and excluding En"
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "79"
            }
        ]
    },
    {
        "page_id": 18,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    287,
                    266,
                    1210,
                    375
                ],
                "angle": 0,
                "content": "glish compared to into-English language pairs, achieving 7 out of 11 “wins” there."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    449,
                    1049,
                    501
                ],
                "angle": 0,
                "content": "6.2.2 Segment-Level Evaluation"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    536,
                    1215,
                    926
                ],
                "angle": 0,
                "content": "For segment-level evaluation, most language pairs are quite discerning, with only one or two metrics taking the \"winner\" position (of not being significantly surpassed by others). Only French-German differs, with all metrics performing similarly except the significantly worse SENTBLEU."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    943,
                    1215,
                    1340
                ],
                "angle": 0,
                "content": "YiSi-1_SRL stands out as the \"winner\" for all language pairs in which it participated. The excluded language pairs were probably due to the lack of semantic information required by YiSi-1_SRL. YiSi-1 participated all language pairs and its correlations are comparable with those of YiSi-1_SRL."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    287,
                    1350,
                    1215,
                    1462
                ],
                "angle": 0,
                "content": "ESIM obtain 6 \"winners\" out of all 18 languages pairs."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1473,
                    1215,
                    1813
                ],
                "angle": 0,
                "content": "Both YiSi and ESIM are based on neural networks (YiSi via word and phrase embeddings, as well as other types of available resources, ESIM via sentence embeddings). This is a confirmation of a trend observed last year."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    287,
                    1883,
                    989,
                    1936
                ],
                "angle": 0,
                "content": "6.2.3 QE Systems as Metrics"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    1971,
                    1215,
                    2252
                ],
                "angle": 0,
                "content": "Generally, correlations for the standard reference-based metrics are obviously better than those in \"QE as a Metric\" track, both when using monolingual and bilingual golden truth."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    285,
                    2266,
                    1215,
                    2662
                ],
                "angle": 0,
                "content": "In system-level evaluation, correlations for \"QE as a Metric\" range from 0.028 to 0.947 across all language pairs and all metrics but they are very unstable. Even for a single metric, take UNI for example, the correlations range from 0.028 to 0.930 across language pairs."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    287,
                    2673,
                    1215,
                    2901
                ],
                "angle": 0,
                "content": "In segment-level evaluation, correlations for QE metrics range from -0.153 to 0.351 across all language pairs and show the same instability across language pairs for a given metric."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    285,
                    2911,
                    1215,
                    3195
                ],
                "angle": 0,
                "content": "In either case, we do not see any pattern that could explain the behaviour, e.g. whether the manual evaluation was monolingual or bilingual, or the characteristics of the given language pair."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1267,
                    266,
                    2126,
                    319
                ],
                "angle": 0,
                "content": "6.3 Dependence on Implementation"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    340,
                    2193,
                    508
                ],
                "angle": 0,
                "content": "As it already happened in the past, we had multiple implementations for some metrics, BLEU and CHRF in particular."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1265,
                    512,
                    2193,
                    677
                ],
                "angle": 0,
                "content": "The detailed configuration of BLEU and SACREBLEU-BLEU differ and hence their scores and correlation results are different."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1265,
                    680,
                    2195,
                    1126
                ],
                "angle": 0,
                "content": "CHRF and SACREBLEU-CHRF use the same parameters and should thus deliver the same scores but we still observe some differences, leading to different correlations. For instance for German-French Pearson correlation, CHRF obtains 0.931 (no win) but SACREBLEU-CHRF reaches 0.952, tying for a win with other metrics."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1265,
                    1133,
                    2193,
                    1413
                ],
                "angle": 0,
                "content": "We thus fully support the call for clarity by Post (2018b) and invite authors of metrics to include their implementations either in Moses scorer or sacreBLEU to achieve a long-term assessment of their metric."
            },
            {
                "block_id": 16,
                "type": "title",
                "bbox": [
                    1270,
                    1459,
                    1630,
                    1511
                ],
                "angle": 0,
                "content": "7 Conclusion"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1265,
                    1550,
                    2193,
                    1999
                ],
                "angle": 0,
                "content": "This paper summarizes the results of WMT19 shared task in machine translation evaluation, the Metrics Shared Task. Participating metrics were evaluated in terms of their correlation with human judgement at the level of the whole test set (system-level evaluation), as well as at the level of individual sentences (segment-level evaluation)."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1265,
                    2006,
                    2193,
                    2280
                ],
                "angle": 0,
                "content": "We reported scores for standard metrics requiring the reference as well as quality estimation systems which took part in the track \"QE as a metric\", joint with the Quality Estimation task."
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    1265,
                    2290,
                    2190,
                    2736
                ],
                "angle": 0,
                "content": "For system-level, best metrics reach over 0.95 Pearson correlation or better across several language pairs. As expected, QE systems are visibly in all language pairs but they can also reach high system-level correlations, up to .947 (Chinese-English) or .936 (English-German) by YI SI-1_SRL or over .9 for multiple language pairs by UNI."
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1265,
                    2743,
                    2193,
                    3195
                ],
                "angle": 0,
                "content": "An important caveat is that the correlations are heavily affected by the underlying set of MT systems. We explored this by reducing the set of systems to top-\\(n\\) ones for various \\(ns\\) and found out that for many language pairs, system-level correlations are much worse when based on only the better performing systems. With both good and bad MT systems partic-"
            },
            {
                "block_id": 21,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1270,
                    3279
                ],
                "angle": 0,
                "content": "80"
            }
        ]
    },
    {
        "page_id": 19,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    266,
                    1210,
                    431
                ],
                "angle": 0,
                "content": "ipating in the news task, the metrics results can be overly optimistic compared to what we get when evaluating state-of-the-art systems."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    434,
                    1210,
                    655
                ],
                "angle": 0,
                "content": "In terms of segment-level Kendall's \\(\\tau\\) results, the standard metrics correlations varied between 0.03 and 0.59, and QE systems obtained even negative correlations."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    659,
                    1210,
                    884
                ],
                "angle": 0,
                "content": "The results confirm the observation from the last year, namely metrics based on word or sentence-level embeddings (YiSi and ESIM), achieve the highest performance."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    290,
                    926,
                    739,
                    982
                ],
                "angle": 0,
                "content": "Acknowledgments"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1013,
                    1210,
                    1406
                ],
                "angle": 0,
                "content": "Results in this shared task would not be possible without tight collaboration with organizers of the WMT News Translation Task. We would like to thank Marcin Junczys-Dowmunt for the suggestion to examine metrics performance across varying subsets of MT systems, as we did in Appendix A."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1413,
                    1210,
                    1975
                ],
                "angle": 0,
                "content": "This study was supported in parts by the grants 19-26934X (NEUREM3) of the Czech Science Foundation, ADAPT Centre for Digital Content Technology (www.adaptcentre.ie) at Dublin City University funded under the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Regional Development Fund, and Charles University Research Programme \"Progress\" Q18+Q48."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    290,
                    2069,
                    563,
                    2118
                ],
                "angle": 0,
                "content": "References"
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    292,
                    2146,
                    1210,
                    2427
                ],
                "angle": 0,
                "content": "Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL '05, pages 597-604, Stroudsburg, PA, USA. Association for Computational Linguistics."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    292,
                    2462,
                    1210,
                    2922
                ],
                "angle": 0,
                "content": "Loïc Barrault, Ondrej Bojar, Marta R. Costajussa, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    292,
                    2957,
                    1210,
                    3188
                ],
                "angle": 0,
                "content": "Ondrej Bojar, Christian Federmann, Barry Haddow, Philipp Koehn, Matt Post, and Lucia Specia. 2016. Ten Years of WMT Evaluation Campaigns: Lessons Learnt. In Proceedings of the LREC 2016 Workshop \"Translation Evaluation"
            },
            {
                "block_id": 10,
                "type": "list",
                "bbox": [
                    292,
                    2146,
                    1210,
                    3188
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1322,
                    270,
                    2190,
                    406
                ],
                "angle": 0,
                "content": "- From Fragmented Tools and Data Sets to an Integrated Ecosystem\", pages 27-34, Portorose, Slovenia."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1275,
                    442,
                    2190,
                    715
                ],
                "angle": 0,
                "content": "Ondrej Bojar, Yvette Graham, and Amir Kamran. 2017. Results of the WMT17 metrics shared task. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Tasks Papers, Copenhagen, Denmark. Association for Computational Linguistics."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1272,
                    750,
                    2190,
                    1027
                ],
                "angle": 0,
                "content": "Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1657-1668."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1059,
                    2190,
                    1283
                ],
                "angle": 0,
                "content": "Julian Chow, Pranava Madhyastha, and Lucia Specia. 2019a. Wmdo: Fluency-based word mover's distance for machine translation evaluation. In Proceedings of Fourth Conference on Machine Translation."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1275,
                    1322,
                    2190,
                    1599
                ],
                "angle": 0,
                "content": "Julian Chow, Lucia Specia, and Pranava Madhyastha. 2019b. WMDO: Fluency-based Word Mover's Distance for Machine Translation Evaluation. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1275,
                    1631,
                    2190,
                    1953
                ],
                "angle": 0,
                "content": "Michael Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376-380, Baltimore, Maryland, USA. Association for Computational Linguistics."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1275,
                    1985,
                    2190,
                    2445
                ],
                "angle": 0,
                "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1275,
                    2476,
                    2190,
                    2795
                ],
                "angle": 0,
                "content": "George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using Ngram Co-occurrence Statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT '02, pages 138-145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1275,
                    2830,
                    2190,
                    3016
                ],
                "angle": 0,
                "content": "William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005)."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1272,
                    3048,
                    2190,
                    3188
                ],
                "angle": 0,
                "content": "Erick Fonseca, Lisa Yankovskaya, André F. T. Martins, Mark Fishel, and Christian Federmann. 2019. Findings of the WMT 2019 Shared"
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2190,
                    3188
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1262,
                    3279
                ],
                "angle": 0,
                "content": "81"
            }
        ]
    },
    {
        "page_id": 20,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    337,
                    270,
                    1215,
                    456
                ],
                "angle": 0,
                "content": "Task on Quality Estimation. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    295,
                    494,
                    1215,
                    820
                ],
                "angle": 0,
                "content": "Yvette Graham and Timothy Baldwin. 2014. Testing for Significance of Increased Correlation with Human Judgment. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172-176, Doha, Qatar. Association for Computational Linguistics."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    295,
                    859,
                    1215,
                    1185
                ],
                "angle": 0,
                "content": "Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous Measurement Scales in Human Evaluation of Machine Translation. In Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33-41, Sofia, Bulgaria. Association for Computational Linguistics."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    295,
                    1224,
                    1215,
                    1550
                ],
                "angle": 0,
                "content": "Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2014a. Is Machine Translation Getting Better over Time? In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 443-451, Gothenburg, Sweden. Association for Computational Linguistics."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    295,
                    1589,
                    1210,
                    1778
                ],
                "angle": 0,
                "content": "Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2016. Can machine translation systems be evaluated by the crowd alone. Natural Language Engineering, FirstView:1-28."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    295,
                    1813,
                    1218,
                    2188
                ],
                "angle": 0,
                "content": "Yvette Graham and Qun Liu. 2016. Achieving Accurate Conclusions in Evaluation of Automatic Machine Translation Metrics. In Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego, CA. Association for Computational Linguistics."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    295,
                    2224,
                    1215,
                    2508
                ],
                "angle": 0,
                "content": "Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014b. Randomized significance tests in machine translation. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, pages 266-274. Association for Computational Linguistics."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    295,
                    2546,
                    1215,
                    2873
                ],
                "angle": 0,
                "content": "Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2015. Accurate Evaluation of Segment-level Machine Translation Metrics. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, Denver, Colorado."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    295,
                    2908,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "Yinuo Guo and Junfeng Hu. 2019. Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 9,
                "type": "list",
                "bbox": [
                    295,
                    270,
                    1218,
                    3192
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    1277,
                    270,
                    2195,
                    592
                ],
                "angle": 0,
                "content": "Aaron L.-F. Han, Derek F. Wong, and Lidia S. Chao. 2012. Lepor: A robust evaluation metric for machine translation with augmented factors. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 441-450. Association for Computational Linguistics."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    1275,
                    627,
                    2195,
                    950
                ],
                "angle": 0,
                "content": "Aaron L.-F. Han, Derek F. Wong, Lidia S. Chao, Liangye He, Yi Lu, Junwen Xing, and Xiaodong Zeng. 2013. Language-independent model for machine translation evaluation with reinforced factors. In Machine Translation Summit XIV, pages 215-222. International Association for Machine Translation."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1272,
                    985,
                    2195,
                    1220
                ],
                "angle": 0,
                "content": "Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of Empirical Methods in Natural Language Processing, pages 388-395, Barcelona, Spain. Association for Computational Linguistics."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1275,
                    1255,
                    2195,
                    1582
                ],
                "angle": 0,
                "content": "Philipp Koehn and Christof Monz. 2006. Manual and Automatic Evaluation of Machine Translation Between European Languages. In Proceedings of the Workshop on Statistical Machine Translation, StatMT '06, pages 102-121, Stroudsburg, PA, USA. Association for Computational Linguistics."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1275,
                    1613,
                    2195,
                    1845
                ],
                "angle": 0,
                "content": "Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A novel string-to-string distance measure with applications to machine translation evaluation. In Proceedings of Mt Summit IX, pages 240-247."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1275,
                    1883,
                    2195,
                    2073
                ],
                "angle": 0,
                "content": "Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In *In Proceedings of EACL*, pages 241-248."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1275,
                    2104,
                    2195,
                    2385
                ],
                "angle": 0,
                "content": "Chi-kiu Lo. 2019. YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1275,
                    2420,
                    2195,
                    2743
                ],
                "angle": 0,
                "content": "Qingsong Ma, Ondrej Bojar, and Yvette Graham. 2018. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers, Brussels, Belgium. Association for Computational Linguistics."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1275,
                    2778,
                    2195,
                    3058
                ],
                "angle": 0,
                "content": "Matouš Macháček and Ondřej Bojar. 2014. Results of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293-301, Baltimore, MD, USA. Association for Computational Linguistics."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1272,
                    3094,
                    2195,
                    3185
                ],
                "angle": 0,
                "content": "Matouš Macháček and Ondřej Bojar. 2013. Results of the WMT13 Metrics Shared Task. In Proceed-"
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2195,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "82"
            }
        ]
    },
    {
        "page_id": 21,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    337,
                    270,
                    1213,
                    406
                ],
                "angle": 0,
                "content": "ings of the Eighth Workshop on Statistical Machine Translation, pages 45-51, Sofia, Bulgaria. Association for Computational Linguistics."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    295,
                    434,
                    1208,
                    670
                ],
                "angle": 0,
                "content": "Nitika Mathur, Tim Baldwin, and Trevor Cohn. 2019. Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In Proc. of ACL (short papers). To appear."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    295,
                    698,
                    1208,
                    971
                ],
                "angle": 0,
                "content": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, pages 311-318."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    295,
                    1003,
                    1208,
                    1276
                ],
                "angle": 0,
                "content": "Maja Popovic. 2012. Morpheme- and POS-based IBM1 and language model scores for translation quality estimation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT@NAACL-HLT 2012, June 7-8, 2012, Montreal, Canada, pages 133-137."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    295,
                    1308,
                    1208,
                    1536
                ],
                "angle": 0,
                "content": "Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, Lisboa, Portugal. Association for Computational Linguistics."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    295,
                    1568,
                    1208,
                    1796
                ],
                "angle": 0,
                "content": "Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Tasks Papers, Copenhagen, Denmark. Association for Computational Linguistics."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    295,
                    1827,
                    1208,
                    2055
                ],
                "angle": 0,
                "content": "Matt Post. 2018a. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Belgium, Brussels. Association for Computational Linguistics."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    295,
                    2087,
                    1208,
                    2273
                ],
                "angle": 0,
                "content": "Matt Post. 2018b. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference on Machine Translation, Belgium, Brussels. Association for Computational Linguistics."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    295,
                    2301,
                    1208,
                    2578
                ],
                "angle": 0,
                "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In *In Proceedings of Association for Machine Translation in the Americas*, pages 223-231."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    295,
                    2606,
                    1208,
                    2880
                ],
                "angle": 0,
                "content": "Peter Stanchev, Weiyue Wang, and Hermann Ney. 2019. EED: Extended Edit Distance Measure for Machine Translation. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    295,
                    2911,
                    1208,
                    3181
                ],
                "angle": 0,
                "content": "Miloš Stanojevic and Khalil Sima'an. 2015. BEER 1.1: ILLC UvA submission to metrics and tuning task. In Proceedings of the Tenth Workshop on Statistical Machine Translation, Lisboa, Portugal. Association for Computational Linguistics."
            },
            {
                "block_id": 11,
                "type": "list",
                "bbox": [
                    295,
                    270,
                    1213,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1272,
                    270,
                    2188,
                    501
                ],
                "angle": 0,
                "content": "Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016a. Character: Translation edit rate on character level. In ACL 2016 First Conference on Machine Translation, pages 505-510, Berlin, Germany."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1272,
                    533,
                    2188,
                    810
                ],
                "angle": 0,
                "content": "Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016b. Character: Translation Edit Rate on Character Level. In Proceedings of the First Conference on Machine Translation, Berlin, Germany. Association for Computational Linguistics."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1272,
                    845,
                    2188,
                    940
                ],
                "angle": 0,
                "content": "Evan James Williams. 1959. Regression analysis, volume 14. Wiley New York."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1272,
                    971,
                    2188,
                    1248
                ],
                "angle": 0,
                "content": "Elizaveta Yankovskaya, Andre Tattar, and Mark Fishel. 2019. Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1283,
                    2188,
                    1645
                ],
                "angle": 0,
                "content": "Ryoma Yoshimura, Hiroki Shimanaka, Yukio Matsumura, Hayahide Yamagishi, and Mamoru Komachi. 2019. Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation. In Proceedings of the Fourth Conference on Machine Translation, Florence, Italy. Association for Computational Linguistics."
            },
            {
                "block_id": 17,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2188,
                    1645
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "83"
            }
        ]
    },
    {
        "page_id": 22,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    290,
                    259,
                    1175,
                    319
                ],
                "angle": 0,
                "content": "A Correlations for Top-N Systems"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    292,
                    420,
                    677,
                    1059
                ],
                "angle": 0,
                "content": null,
                "caption": "A.1 de-cs"
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    704,
                    417,
                    1054,
                    1059
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1084,
                    417,
                    1434,
                    1059
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1461,
                    417,
                    1813,
                    1059
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    1845,
                    417,
                    2195,
                    1059
                ],
                "angle": 0,
                "content": null,
                "caption": "A.2 de-en"
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    292,
                    1182,
                    674,
                    2220
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    709,
                    1178,
                    1051,
                    2220
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1086,
                    1178,
                    1429,
                    2220
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1461,
                    1178,
                    1811,
                    2220
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "image",
                "bbox": [
                    1845,
                    1178,
                    2193,
                    2220
                ],
                "angle": 0,
                "content": null,
                "caption": "A.3 de-fr"
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    292,
                    2339,
                    677,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    707,
                    2339,
                    1054,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "image",
                "bbox": [
                    1084,
                    2339,
                    1434,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "image",
                "bbox": [
                    1461,
                    2339,
                    1813,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "image",
                "bbox": [
                    1845,
                    2339,
                    2195,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 19,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1270,
                    3276
                ],
                "angle": 0,
                "content": "84"
            }
        ]
    },
    {
        "page_id": 23,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    292,
                    364,
                    677,
                    1013
                ],
                "angle": 0,
                "content": null,
                "caption": "A.4 en-cs"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    704,
                    364,
                    1059,
                    1013
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1086,
                    364,
                    1436,
                    1013
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1461,
                    364,
                    1816,
                    820
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1840,
                    364,
                    2198,
                    820
                ],
                "angle": 0,
                "content": null,
                "caption": "A.5 en-de"
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    292,
                    1252,
                    677,
                    2308
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    704,
                    1252,
                    1056,
                    2104
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1086,
                    1252,
                    1431,
                    2101
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1461,
                    1252,
                    1813,
                    2101
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1835,
                    1252,
                    2198,
                    2101
                ],
                "angle": 0,
                "content": null,
                "caption": "A.6 en-fi"
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    292,
                    2532,
                    677,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    702,
                    2532,
                    1054,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    1076,
                    2532,
                    1434,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "image",
                "bbox": [
                    1461,
                    2532,
                    1813,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "image",
                "bbox": [
                    1835,
                    2532,
                    2198,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "85"
            }
        ]
    },
    {
        "page_id": 24,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    292,
                    431,
                    677,
                    1083
                ],
                "angle": 0,
                "content": null,
                "caption": "A.7 en-gu"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    704,
                    431,
                    1054,
                    1080
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1086,
                    431,
                    1431,
                    1080
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1463,
                    431,
                    1811,
                    884
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1845,
                    438,
                    2195,
                    884
                ],
                "angle": 0,
                "content": null,
                "caption": "A.8 en-kk"
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    292,
                    1483,
                    677,
                    2129
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    704,
                    1483,
                    1054,
                    2129
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1086,
                    1483,
                    1431,
                    2129
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1466,
                    1483,
                    1811,
                    2129
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1845,
                    1487,
                    2195,
                    1936
                ],
                "angle": 0,
                "content": null,
                "caption": "A.9 en-lt"
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    292,
                    2532,
                    677,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    704,
                    2532,
                    1054,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    1081,
                    2532,
                    1431,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "image",
                "bbox": [
                    1461,
                    2532,
                    1813,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "image",
                "bbox": [
                    1840,
                    2532,
                    2195,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "86"
            }
        ]
    },
    {
        "page_id": 25,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    290,
                    340,
                    677,
                    1182
                ],
                "angle": 0,
                "content": null,
                "caption": "A.10 en-ru"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    704,
                    340,
                    1051,
                    1182
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1076,
                    340,
                    1431,
                    1182
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1456,
                    340,
                    1811,
                    982
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1835,
                    340,
                    2195,
                    982
                ],
                "angle": 0,
                "content": null,
                "caption": "A.11 en-zh"
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    290,
                    1336,
                    677,
                    1985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    704,
                    1336,
                    1051,
                    1982
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1076,
                    1336,
                    1431,
                    1982
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1456,
                    1336,
                    1813,
                    1982
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1835,
                    1336,
                    2195,
                    1982
                ],
                "angle": 0,
                "content": null,
                "caption": "A.12 fi-en"
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    290,
                    2136,
                    677,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    704,
                    2136,
                    1051,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    1076,
                    2136,
                    1431,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "image",
                "bbox": [
                    1456,
                    2136,
                    1811,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "image",
                "bbox": [
                    1835,
                    2136,
                    2195,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1215,
                    3230,
                    1270,
                    3276
                ],
                "angle": 0,
                "content": "87"
            }
        ]
    },
    {
        "page_id": 26,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    290,
                    364,
                    677,
                    1013
                ],
                "angle": 0,
                "content": null,
                "caption": "A.13 fr-de"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    704,
                    361,
                    1059,
                    1013
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1086,
                    361,
                    1436,
                    1013
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1466,
                    361,
                    1816,
                    1013
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1845,
                    361,
                    2198,
                    1013
                ],
                "angle": 0,
                "content": null,
                "caption": "A.14 gu-en"
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    290,
                    1255,
                    677,
                    2108
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    704,
                    1255,
                    1059,
                    2104
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1086,
                    1255,
                    1436,
                    2104
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1466,
                    1255,
                    1816,
                    1904
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1845,
                    1255,
                    2198,
                    1904
                ],
                "angle": 0,
                "content": null,
                "caption": "A.15 kk-en"
            },
            {
                "block_id": 13,
                "type": "image",
                "bbox": [
                    290,
                    2332,
                    677,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 14,
                "type": "image",
                "bbox": [
                    704,
                    2332,
                    1059,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "image",
                "bbox": [
                    1086,
                    2332,
                    1436,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "image",
                "bbox": [
                    1466,
                    2332,
                    1816,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 17,
                "type": "image",
                "bbox": [
                    1845,
                    2332,
                    2198,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 18,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "88"
            }
        ]
    },
    {
        "page_id": 27,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "header",
                "bbox": [
                    292,
                    266,
                    560,
                    312
                ],
                "angle": 0,
                "content": "A.16 lt-en"
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    292,
                    547,
                    677,
                    1392
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    704,
                    550,
                    1054,
                    1392
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1084,
                    550,
                    1431,
                    1392
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1456,
                    550,
                    1811,
                    1392
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1840,
                    550,
                    2195,
                    1392
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    292,
                    1852,
                    575,
                    1901
                ],
                "angle": 0,
                "content": "A.17 ru-en"
            },
            {
                "block_id": 7,
                "type": "image",
                "bbox": [
                    292,
                    2132,
                    677,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 8,
                "type": "image",
                "bbox": [
                    704,
                    2132,
                    1054,
                    3181
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 9,
                "type": "image",
                "bbox": [
                    1076,
                    2132,
                    1434,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1456,
                    2132,
                    1811,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 11,
                "type": "image",
                "bbox": [
                    1838,
                    2132,
                    2195,
                    2985
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3276
                ],
                "angle": 0,
                "content": "89"
            }
        ]
    },
    {
        "page_id": 28,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    292,
                    315,
                    669,
                    1357
                ],
                "angle": 0,
                "content": null,
                "caption": "A.18 zh-en"
            },
            {
                "block_id": 2,
                "type": "image",
                "bbox": [
                    704,
                    312,
                    1051,
                    1157
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 3,
                "type": "image",
                "bbox": [
                    1076,
                    312,
                    1431,
                    1157
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 4,
                "type": "image",
                "bbox": [
                    1456,
                    312,
                    1806,
                    1157
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 5,
                "type": "image",
                "bbox": [
                    1826,
                    312,
                    2198,
                    1157
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 6,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1270,
                    3276
                ],
                "angle": 0,
                "content": "90"
            }
        ]
    }
]