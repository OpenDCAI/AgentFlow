[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    702,
                    284,
                    1793,
                    420
                ],
                "angle": 0,
                "content": "Disentangled Representation Learning for Non-Parallel Text Style Transfer"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    595,
                    515,
                    1915,
                    575
                ],
                "angle": 0,
                "content": "Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    1017,
                    578,
                    1483,
                    631
                ],
                "angle": 0,
                "content": "University of Waterloo"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    595,
                    634,
                    1905,
                    694
                ],
                "angle": 0,
                "content": "{vineet. john, hpallika, ovechtom}@uwaterloo.ca"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    873,
                    698,
                    1627,
                    750
                ],
                "angle": 0,
                "content": "doublepower.mou@gmail.com"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    654,
                    929,
                    850,
                    978
                ],
                "angle": 0,
                "content": "Abstract"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    354,
                    1038,
                    1143,
                    1789
                ],
                "angle": 0,
                "content": "This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    290,
                    1841,
                    650,
                    1894
                ],
                "angle": 0,
                "content": "1 Introduction"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    285,
                    1939,
                    1215,
                    2448
                ],
                "angle": 0,
                "content": "The neural network has been a successful learning machine during the past decade due to its highly expressive modeling capability, which is a consequence of multiple layers of non-linear transformations of input features. Such transformations, however, make intermediate features \"latent,\" in the sense that they do not have explicit meaning and are not interpretable. Therefore, neural networks are usually treated as black-box machinery."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    285,
                    2455,
                    1215,
                    3016
                ],
                "angle": 0,
                "content": "Disentangling the latent space of neural networks has become an increasingly important research topic. In the image domain, for example, Chen et al. (2016) use adversarial and information maximization objectives to produce interpretable latent representations that can be tweaked to adjust writing style for handwritten digits, as well as lighting and orientation for face models. However, this problem is less explored in natural language processing."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    929,
                    2195,
                    1378
                ],
                "angle": 0,
                "content": "In this paper, we address the problem of disentangling the latent space of neural networks for text generation. Our model is built on an autoencoder that encodes a sentence to the latent space (vector representation) by learning to reconstruct the sentence itself. We would like the latent space to be disentangled with respect to different features, namely, style and content in our task."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    1385,
                    2195,
                    2620
                ],
                "angle": 0,
                "content": "To accomplish this, we propose a simple yet effective approach that combines multi-task and adversarial objectives. We artificially divide the latent representation into two parts: the style space and content space, where we consider the sentiment of a sentence as its style. We design a systematic set of auxiliary losses, enforcing the separation of style and content latent spaces. In particular, the multi-task loss operates on a latent space to ensure that the space does contain the information we wish to encode. The adversarial loss, on the contrary, minimizes the predictability of information that should not be contained in a given latent space. In early work, researchers typically work with the style space (Shen et al., 2017; Fu et al., 2018), but simply ignore the content space, as it is hard to formalize what \"content\" actually refers to. Cycle consistency of back-translation defines content implicitly (Xu et al., 2018), but requires reinforcement learning over the discrete sentence space, which could be extremely difficult to train."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    2627,
                    2195,
                    3020
                ],
                "angle": 0,
                "content": "In our paper, we propose to approximate the content information by bag-of-words (BoW) features, where we focus on style-neutral, non-stopwords. Along with traditional style-oriented auxiliary losses, our BoW multi-task loss and BoW adversarial loss enable better disentanglement of the style and content spaces."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1265,
                    3023,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "The learned disentangled latent space can be directly used for text style transfer, which aims to transform a given sentence to a new sentence with"
            },
            {
                "block_id": 14,
                "type": "page_footnote",
                "bbox": [
                    287,
                    3055,
                    1215,
                    3188
                ],
                "angle": 0,
                "content": "1Our code and all model output are available at https://sites.google.com/view/disentangle4transfer."
            },
            {
                "block_id": 15,
                "type": "page_number",
                "bbox": [
                    1203,
                    3230,
                    1285,
                    3279
                ],
                "angle": 0,
                "content": "424"
            },
            {
                "block_id": 16,
                "type": "footer",
                "bbox": [
                    439,
                    3301,
                    2036,
                    3395
                ],
                "angle": 0,
                "content": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 424-434 Florence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    266,
                    1215,
                    1108
                ],
                "angle": 0,
                "content": "the same content but a different style. We follow the setting where the model is trained on a non-parallel but style-labeled corpus (Hu et al., 2017; Shen et al., 2017); thus, we call it non-parallel text style transfer. With our disentangled latent space, we simply use the autoencoder to encode the content vector of a sentence, but ignore its encoded style vector. We then infer from the training data an empirical embedding of the style that we would like to transfer to. The encoded content vector and the empirically-inferred style vector are concatenated and fed to the decoder. This grafting technique enables us to obtain a new sentence similar in content to the input sentence, but with a different style."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    1115,
                    1215,
                    1680
                ],
                "angle": 0,
                "content": "We conducted experiments on two benchmark datasets. Both qualitative and quantitative results show that the style and content spaces are indeed disentangled well. In the style-transfer evaluation, we achieve high performance in style-transfer accuracy, content preservation, as well as language fluency, compared with previous results. Ablation tests also show that all our auxiliary losses can be combined well, each playing its own role in disentangling the latent space."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    287,
                    1732,
                    677,
                    1785
                ],
                "angle": 0,
                "content": "2 Related Work"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1831,
                    1215,
                    2448
                ],
                "angle": 0,
                "content": "Disentangling neural networks' latent space has been explored in computer vision in recent years, and researchers have successfully disentangled the features (such as rotation and color) of images (Chen et al., 2016; Higgins et al., 2017). In these approaches, the disentanglement is purely unsupervised, as no style labels are needed. Unfortunately, we have not observed disentangled features by applying these approaches in text representations, and thus we require style labels in our approach."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    2455,
                    1215,
                    2680
                ],
                "angle": 0,
                "content": "Style-transfer has also been explored in computer vision. For example, Gatys et al. (2016) show that the artistic style of an image can be captured well by certain statistics."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2683,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "In NLP, the definition of “style” itself is vague, and as a convenient starting point, researchers often treat sentiment as a salient style attribute. Hu et al. (2017) propose to control the sentiment by using discriminators to reconstruct sentiment and content from generated sentences. However, there is no evidence that the latent space would be disentangled by simply reconstructing a sentence. Shen et al. (2017) use a pair of adversarial discrimina"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1262,
                    266,
                    2195,
                    1280
                ],
                "angle": 0,
                "content": "tors to align the recurrent hidden decoder states of original and style-transferred sentences, for a given style. Fu et al. (2018) propose two approaches: training style-specific embeddings and training separate style-specific decoders. Their style embeddings are similar to an earlier study by study by Ficler and Goldberg (2017). Their multi-decoder approach is used by Nogueira dos Santos et al. (2018), and is extended to private-shared networks for styled generation (Zhang et al., 2018). Zhao et al. (2018) also extend the multi-decoder approach and use a Wasserstein-distance penalty to align content representations of sentences with different styles. Tsvetkov et al. (2018) use a machine-translation preprocessing step to strip author style from documents, and then use a multi-decoder model to convert the result into a sentence with a specific style."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1265,
                    1294,
                    2195,
                    1571
                ],
                "angle": 0,
                "content": "Recently, cycle consistency of back-translation is applied to ensure content preservation (Xu et al., 2018; Logeswaran et al., 2018). These methods require reinforcement learning and are usually difficult to train."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    1589,
                    2193,
                    1810
                ],
                "angle": 0,
                "content": "Li et al. (2018) propose a hybrid retrieval and generation method that transfers the style by retrieving and incrementally editing a sentence similar to the source sentence."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    1827,
                    2193,
                    2108
                ],
                "angle": 0,
                "content": "Rao and Tetreault (2018) treat the formality of writing as a style, and create a parallel corpus for style transfer with sequence-to-sequence models. This is beyond the scope of our paper, as we focus on non-parallel text style transfer."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    2125,
                    2193,
                    2402
                ],
                "angle": 0,
                "content": "Style transfer generation is also related to non-parallel machine translation, where researchers apply similar techniques of adversarial alignment, back translation, etc. (Lample et al., 2018a,b; Conneau et al., 2018)."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    2420,
                    2193,
                    2645
                ],
                "angle": 0,
                "content": "Our paper differs from previous work in that we accomplish style transfer with a disentangled latent space, for which we propose a systematic set of auxiliary losses."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1267,
                    2732,
                    1572,
                    2792
                ],
                "angle": 0,
                "content": "3 Approach"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1265,
                    2855,
                    2193,
                    3188
                ],
                "angle": 0,
                "content": "Figure 1 shows the overall framework of our approach. We will first present an autoencoder as our base model. Then we design the auxiliary losses for style and content disentanglement. Finally, we introduce our approach to style-transfer text generation."
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3279
                ],
                "angle": 0,
                "content": "425"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "image",
                "bbox": [
                    334,
                    249,
                    1166,
                    599
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 1,
                "type": "image",
                "bbox": [
                    334,
                    610,
                    1069,
                    859
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 1: Overview of our approach."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    287,
                    1055,
                    652,
                    1101
                ],
                "angle": 0,
                "content": "3.1 Autoencoder"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    287,
                    1150,
                    1213,
                    1259
                ],
                "angle": 0,
                "content": "An autoencoder encodes an input to a latent vector space, from which it reconstructs the input itself."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    287,
                    1273,
                    1213,
                    1655
                ],
                "angle": 0,
                "content": "Let \\( \\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n}) \\) be an input sequence with \\( n \\) words. Our encoder uses a recurrent neural network (RNN) with gated recurrent units (GRUs, Cho et al., 2014); it reads \\( \\mathbf{x} \\) word-by-word, and performs a linear transformation of the final hidden state to obtain a hidden vector representation \\( \\mathbf{h} \\)."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    287,
                    1680,
                    1213,
                    2017
                ],
                "angle": 0,
                "content": "Then, a decoder RNN generates a sentence word-by-word, which ideally should be \\( \\mathbf{x} \\) itself. Suppose at a time step \\( t \\) the decoder RNN predicts the word \\( x_{t} \\) with probability \\( p(x_{t}|h,x_{1}\\dots x_{t - 1}) \\), the autoencoder is trained with a sequence-aggregated cross-entropy loss, given by"
            },
            {
                "block_id": 7,
                "type": "equation",
                "bbox": [
                    327,
                    2087,
                    1210,
                    2273
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {A E}} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}, \\boldsymbol {\\theta} _ {\\mathrm {D}}\\right) = - \\sum_ {t = 1} ^ {n} \\log p \\left(x _ {t} \\mid \\boldsymbol {h}, x _ {1} \\dots x _ {t - 1}\\right) \\tag {1}\n\\]"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    287,
                    2280,
                    1213,
                    2781
                ],
                "angle": 0,
                "content": "where \\(\\theta_{\\mathrm{E}}\\) and \\(\\theta_{\\mathrm{D}}\\) are the parameters of the encoder and decoder, respectively. For brevity, we only present the loss for a single data point (i.e., a sentence) throughout the paper. Total loss sums over all data points, and is implemented with minibatches. Both the encoder and decoder are deterministic functions in the this model (Rumelhart et al., 1986), and thus, we call it a deterministic autoencoder (DAE)."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    287,
                    2799,
                    1213,
                    3192
                ],
                "angle": 0,
                "content": "Variational Autoencoder. Alternatively, we may use a variational autoencoder (VAE, Kingma and Welling, 2013), which imposes a probabilistic distribution on the latent vector. The decoder reconstructs data based on the sampled latent vector from its posterior, and the Kullback-Leibler (KL, 1951) divergence is penalized for regularization."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1314,
                    266,
                    1791,
                    315
                ],
                "angle": 0,
                "content": "Formally, the VAE loss is"
            },
            {
                "block_id": 11,
                "type": "equation",
                "bbox": [
                    1319,
                    368,
                    2188,
                    501
                ],
                "angle": 0,
                "content": "\\[\n\\begin{array}{l} J _ {\\mathrm {A E}} (\\pmb {\\theta} _ {\\mathrm {E}}, \\pmb {\\theta} _ {\\mathrm {D}}) = - \\mathbb {E} _ {q _ {E} (\\pmb {h} | \\mathrm {x})} [ \\log p (\\mathrm {x} | \\pmb {h}) ] \\\\ + \\lambda_ {\\mathrm {k l}} \\operatorname {K L} \\left(q _ {E} (\\boldsymbol {h} | \\mathrm {x}) \\| p (\\boldsymbol {h})\\right) \\tag {2} \\\\ \\end{array}\n\\]"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    554,
                    2190,
                    884
                ],
                "angle": 0,
                "content": "where \\(\\lambda_{\\mathrm{kl}}\\) is the hyperparameter balancing the reconstruction loss and the KL term. \\(p(h)\\) is the prior, typically the standard normal \\(\\mathcal{N}(\\mathbf{0},\\mathrm{I})\\). \\(q_{E}(h|\\mathrm{x})\\) is the posterior in the form \\(\\mathcal{N}(\\boldsymbol{\\mu},\\mathrm{diag}\\sigma^2)\\), where \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\) are predicted by the encoder."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1265,
                    894,
                    2193,
                    1283
                ],
                "angle": 0,
                "content": "Compared with DAE, the reconstruction of VAE is based on the samples of the posterior, which populates encoded representations into a neighbourhood close to its prior and thus smoothes the latent space. Bowman et al. (2016) show that VAE enables more fluent sentence generation from a latent space than DAE."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1265,
                    1290,
                    2193,
                    1739
                ],
                "angle": 0,
                "content": "The autoencoding loss serves as our primary training objective for sentence generation. For disentangled representation learning, we hope that \\( h \\) can be separated into two spaces \\( s \\) and \\( c \\), representing style and content, respectively, i.e., \\( h = [s; c] \\), where \\( [\\cdot; \\cdot] \\) denotes concatenation. This is accomplished by a systematic design of auxiliary losses described below, and shown in Figure 1a."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1267,
                    1785,
                    1808,
                    1834
                ],
                "angle": 0,
                "content": "3.2 Style-Oriented Losses"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1265,
                    1859,
                    2190,
                    2132
                ],
                "angle": 0,
                "content": "We first design auxiliary losses that ensure the style information is contained in the style space \\( s \\). This involves (1) a multi-task loss that ensures \\( s \\) is discriminative for the style, and (2) an adversarial loss that ensures \\( c \\) is not."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1265,
                    2143,
                    2190,
                    2420
                ],
                "angle": 0,
                "content": "Multi-Task Loss for Style. In the dataset, each sentence is labeled with its style, particularly, binary sentiment of positive or negative, following most previous work (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018)."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1265,
                    2427,
                    2190,
                    2595
                ],
                "angle": 0,
                "content": "We build a two-way softmax classifier (equivalent to logistic regression) on the style space \\( s \\) to predict the style label, given by"
            },
            {
                "block_id": 19,
                "type": "equation",
                "bbox": [
                    1414,
                    2648,
                    2188,
                    2704
                ],
                "angle": 0,
                "content": "\\[\n\\boldsymbol {y} _ {s} = \\operatorname {s o f t m a x} \\left(W _ {\\mathrm {m u l} (\\mathrm {s})} \\boldsymbol {s} + \\boldsymbol {b} _ {\\mathrm {m u l} (\\mathrm {s})}\\right) \\tag {3}\n\\]"
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    1265,
                    2757,
                    2190,
                    2925
                ],
                "angle": 0,
                "content": "where \\(\\pmb{\\theta}_{\\mathrm{mul(s)}} = [W_{\\mathrm{mul(s)}};\\pmb{b}_{\\mathrm{mul(s)}}]\\) are the parameters of the style classifier in the setting of multitask learning, and \\(\\pmb{y}_s\\) is the output of softmax layer."
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1265,
                    2929,
                    2190,
                    3041
                ],
                "angle": 0,
                "content": "The classifier is trained with cross-entropy loss against the ground-truth distribution \\( t_{s}(\\cdot) \\) by"
            },
            {
                "block_id": 22,
                "type": "equation",
                "bbox": [
                    1295,
                    3083,
                    2190,
                    3202
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {m u l} (\\mathrm {s})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}; \\boldsymbol {\\theta} _ {\\mathrm {m u l} (\\mathrm {s})}\\right) = - \\sum_ {l \\in \\text {l a b e l s}} t _ {s} (l) \\log y _ {s} (l) \\tag {4}\n\\]"
            },
            {
                "block_id": 23,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "426"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    266,
                    1215,
                    827
                ],
                "angle": 0,
                "content": "In fact, we train the style classifier at the same time as the autoencoding loss. Thus, this could be viewed as multi-task learning, incentivizing the entire model to not only decode the sentence, but also predict its sentiment from the style vector \\( s \\). We denote it by \"mul(s).\" The idea of multi-task training is not new and has been used in previous work for sentence representation learning (Jernite et al., 2017) and sentiment analysis (Balogas et al., 2017), among others."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    834,
                    1215,
                    1112
                ],
                "angle": 0,
                "content": "Adversarial Loss for Style. The multi-task loss only ensures that the style space contains style information. However, the content space might also contain style information, which is undesirable for disentanglement."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    1119,
                    1215,
                    1511
                ],
                "angle": 0,
                "content": "We thus apply an adversarial loss to discourage the content space containing style information. We first train a separate classifier, called an adversary, that deliberately discriminates the style label based on the content vector \\( \\pmb{c} \\). Then, the encoder is trained to encode a content space from which its adversary cannot predict the style."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1518,
                    1215,
                    1739
                ],
                "angle": 0,
                "content": "Concretely, the adversarial discriminator and its training objective have a similar form as Eqns. (3) and (4), but with different input and parameters, given by"
            },
            {
                "block_id": 4,
                "type": "equation",
                "bbox": [
                    516,
                    1799,
                    1210,
                    1859
                ],
                "angle": 0,
                "content": "\\[\n\\boldsymbol {y} _ {s} = \\operatorname {s o f t m a x} \\left(W _ {\\mathrm {d i s (s)}} \\boldsymbol {c} + \\boldsymbol {b} _ {\\mathrm {d i s (s)}}\\right) \\tag {5}\n\\]"
            },
            {
                "block_id": 5,
                "type": "equation",
                "bbox": [
                    327,
                    1869,
                    1210,
                    1953
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {d i s} (\\mathrm {s})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {d i s} (\\mathrm {s})}\\right) = - \\sum_ {l \\in \\text {l a b e l s}} t _ {c} (l) \\log y _ {s} (l) \\tag {6}\n\\]"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    1999,
                    1213,
                    2111
                ],
                "angle": 0,
                "content": "where \\(\\pmb{\\theta}_{\\mathrm{dis(s)}} = [W_{\\mathrm{dis(s)}};\\pmb{b}_{\\mathrm{dis(s)}}]\\) are the parameters of the adversary."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2118,
                    1215,
                    2455
                ],
                "angle": 0,
                "content": "It should be emphasized that, when we train the adversary, the gradient is not propagated back to the autoencoder, i.e., the vector \\( \\pmb{c} \\) is treated as shallow features. Therefore, we view \\( J_{\\mathrm{dis(s)}} \\) as a function of \\( \\theta_{\\mathrm{dis(s)}} \\) only, whereas \\( J_{\\mathrm{mul(s)}} \\) is a function of both \\( \\theta_{\\mathrm{E}} \\) and \\( \\theta_{\\mathrm{mul(s)}} \\)."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    285,
                    2459,
                    1215,
                    3080
                ],
                "angle": 0,
                "content": "Having trained an adversary, we would like the autoencoder to be tuned in such an ad hoc fashion that \\( c \\) is not discriminative for style. In existing literature, there could be different approaches, for example, maximizing the adversary's loss (Shen et al., 2017; Zhao et al., 2018) or penalizing the entropy of the adversary's prediction (Fu et al., 2018). In our work, we adopt the latter, as it can be easily extended to multi-category classification, used in Subsection 3.3. Formally, the style-oriented adversarial objective is to maximize"
            },
            {
                "block_id": 9,
                "type": "equation",
                "bbox": [
                    478,
                    3136,
                    1210,
                    3195
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {a d v} (\\mathrm {s})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}\\right) = \\mathcal {H} \\left(\\boldsymbol {y} _ {s} \\mid \\boldsymbol {c}; \\boldsymbol {\\theta} _ {\\mathrm {d i s} (\\mathrm {s})}\\right) \\tag {7}\n\\]"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    266,
                    2195,
                    599
                ],
                "angle": 0,
                "content": "where \\(\\pmb{y}_s\\) is the predicted distribution over the style labels and \\(\\mathcal{H}(\\pmb{p}) = -\\sum_{i \\in \\mathrm{labels}} p_i \\log p_i\\) is the entropy of the adversary. Here, \\(J_{\\mathrm{adv(s)}}\\) is maximized with respect to the encoder \\(\\theta_{\\mathrm{E}}\\) and we fix \\(\\theta_{\\mathrm{dis(s)}}\\). The objective attains maximum value when \\(\\pmb{y}_s\\) is uniform."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    606,
                    2195,
                    1003
                ],
                "angle": 0,
                "content": "While adversarial loss has been explored in previous style-transfer studies (Shen et al., 2017; Fu et al., 2018), it has not been combined with the multi-task loss. As shown in our experiments, a simple combination of these two losses is promisingly effective, achieving better style transfer performance than a variety of previous methods."
            },
            {
                "block_id": 12,
                "type": "title",
                "bbox": [
                    1267,
                    1052,
                    1873,
                    1101
                ],
                "angle": 0,
                "content": "3.3 Content-Oriented Losses"
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1265,
                    1129,
                    2193,
                    1347
                ],
                "angle": 0,
                "content": "The above style-oriented losses only regularize style information, but they do not impose any constraint on how the content information should be encoded."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1265,
                    1361,
                    2193,
                    1810
                ],
                "angle": 0,
                "content": "In practice, the style space is usually smaller than content space. But it is unrealistic to expect that the content would not flow into the style space simply because of its limited capacity. Therefore, we need to design content-oriented losses to regularize the content information. In most previous work, however, the treatment of content is missing (Hu et al., 2017; Fu et al., 2018)."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1265,
                    1813,
                    2193,
                    2038
                ],
                "angle": 0,
                "content": "Inspired by the above combination of multi-task and adversarial losses, we apply the same idea to the content space. However, it is usually hard to define what \"content\" actually refers to."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1265,
                    2045,
                    2193,
                    2841
                ],
                "angle": 0,
                "content": "To this end, we propose to approximate the content information by bag-of-words (BoW) features. The BoW feature of a sentence is a vector, each element indicating the probability of a word's occurrence. For a sentence \\( x \\) with \\( N \\) words, the word \\( w_{*} \\)'s BoW probability is \\( t_c(w_*) = \\frac{\\sum_{i=1}^{N} \\mathbb{I}\\{w_i = w_*\\}}{N} \\), where \\( \\mathbb{I}\\{\\cdot\\} \\) is an indicator function. Here, we only consider content words, excluding stopwords and sentiment words (Hu and Liu, 2004), since we focus on \"content\" information. It should be mentioned that the removal of stopwords and sentiment words is not essential, but results in better performance. We analyze the effect of using different vocabularies in Appendix B."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1265,
                    2848,
                    2193,
                    3016
                ],
                "angle": 0,
                "content": "Multi-Task Loss for Content. Similar to the style-oriented loss, the multi-task loss for content, denoted as “mul(c),” ensures that the content space"
            },
            {
                "block_id": 18,
                "type": "page_footnote",
                "bbox": [
                    1267,
                    3058,
                    2193,
                    3188
                ],
                "angle": 0,
                "content": "The list of sentiment words is available at https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"
            },
            {
                "block_id": 19,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3279
                ],
                "angle": 0,
                "content": "427"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    287,
                    266,
                    1210,
                    431
                ],
                "angle": 0,
                "content": "\\(c\\) contains content information, i.e., BoW features. We introduce a softmax classifier over the BoW vocabulary"
            },
            {
                "block_id": 1,
                "type": "equation",
                "bbox": [
                    434,
                    466,
                    1210,
                    526
                ],
                "angle": 0,
                "content": "\\[\n\\boldsymbol {y} _ {c} = \\operatorname {s o f t m a x} \\left(W _ {\\mathrm {m u l} (\\mathrm {c})} \\boldsymbol {c} + \\boldsymbol {b} _ {\\mathrm {m u l} (\\mathrm {c})}\\right) \\tag {8}\n\\]"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    287,
                    550,
                    1213,
                    663
                ],
                "angle": 0,
                "content": "where \\(\\theta_{\\mathrm{mul(c)}} = [W_{\\mathrm{mul(c)}}; b_{\\mathrm{mul(c)}}]\\) are the classifier's parameters; \\(y_c\\) is the predicted BoW distribution."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    287,
                    666,
                    1213,
                    778
                ],
                "angle": 0,
                "content": "The training objective is a cross-entropy loss against the ground-truth distribution \\( t_c(\\cdot) \\):"
            },
            {
                "block_id": 4,
                "type": "equation",
                "bbox": [
                    329,
                    799,
                    1210,
                    964
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {m u l} (\\mathrm {c})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}; \\boldsymbol {\\theta} _ {\\mathrm {m u l} (\\mathrm {c})}\\right) = - \\sum_ {w \\in \\text {v o c a b}} t _ {c} (w) \\log y _ {c} (w) \\tag {9}\n\\]"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    287,
                    968,
                    1213,
                    1248
                ],
                "angle": 0,
                "content": "where the optimization is performed with both encoder parameters \\(\\theta_{\\mathrm{E}}\\) and the multi-task classifier \\(\\theta_{\\mathrm{mul(c)}}\\). Notice that, although the target distribution is not one-hot for BoW, the cross-entropy loss in Eqn. (9) has the same form as (4)."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    1252,
                    1213,
                    1757
                ],
                "angle": 0,
                "content": "It is also interesting that, at first glance, the multi-task loss for content appears to be redundant to the autoencoding loss, when in fact, it is not. The autoencoding loss only requires that the model could reconstruct the sentence based on the combined content and style spaces, but does not ensure their separation. The multi-task loss focuses on content words and is applied to the content space only."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    287,
                    1761,
                    1213,
                    1982
                ],
                "angle": 0,
                "content": "Adversarial Loss for Content. To ensure that the style space does not contain content information, we design our final auxiliary loss, the BoW adversarial loss for content, denoted as \"adv(c).\""
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    287,
                    1985,
                    1213,
                    2097
                ],
                "angle": 0,
                "content": "We build a content adversary, a softmax classifier on the style space predicting BoW features"
            },
            {
                "block_id": 9,
                "type": "equation",
                "bbox": [
                    337,
                    2122,
                    1210,
                    2188
                ],
                "angle": 0,
                "content": "\\[\n\\boldsymbol {y} _ {c} = \\operatorname {s o f t m a x} \\left(W _ {\\operatorname {d i s} (\\mathrm {c})} ^ {\\top} \\boldsymbol {s} + \\boldsymbol {b} _ {\\operatorname {d i s} (\\mathrm {c})}\\right) \\tag {10}\n\\]"
            },
            {
                "block_id": 10,
                "type": "equation",
                "bbox": [
                    349,
                    2199,
                    1210,
                    2311
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {d i s} (\\mathrm {c})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {d i s} (\\mathrm {c})}\\right) = - \\sum_ {w \\in \\text {v o c a b}} t _ {c} (w) \\log y _ {c} (w) \\tag {11}\n\\]"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    287,
                    2339,
                    1213,
                    2452
                ],
                "angle": 0,
                "content": "where \\(\\pmb{\\theta}_{\\mathrm{dis(c)}} = [W_{\\mathrm{dis(c)}};\\pmb{b}_{\\mathrm{dis(c)}}]\\) are the classifier's parameters for BoW prediction."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    287,
                    2455,
                    1213,
                    2564
                ],
                "angle": 0,
                "content": "The adversarial loss for the model is to maximize the entropy of the discriminator"
            },
            {
                "block_id": 13,
                "type": "equation",
                "bbox": [
                    476,
                    2595,
                    1210,
                    2659
                ],
                "angle": 0,
                "content": "\\[\nJ _ {\\mathrm {a d v} (\\mathrm {c})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}\\right) = \\mathcal {H} \\left(\\boldsymbol {y} _ {c} \\mid \\boldsymbol {s}; \\boldsymbol {\\theta} _ {\\mathrm {d i s} (\\mathrm {c})}\\right) \\tag {12}\n\\]"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    287,
                    2687,
                    1213,
                    2904
                ],
                "angle": 0,
                "content": "Again, \\(J_{\\mathrm{dis(c)}}\\) is trained with respect to the discriminator's parameters \\(\\theta_{\\mathrm{dis(c)}}\\), whereas \\(J_{\\mathrm{adv(c)}}\\) is trained with respect to \\(\\theta_{\\mathrm{E}}\\), similar to the adversarial loss for style."
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    287,
                    2911,
                    1213,
                    3192
                ],
                "angle": 0,
                "content": "Our BoW-based, content-oriented losses are novel in the style-transfer literature. While they do not directly work with \"style,\" they regularize the content information, so that the style and content can be better disentangled."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1277,
                    270,
                    1744,
                    319
                ],
                "angle": 0,
                "content": "1 foreach mini-batch do"
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1277,
                    326,
                    2046,
                    385
                ],
                "angle": 0,
                "content": "2 minimize \\(J_{\\mathrm{dis(s)}}(\\pmb{\\theta}_{\\mathrm{dis(s)}})\\) w.r.t. \\(\\pmb{\\theta}_{\\mathrm{dis(s)}}\\)"
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    1280,
                    389,
                    2054,
                    442
                ],
                "angle": 0,
                "content": "3 minimize \\(J_{\\mathrm{dis(c)}}(\\pmb{\\theta}_{\\mathrm{dis(c)}})\\) w.r.t. \\(\\pmb{\\theta}_{\\mathrm{dis(c)}}\\)"
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    1280,
                    445,
                    2165,
                    501
                ],
                "angle": 0,
                "content": "4 minimize \\(J_{\\mathrm{ovr}}\\) w.r.t. \\(\\theta_{\\mathrm{E}},\\theta_{\\mathrm{D}},\\theta_{\\mathrm{mul(s)},}\\theta_{\\mathrm{mul(c)}};\\)"
            },
            {
                "block_id": 20,
                "type": "list",
                "bbox": [
                    1277,
                    270,
                    2165,
                    501
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    1280,
                    508,
                    1399,
                    547
                ],
                "angle": 0,
                "content": "5 end"
            },
            {
                "block_id": 22,
                "type": "text",
                "bbox": [
                    1426,
                    554,
                    2014,
                    610
                ],
                "angle": 0,
                "content": "Algorithm 1: Training process."
            },
            {
                "block_id": 23,
                "type": "title",
                "bbox": [
                    1267,
                    701,
                    1714,
                    757
                ],
                "angle": 0,
                "content": "3.4 Training Process"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    1265,
                    775,
                    2193,
                    1003
                ],
                "angle": 0,
                "content": "The overall loss \\( J_{\\mathrm{ovr}} \\) for our model comprises several terms: the autoencoder's reconstruction objective, the multi-task and adversarial objectives, for style and content, respectively, given by"
            },
            {
                "block_id": 25,
                "type": "equation",
                "bbox": [
                    1282,
                    1041,
                    2188,
                    1241
                ],
                "angle": 0,
                "content": "\\[\n\\begin{array}{l} J _ {\\mathrm {o v r}} = J _ {\\mathrm {A E}} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}, \\boldsymbol {\\theta} _ {\\mathrm {D}}\\right) \\tag {13} \\\\ + \\lambda_ {\\mathrm {m u l (s)}} J _ {\\mathrm {m u l (s)}} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}, \\boldsymbol {\\theta} _ {\\mathrm {m u l (s)}}\\right) - \\lambda_ {\\mathrm {a d v (s)}} J _ {\\mathrm {a d v (s)}} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}\\right) \\\\ + \\lambda_ {\\mathrm {m u l} (\\mathrm {c})} J _ {\\mathrm {m u l} (\\mathrm {c})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}, \\boldsymbol {\\theta} _ {\\mathrm {m u l} (\\mathrm {c})}\\right) - \\lambda_ {\\mathrm {a d v} (\\mathrm {c})} J _ {\\mathrm {a d v} (\\mathrm {c})} \\left(\\boldsymbol {\\theta} _ {\\mathrm {E}}\\right) \\\\ \\end{array}\n\\]"
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    1265,
                    1280,
                    2193,
                    1392
                ],
                "angle": 0,
                "content": "where \\(\\lambda\\)s are the hyperparameters that balance the autoencoding loss and these auxiliary losses."
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    1265,
                    1396,
                    2193,
                    1617
                ],
                "angle": 0,
                "content": "To put it all together, the model training involves an alternation of optimizing the adversaries by \\( J_{\\mathrm{dis(s)}} \\) and \\( J_{\\mathrm{dis(c)}} \\), and the model itself by \\( J_{\\mathrm{ovr}} \\), shown in Algorithm 1."
            },
            {
                "block_id": 28,
                "type": "title",
                "bbox": [
                    1267,
                    1655,
                    2160,
                    1708
                ],
                "angle": 0,
                "content": "3.5 Generating Style-Transferred Sentences"
            },
            {
                "block_id": 29,
                "type": "text",
                "bbox": [
                    1265,
                    1729,
                    2193,
                    1953
                ],
                "angle": 0,
                "content": "A direct application of our disentangled latent space is style-transfer sentence generation, i.e., we can synthesize a sentence with generally the same meaning but a different style in the inference stage."
            },
            {
                "block_id": 30,
                "type": "text",
                "bbox": [
                    1265,
                    1957,
                    2193,
                    2287
                ],
                "angle": 0,
                "content": "Let \\( \\mathbf{x}^* \\) be an input sentence with \\( s^* \\) and \\( c^* \\) being the encoded style and content vectors, respectively. If we would like to transfer its content to a different style, we compute an empirical estimate of the target style's vector \\( \\hat{s} \\) of the training set, using"
            },
            {
                "block_id": 31,
                "type": "equation",
                "bbox": [
                    1483,
                    2287,
                    2188,
                    2409
                ],
                "angle": 0,
                "content": "\\[\n\\hat {\\boldsymbol {s}} = \\frac {\\sum_ {i \\in \\text {t a r g e t s t y l e}} \\boldsymbol {s} _ {i}}{\\# \\text {t a r g e t s t y l e s a m p l e s}} \\tag {14}\n\\]"
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    1265,
                    2427,
                    2193,
                    2595
                ],
                "angle": 0,
                "content": "The inferred target style \\(\\hat{s}\\) is concatenated with the encoded content \\(c^*\\) for decoding style-transferred sentences, as shown in Figure 1b."
            },
            {
                "block_id": 33,
                "type": "title",
                "bbox": [
                    1267,
                    2634,
                    1630,
                    2694
                ],
                "angle": 0,
                "content": "4 Experiments"
            },
            {
                "block_id": 34,
                "type": "title",
                "bbox": [
                    1267,
                    2725,
                    1550,
                    2771
                ],
                "angle": 0,
                "content": "4.1 Datasets"
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    1265,
                    2795,
                    2193,
                    3073
                ],
                "angle": 0,
                "content": "We conducted experiments on two datasets, Yelp and Amazon reviews. Both comprise sentences labeled by binary sentiment (positive or negative). They are used to train latent space disentanglement as well as to evaluate sentiment transfer."
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    1267,
                    3080,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "Yelp Service Reviews. We used the Yelp review dataset, following previous work (Shen et al.,"
            },
            {
                "block_id": 37,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "428"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    263,
                    1210,
                    547
                ],
                "angle": 0,
                "content": "2017; Zhao et al., 2018).<sup>3</sup> It contains 444101, 63483, and 126670 labeled reviews for train, validation, and test, respectively. We set the maximum length of a sentence to 15 words and the vocabulary size to \\(\\sim 9200\\), following Shen et al. (2017)."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    547,
                    1210,
                    996
                ],
                "angle": 0,
                "content": "Amazon Product Reviews. We further evaluate our model with an Amazon review dataset, following some other previous papers (Fu et al., 2018).<sup>4</sup> It contains 555142, 2000, and 2000 labeled reviews for train, validation, and test, respectively. The maximum length of a sentence is set to 20 words and the vocabulary size is \\(\\sim 58\\mathrm{k}\\), as in Fu et al. (2018)."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    287,
                    1038,
                    838,
                    1094
                ],
                "angle": 0,
                "content": "4.2 Experimental Settings"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1112,
                    1210,
                    1504
                ],
                "angle": 0,
                "content": "Our RNN has a hidden state of 256 dimensions, linearly transformed to a style space of 8 dimensions and a content space of 128 dimensions. They were chosen empirically, and we found them robust to model performance. For the decoder, we fed the latent vector \\( \\pmb{h} = [s, c] \\) to the hidden state at each step."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1511,
                    1210,
                    2409
                ],
                "angle": 0,
                "content": "We used the Adam optimizer (Kingma and Ba, 2014) for the autoencoder and the RMSProp optimizer (Tieleman and Hinton, 2012) for the discriminators, following stability tricks in adversarial training (Arjovsky et al., 2017). Each optimizer has an initial learning rate of \\(10^{-3}\\). Our model is trained for 20 epochs, by which time it has converged. The word embedding layer was initialized by word2vec (Mikolov et al., 2013) trained on respective training sets. Both the autoencoder and the discriminators are trained once per minibatch with \\(\\lambda_{\\mathrm{mul(s)}} = 10\\), \\(\\lambda_{\\mathrm{mul(c)}} = 3\\), \\(\\lambda_{\\mathrm{adv(s)}} = 1\\), and \\(\\lambda_{\\mathrm{adv(c)}} = 0.03\\). These hyperparameters were tuned by a log-scale grid search within two orders of magnitude around the default value 1; we chose the values yielding the best validation results."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2413,
                    1210,
                    2750
                ],
                "angle": 0,
                "content": "For the VAE model, the KL penalty is weighted by \\(\\lambda_{\\mathrm{kl(s)}}\\) and \\(\\lambda_{\\mathrm{kl(c)}}\\) for style and content, respectively. We set both to 0.03, tuned by the same method of log-scale grid search. During training, we also used the sigmoid KL annealing schedule, following Bahuleyan et al. (2018)."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    287,
                    2792,
                    1099,
                    2848
                ],
                "angle": 0,
                "content": "4.3 Exp. I: Disentangling Latent Space"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2866,
                    1210,
                    2978
                ],
                "angle": 0,
                "content": "First, we analyze how the style (sentiment) and content of the latent space are disentangled. We"
            },
            {
                "block_id": 8,
                "type": "table",
                "bbox": [
                    1329,
                    256,
                    2146,
                    526
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\">Latent Space</td><td colspan=\"2\">Yelp</td><td colspan=\"2\">Amazon</td></tr><tr><td>DAE</td><td>VAE</td><td>DAE</td><td>VAE</td></tr><tr><td>None (majority guess)</td><td colspan=\"2\">0.60</td><td colspan=\"2\">0.51</td></tr><tr><td>Content space (c)</td><td>0.66</td><td>0.70</td><td>0.67</td><td>0.69</td></tr><tr><td>Style space (s)</td><td>0.97</td><td>0.97</td><td>0.82</td><td>0.81</td></tr><tr><td>Complete space ([s; c])</td><td>0.97</td><td>0.97</td><td>0.82</td><td>0.81</td></tr></table>",
                "caption": "Table 1: Classification accuracy on latent spaces."
            },
            {
                "block_id": 10,
                "type": "image",
                "bbox": [
                    1359,
                    655,
                    2101,
                    1147
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 2: t-SNE plots of the disentangled style and content spaces on Yelp (with all auxiliary losses)."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1262,
                    1375,
                    2193,
                    1540
                ],
                "angle": 0,
                "content": "train separate logistic regression sentiment classifiers on different latent spaces, and report their classification accuracy in Table 1."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    1262,
                    1543,
                    2193,
                    2164
                ],
                "angle": 0,
                "content": "We see the 128-dimensional content vector \\( c \\) is not particularly discriminative for style. Its accuracy is slightly better than majority guess. However, the 8-dimensional style vector \\( s \\), despite its low dimensionality, achieves substantially higher style classification accuracy. When combining content and style vectors, we observe no further improvement. These results verify the effectiveness of our disentangling approach, as the style space contains style information, whereas the content space does not."
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    1262,
                    2167,
                    2193,
                    2616
                ],
                "angle": 0,
                "content": "We show t-SNE plots (van der Maaten and Hinton, 2008) for both DAE and VAE in Figure 2. As seen, sentences with different styles are noticeably separated in a clean manner in the style space (LHS), but are indistinguishable in the content space (RHS). It is also evident that the latent space learned by VAE is considerably smoother and more continuous than the one learned by DAE."
            },
            {
                "block_id": 15,
                "type": "title",
                "bbox": [
                    1265,
                    2662,
                    2178,
                    2715
                ],
                "angle": 0,
                "content": "4.4 Exp. II: Non-Parallel Text Style Transfer"
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    1262,
                    2739,
                    2188,
                    2848
                ],
                "angle": 0,
                "content": "In this experiment, we apply the disentangled latent space to sentiment-transfer text generation."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    1262,
                    2855,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "Metrics. We evaluate competing models based on (1) style transfer accuracy, (2) content preservation, and (3) quality of generated language. The evaluation of sentence generation has proven to be difficult in contemporary literature, so we adopt a few automatic metrics and use human judgment as"
            },
            {
                "block_id": 18,
                "type": "page_footnote",
                "bbox": [
                    287,
                    3009,
                    1210,
                    3192
                ],
                "angle": 0,
                "content": "3The Yelp dataset is available at https://github. com/shentianxiao/language-style-transfer 4The Amazon dataset is available at https:// github.com/fuzhenxin/text_style_transfer"
            },
            {
                "block_id": 19,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "429"
            }
        ]
    },
    {
        "page_id": 6,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    290,
                    266,
                    391,
                    312
                ],
                "angle": 0,
                "content": "well."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    329,
                    1215,
                    1115
                ],
                "angle": 0,
                "content": "Style-Transfer Accuracy (STA): We follow most previous work (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018) and train a separate convolutional neural network (CNN) to predict the sentiment of a sentence (Kim, 2014), which is then used to approximate the style transfer accuracy. In other words, we report the CNN classifier's accuracy on the style-transferred sentences, considering the target style to be the ground-truth. While the style classifier itself may not be perfect, it achieves a reasonable sentiment accuracy on the validation sets (97% for Yelp; 82% for Amazon). Thus, it provides a quantitative way of evaluating the strength of style transfer."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    287,
                    1126,
                    1215,
                    1518
                ],
                "angle": 0,
                "content": "Cosine Similarity (CS): We followed Fu et al. (2018) and computed the cosine measure between source and generated sentence embeddings, which are the concatenation of min, max, and mean of word embeddings (sentiment words removed). This provides a rough estimation of content preservation."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    287,
                    1529,
                    1215,
                    1978
                ],
                "angle": 0,
                "content": "Word Overlap (WO): We find that cosine similarity, although correlated to human judgment, is not a sensitive measure. Instead, we propose a simple and effective measure that counts the unigram word overlap rate of the original sentence x and the style-transferred sentence y, computed by \\(\\frac{\\text{count}(\\mathbf{x} \\cap \\mathbf{y})}{\\text{count}(\\mathbf{x} \\cup \\mathbf{y})}\\). Here, we exclude both stopwords and sentiment words."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1992,
                    1215,
                    2494
                ],
                "angle": 0,
                "content": "Perplexity (PPL): We use a trigram Kneser-Ney (KN, Kneser and Ney, 1995) language model as a quantitative and automated metric to evaluate the fluency of a sentence. It estimates the empirical distribution of trigrams in a corpus, and computes the perplexity of a test sentence. We trained the language model on the respective datasets, and report PPL on the generated sentences. A smaller PPL indicates more fluent sentences."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2508,
                    1215,
                    3125
                ],
                "angle": 0,
                "content": "Geometric Mean (GM): We use the geometric mean of STA, WO, and 1/PPL—reflecting transfer strength, content preservation, and fluency, respectively—to obtain an aggregated score considering all aspects. Notice that a smaller PPL is desired; thus, we use 1/PPL when computing GM. Also, cosine similarity (CS) is not included, because it is insensitive yet repetitive with word overlap (WO). Here, we adopt the geometric mean so that the scale of each metric does not influence the judgment."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    332,
                    3136,
                    1213,
                    3192
                ],
                "angle": 0,
                "content": "Manual Evaluation: Despite the above auto"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1265,
                    266,
                    2195,
                    1340
                ],
                "angle": 0,
                "content": "matic metrics, we also conduct human evaluations to further confirm the performance of our model. This was done on the Yelp dataset only, due to the amount of manual effort involved. We asked 6 human annotators to rate each sentence on a 1-5 Likert scale (Stent et al., 2005) in terms of transfer strength (TS), content preservation (CP), and language quality (LQ). This evaluation was conducted in a strictly blind fashion: samples obtained from all evaluated models were randomly shuffled, so that the annotator was unaware of which model generated a particular sentence. The inter-rater agreement—as measured by Krippendorff's alpha (Klaus, 2004) for our Likert scale ratings—is 0.74, 0.68, and 0.72 for these three aspects, respectively. According to Klaus (2004), this is an acceptable inter-rater agreement. We also computed the geometric mean (GM) to obtain an aggregated score."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    1350,
                    2195,
                    2417
                ],
                "angle": 0,
                "content": "Overall performance. We compare our approach with previous state-of-the-art work in Table 2. For competing methods, we quote results from existing papers whenever possible. In some studies, the authors have released their style-transferred sentences, and we tested them with our metrics. A caveat is that this may involve a different data split, providing a rough (but unbiased) comparison. For others, we re-evaluated the model using publicly available code. We sought comparison with Hu et al. (2017), but unfortunately could not find publicly available code. Instead we sought performance comparisons of their model in subsequent work, and found that, according to the human evaluation in Shen et al. (2017), Hu et al. (2017) is comparable but slightly worse than Shen et al. (2017). The latter is compared with our model in terms of both automatic metrics and human evaluation."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    2434,
                    2195,
                    2823
                ],
                "angle": 0,
                "content": "We see in Table 2 a clear trade-off between style transfer and content preservation, as they are contradictory goals. Especially, a few models have a transfer accuracy lower than \\(50\\%\\). They are shown in gray, and not the focus of the comparison, because the system cannot achieve the goal of style transfer most of the time."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    2841,
                    2195,
                    3122
                ],
                "angle": 0,
                "content": "Our method achieves high style-transfer accuracy (STA) in both experiments. On the Yelp dataset, it outperforms previous methods by more than \\(7\\%\\), whereas on Amazon, VAE is \\(1\\%\\) lower than Tsvetkov et al. (2018), ranking second."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1312,
                    3136,
                    2190,
                    3192
                ],
                "angle": 0,
                "content": "Our approach achieves high content preserva"
            },
            {
                "block_id": 12,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1285,
                    3279
                ],
                "angle": 0,
                "content": "430"
            }
        ]
    },
    {
        "page_id": 7,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    396,
                    252,
                    2101,
                    712
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"5\">Yelp Dataset</td><td colspan=\"5\">Amazon Dataset</td></tr><tr><td>\\( \\mathbf{STA}^{\\uparrow } \\)</td><td>\\( \\mathbf{CS}^{\\uparrow } \\)</td><td>\\( \\mathbf{WO}^{\\uparrow } \\)</td><td>\\( \\mathbf{PPL}^{\\downarrow } \\)</td><td>\\( \\mathbf{GM}^{\\uparrow } \\)</td><td>\\( \\mathbf{STA}^{\\uparrow } \\)</td><td>\\( \\mathbf{CS}^{\\uparrow } \\)</td><td>\\( \\mathbf{WO}^{\\uparrow } \\)</td><td>\\( \\mathbf{PPL}^{\\downarrow } \\)</td><td>\\( \\mathbf{GM}^{\\uparrow } \\)</td></tr><tr><td>Style-Embedding (Fu et al., 2018)</td><td>0.18</td><td>0.96</td><td>0.67</td><td>124</td><td>0.10</td><td>\\( 0.40^{\\dagger } \\)</td><td>\\( 0.93^{\\dagger } \\)</td><td>0.36</td><td>32</td><td>0.17</td></tr><tr><td>Cross-Alignment (Shen et al., 2017)</td><td>\\( 0.78^{\\dagger } \\)</td><td>0.89</td><td>0.21</td><td>93</td><td>0.12</td><td>0.61</td><td>0.89</td><td>0.02</td><td>202</td><td>0.04</td></tr><tr><td>Multi-Decoder (Zhao et al., 2018)</td><td>\\( 0.82^{\\dagger } \\)</td><td>0.88</td><td>0.27</td><td>85</td><td>0.14</td><td>0.55</td><td>0.93</td><td>0.17</td><td>75</td><td>0.11</td></tr><tr><td>Del-Ret-Gen (Li et al., 2018)\\( ^‡ \\)</td><td>0.86</td><td>0.94</td><td>0.52</td><td>70</td><td>0.19</td><td>0.43</td><td>0.98</td><td>0.80</td><td>65</td><td>0.17</td></tr><tr><td>BackTranslate (Tsvetkov et al., 2018)</td><td>0.85</td><td>0.83</td><td>0.08</td><td>206</td><td>0.07</td><td>0.83</td><td>0.82</td><td>0.02</td><td>115</td><td>0.05</td></tr><tr><td>Cycle-RL (Xu et al., 2018)\\( ^‡ \\)</td><td>0.80</td><td>0.92</td><td>0.43</td><td>470</td><td>0.09</td><td>0.72</td><td>0.91</td><td>0.22</td><td>332</td><td>0.08</td></tr><tr><td>Ours (DAE)</td><td>0.88</td><td>0.92</td><td>0.55</td><td>52</td><td>0.21</td><td>0.72</td><td>0.92</td><td>0.35</td><td>73</td><td>0.15</td></tr><tr><td>Ours (VAE)</td><td>0.93</td><td>0.90</td><td>0.47</td><td>32</td><td>0.24</td><td>0.82</td><td>0.90</td><td>0.20</td><td>63</td><td>0.14</td></tr></table>",
                "caption": "Table 2: Performance of text style transfer. STA: Style transfer accuracy. CS: Cosine similarity. WO: Word overlap rate. PPL: Perplexity. GM: Geometric mean. The larger\\(\\uparrow\\) (or lower\\(\\downarrow\\)), the better. \\({}^{\\dagger}\\)Quoted from previous papers (with the same data split). \\({}^{\\ddagger}\\)Involving custom data splits, providing a rough (but unbiased) comparison. Others are based on our replication, and we use published code whenever possible. We achieve 0.809 and 0.835 transfer accuracy on the Yelp dataset, close to the results in Shen et al. (2017) and Zhao et al. (2018), respectively, showing that our replication is fair. Gray numbers show that a method fails to transfer style most of the time."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    424,
                    1122,
                    1089,
                    1396
                ],
                "angle": 0,
                "content": "<table><tr><td>Model</td><td>TS</td><td>CP</td><td>LQ</td><td>GM</td></tr><tr><td>Fu et al. (2018)</td><td>1.67</td><td>3.84</td><td>3.66</td><td>2.86</td></tr><tr><td>Shen et al. (2017)</td><td>3.63</td><td>3.07</td><td>3.08</td><td>3.25</td></tr><tr><td>Zhao et al. (2018)</td><td>3.55</td><td>3.09</td><td>3.77</td><td>3.46</td></tr><tr><td>Ours (DAE)</td><td>3.67</td><td>3.64</td><td>4.19</td><td>3.83</td></tr><tr><td>Ours (VAE)</td><td>4.32</td><td>3.73</td><td>4.48</td><td>4.16</td></tr></table>",
                "caption": "Table 3: Manual evaluation on the Yelp dataset."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1571,
                    1215,
                    2020
                ],
                "angle": 0,
                "content": "tion as well. Among all the methods that can achieve more than \\(50\\%\\) transfer accuracy, DAE has the highest word overlap (WO) on Yelp; VAE is also high, although slightly lower than Li et al. (2018). On Amazon, the phenomenon is similar. DAE is the best; VAE is \\(2\\%\\) lower in WO (although \\(10\\%\\) better in transfer accuracy), compared with Xu et al. (2018)."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2027,
                    1215,
                    2525
                ],
                "angle": 0,
                "content": "For language fluency, VAE yields the best PPL in both datasets. It is also noted that, the cycle reinforcement learning (Cycle-RL) approach does not generate fluent sentences (Xu et al., 2018). They have unusually high PPL scores, but after reading the samples provided by the authors (via personal email correspondence) we are assured that the sentences obtained by Cycle-RL are less fluent."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2536,
                    1215,
                    2820
                ],
                "angle": 0,
                "content": "When we consider all the above aspects, our approach (either DAE or VAE) has the highest geometric meaning (GM), showing that we have achieved good balance on transfer strength, content preservation, as well as language fluency."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2823,
                    1215,
                    3101
                ],
                "angle": 0,
                "content": "Table 3 presents the results of human evaluation on selected methods. Again, we see that the style embedding model (Fu et al., 2018) is ineffective as it has a very low transfer strength, and that our method outperforms other baselines in all as-"
            },
            {
                "block_id": 8,
                "type": "table",
                "bbox": [
                    1324,
                    1122,
                    2153,
                    1399
                ],
                "angle": 0,
                "content": "<table><tr><td>Objectives</td><td>STA</td><td>CS</td><td>WO</td><td>PPL</td><td>GM</td></tr><tr><td>JAE</td><td>0.11</td><td>0.94</td><td>0.47</td><td>40</td><td>0.11</td></tr><tr><td>JAE, Jmul(s)</td><td>0.77</td><td>0.91</td><td>0.33</td><td>41</td><td>0.18</td></tr><tr><td>JAE, Jadv(s)</td><td>0.78</td><td>0.89</td><td>0.23</td><td>35</td><td>0.17</td></tr><tr><td>JAE, Jmul(s), Jadv(s)</td><td>0.91</td><td>0.87</td><td>0.17</td><td>23</td><td>0.19</td></tr><tr><td>JAE, Jmul(s), Jadv(s), Jmul(c), Jadv(c)</td><td>0.93</td><td>0.90</td><td>0.47</td><td>32</td><td>0.24</td></tr></table>",
                "caption": "Table 4: Ablation tests on Yelp. In all variants, we follow the same protocol of style transfer by substituting an empirical estimate of the target style vector."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    1697,
                    2195,
                    1985
                ],
                "angle": 0,
                "content": "pects. The results are consistent with Table 2. This also implies that the automatic metrics we used are reasonable, and could be extrapolated to different models; it also shows consistent evidence of the effectiveness of our approach."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    1996,
                    2198,
                    2901
                ],
                "angle": 0,
                "content": "Ablation Test. We conducted ablation tests on the Yelp dataset, and show results in Table 4. With \\( J_{\\mathrm{AE}} \\) only, we cannot achieve reasonable style transfer accuracy by substituting an empirically estimated style vector of the target style. This is because the style and content spaces would not be disentangled spontaneously with the autoencoding loss alone. With either \\( J_{\\mathrm{mul(s)}} \\) or \\( J_{\\mathrm{adv(s)}} \\), the model achieves reasonable transfer accuracy and cosine similarity. Combining them together improves the transfer accuracy to \\( 90\\% \\), outperforming previous methods by a margin of \\( 5\\% \\) (Table 2). This shows that the multi-task loss and the adversarial loss work in different ways. Our insight of combining the two auxiliary losses is a simple yet effective way of disentangling latent space."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    2911,
                    2195,
                    3195
                ],
                "angle": 0,
                "content": "On the other hand, \\( J_{\\mathrm{mul(s)}} \\) and \\( J_{\\mathrm{adv(s)}} \\) only regularize the style information, leading to gradual drop of content preserving scores. Then, we use another insight of introducing content-oriented auxiliary losses, \\( J_{\\mathrm{mul(c)}} \\) and \\( J_{\\mathrm{adv(c)}} \\), based on BoW"
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    342,
                    3139,
                    1071,
                    3192
                ],
                "angle": 0,
                "content": "\\( {}^{5} \\) Selection was based on the time of availability."
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1277,
                    3279
                ],
                "angle": 0,
                "content": "431"
            }
        ]
    },
    {
        "page_id": 8,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    266,
                    1215,
                    550
                ],
                "angle": 0,
                "content": "features, which regularize the content information in the same way as style. By incorporating all these auxiliary losses, we achieve high transfer accuracy, high content preservation, as well as high language fluency."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    592,
                    1004,
                    648
                ],
                "angle": 0,
                "content": "5 Conclusion and Future Work"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    687,
                    1218,
                    1080
                ],
                "angle": 0,
                "content": "In this paper, we propose an effective approach for disentangling style and content latent spaces. We systematically combine multi-task and adversarial objectives to separate content and style from each other, where we also propose to approximate content information with bag-of-words features of style-neutral, non-stopword vocabulary."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1083,
                    1215,
                    1476
                ],
                "angle": 0,
                "content": "Both qualitative and quantitative experiments show that the latent space is indeed separated into style and content parts. The disentangled space can be directly applied to text style-transfer tasks. Our method achieves high style-transfer strength, high content-preservation scores, as well as high language fluency, compared with previous work."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1480,
                    1215,
                    1929
                ],
                "angle": 0,
                "content": "Our approach can be naturally extended to noncategorical styles, because our style feature is encoded from the input sentence. Non-categorical styles cannot be easily handled by fixed style embeddings or style-specific decoders (Fu et al., 2018). Bao et al. (2019) have successfully shown that the syntax and semantics of a sentence can be disentangled from each other."
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    287,
                    1982,
                    697,
                    2038
                ],
                "angle": 0,
                "content": "Acknowledgments"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2073,
                    1215,
                    2409
                ],
                "angle": 0,
                "content": "We thank all reviewers for insightful comments. This work was supported in part by the NSERC grant RGPIN-261439-2013 and an Amazon Research Award. We would also like to acknowledge NVIDIA Corporation for the donated Titan Xp GPU."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    290,
                    2508,
                    535,
                    2560
                ],
                "angle": 0,
                "content": "References"
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    290,
                    2592,
                    1210,
                    2736
                ],
                "angle": 0,
                "content": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. 2017. Wasserstein generative adversarial networks. In ICML, pages 214-223."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    290,
                    2774,
                    1215,
                    2960
                ],
                "angle": 0,
                "content": "Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. 2018. Variational attention for sequence-to-sequence models. In COLING, pages 1672-1682."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    290,
                    3002,
                    1215,
                    3185
                ],
                "angle": 0,
                "content": "Georgios Balikas, Simon Moura, and Massih-Reza Amini. 2017. Multitask learning for fine-grained twitter sentiment analysis. In SIGIR, pages 1005-1008."
            },
            {
                "block_id": 11,
                "type": "list",
                "bbox": [
                    290,
                    2592,
                    1215,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    1272,
                    270,
                    2195,
                    456
                ],
                "angle": 0,
                "content": "Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, XIN-YU DAI, and Jiajun CHEN. 2019. Generating sentences from disentangled syntactic and semantic spaces. In ACL."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1272,
                    491,
                    2195,
                    677
                ],
                "angle": 0,
                "content": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. 2016. Generating sentences from a continuous space. In CoNLL, pages 10-21."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1272,
                    712,
                    2195,
                    943
                ],
                "angle": 0,
                "content": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS, pages 2172-2180."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1272,
                    978,
                    2195,
                    1252
                ],
                "angle": 0,
                "content": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, pages 1724-1734."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1287,
                    2195,
                    1431
                ],
                "angle": 0,
                "content": "Alexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018. Word translation without parallel data. In ICLR."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1466,
                    2195,
                    1648
                ],
                "angle": 0,
                "content": "Jessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language generation. In Proc. Workshop on Stylistic Variation, pages 94-104."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1687,
                    2195,
                    1827
                ],
                "angle": 0,
                "content": "Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018. Style transfer in text: Exploration and evaluation. In AAAI, pages 663-670."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1859,
                    2195,
                    2003
                ],
                "angle": 0,
                "content": "Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 2016. Image style transfer using convolutional neural networks. In CVPR, pages 2414-2423."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2034,
                    2195,
                    2266
                ],
                "angle": 0,
                "content": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. BetavAE: Learning basic visual concepts with a constrained variational framework. In ICLR."
            },
            {
                "block_id": 21,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2301,
                    2195,
                    2399
                ],
                "angle": 0,
                "content": "Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In KDD, pages 168-177."
            },
            {
                "block_id": 22,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2431,
                    2195,
                    2613
                ],
                "angle": 0,
                "content": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017. Toward controlled generation of text. In ICML, pages 1587-1596."
            },
            {
                "block_id": 23,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2652,
                    2195,
                    2834
                ],
                "angle": 0,
                "content": "Yacine Jernite, Samuel R. Bowman, and David Sontag. 2017. Discourse-based objectives for fast unsupervised sentence representation learning. arXiv, abs/1705.00557."
            },
            {
                "block_id": 24,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2873,
                    2195,
                    3009
                ],
                "angle": 0,
                "content": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP, pages 1746-1751."
            },
            {
                "block_id": 25,
                "type": "ref_text",
                "bbox": [
                    1272,
                    3048,
                    2195,
                    3185
                ],
                "angle": 0,
                "content": "Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980."
            },
            {
                "block_id": 26,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2195,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 27,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1282,
                    3276
                ],
                "angle": 0,
                "content": "432"
            }
        ]
    },
    {
        "page_id": 9,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    292,
                    270,
                    1213,
                    406
                ],
                "angle": 0,
                "content": "Diederik P. Kingma and Max Welling. 2013. Autoencoding variational Bayes. arXiv preprint arXiv:1312.6114."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    292,
                    445,
                    1208,
                    540
                ],
                "angle": 0,
                "content": "Krippendorff Klaus. 2004. Content Analysis: An Introduction to Its Methodology. Sage Publications."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    295,
                    575,
                    1210,
                    712
                ],
                "angle": 0,
                "content": "Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In ICASSP, pages 181-184."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    295,
                    750,
                    1210,
                    887
                ],
                "angle": 0,
                "content": "Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The Annals of Mathematical Statistics, 22(1):79-86."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    295,
                    926,
                    1210,
                    1108
                ],
                "angle": 0,
                "content": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual corpora only. In ICLR."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    295,
                    1147,
                    1210,
                    1333
                ],
                "angle": 0,
                "content": "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine translation. In EMNLP, pages 5039-5049."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    295,
                    1368,
                    1210,
                    1550
                ],
                "angle": 0,
                "content": "Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: A simple approach to sentiment and style transfer. In *NAACL-HLT*, pages 1865–1874."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    295,
                    1589,
                    1210,
                    1729
                ],
                "angle": 0,
                "content": "Lajanugen Logeswaran, Honglak Lee, and Samy Bengio. 2018. Content preserving text generation with attribute controls. In NIPS, pages 5108-5118."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    295,
                    1764,
                    1208,
                    1859
                ],
                "angle": 0,
                "content": "Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. JMLR, 9:2579-2605."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    295,
                    1894,
                    1210,
                    2080
                ],
                "angle": 0,
                "content": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111-3119."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    295,
                    2118,
                    1210,
                    2304
                ],
                "angle": 0,
                "content": "Sudha Rao and Joel R. Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In NAACL-HLT, pages 129-140."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    295,
                    2339,
                    1210,
                    2567
                ],
                "angle": 0,
                "content": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1, pages 318-362."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    295,
                    2606,
                    1210,
                    2792
                ],
                "angle": 0,
                "content": "Cicero Nogueira dos Santos, Igor Melnyk, and Inkit Padhi. 2018. Fighting offensive language on social media with unsupervised text style transfer. In ACL (Short Papers), pages 189-194."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    295,
                    2827,
                    1210,
                    2967
                ],
                "angle": 0,
                "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2017. Style transfer from non-parallel text by cross-alignment. In NIPS, pages 6833–6844."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    295,
                    3002,
                    1210,
                    3185
                ],
                "angle": 0,
                "content": "Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In CICling, pages 341-351."
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    292,
                    270,
                    1213,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1272,
                    270,
                    2190,
                    456
                ],
                "angle": 0,
                "content": "Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1272,
                    487,
                    2190,
                    631
                ],
                "angle": 0,
                "content": "Yulia Tsvetkov, Alan W. Black, Ruslan Salakhutdinov, and Shrimai Prabhumoye. 2018. Style transfer through back-translation. In ACL, pages 866-876."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1272,
                    663,
                    2190,
                    894
                ],
                "angle": 0,
                "content": "Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xuancheng Ren, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach. In ACL, pages 979-988."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1272,
                    929,
                    2190,
                    1069
                ],
                "angle": 0,
                "content": "Ye Zhang, Nan Ding, and Radu Soricut. 2018. SHAPED: Shared-private encoder-decoder for text style adaptation. In *NAACL-HLT*, pages 1528-1538."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1101,
                    2190,
                    1283
                ],
                "angle": 0,
                "content": "Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun. 2018. Adversarily regularized autoencoders. In ICML, pages 5897-5906."
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2190,
                    1283
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "title",
                "bbox": [
                    1272,
                    1333,
                    1823,
                    1392
                ],
                "angle": 0,
                "content": "A Qualitative Examples"
            },
            {
                "block_id": 23,
                "type": "text",
                "bbox": [
                    1270,
                    1424,
                    2190,
                    1641
                ],
                "angle": 0,
                "content": "Table 5 provides several examples of our style-transfer model. Results show that we can successfully transfer the sentiment while preserving the content of a sentence."
            },
            {
                "block_id": 24,
                "type": "title",
                "bbox": [
                    1272,
                    1687,
                    1999,
                    1746
                ],
                "angle": 0,
                "content": "B Effect of the BoW Vocabulary"
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    1270,
                    1778,
                    2190,
                    2115
                ],
                "angle": 0,
                "content": "Table 6 demonstrates the effect of choosing different BoW vocabulary for the auxiliary content losses. As seen, we are able to achieve reasonable performance with any of these vocabularies, but using a vocabulary that excludes sentiment words and stopwords performs the best."
            },
            {
                "block_id": 26,
                "type": "page_number",
                "bbox": [
                    1205,
                    3234,
                    1280,
                    3279
                ],
                "angle": 0,
                "content": "433"
            }
        ]
    },
    {
        "page_id": 10,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    302,
                    515,
                    2183,
                    1876
                ],
                "angle": 0,
                "content": "<table><tr><td>Original (Positive)</td><td>DAE Transferred (Negative)</td><td>VAE Transferred (Negative)</td></tr><tr><td>the food is excellent and the service is exceptional</td><td>the food was a bit bad but the staff was exceptional</td><td>the food was bland and i am not thrilled with this</td></tr><tr><td>the waitresses are friendly and helpful</td><td>the guys are rude and helpful</td><td>the waitresses are rude and are lazy</td></tr><tr><td>the restaurant itself is romantic and quiet</td><td>the restaurant itself is awkward and quite crowded</td><td>the restaurant itself was dirty</td></tr><tr><td>great deal</td><td>horrible deal</td><td>no deal</td></tr><tr><td>both times i have eaten the lunch buffet and it was outstanding</td><td>their burgers were decent but the eggs were not the consistency</td><td>both times i have eaten here the food was mediocre at best</td></tr><tr><td>Original (Negative)</td><td>DAE Transferred (Positive)</td><td>VAE Transferred (Positive)</td></tr><tr><td>the desserts were very bland</td><td>the desserts were very good</td><td>the desserts were very good</td></tr><tr><td>it was a bed of lettuce and spinach with some Italian meats and cheeses</td><td>it was a beautiful setting and just had a large variety of german flavors</td><td>it was a huge assortment of flavors and Italian food</td></tr><tr><td>the people behind the counter were not friendly whatsoever</td><td>the best selection behind the register and service presentation</td><td>the people behind the counter is friendly caring</td></tr><tr><td>the interior is old and generally falling apart</td><td>the decor is old and now perfectly</td><td>the interior is old and noble</td></tr><tr><td>they are clueless</td><td>they are stoked</td><td>they are genuinely profession-als</td></tr></table>",
                "caption": "Table 5: Examples of style transferred sentence generation."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    367,
                    2515,
                    2121,
                    2830
                ],
                "angle": 0,
                "content": "<table><tr><td>BoW Vocabulary</td><td>STA</td><td>CS</td><td>WO</td><td>PPL</td><td>GM</td></tr><tr><td>Full corpus vocabulary</td><td>0.822</td><td>0.896</td><td>0.344</td><td>30</td><td>0.21</td></tr><tr><td>Vocabulary without sentiment words</td><td>0.872</td><td>0.901</td><td>0.359</td><td>30</td><td>0.22</td></tr><tr><td>Vocabulary without stopwords</td><td>0.836</td><td>0.894</td><td>0.429</td><td>33</td><td>0.22</td></tr><tr><td>Vocabulary without stopwords and sentiment words</td><td>0.934</td><td>0.904</td><td>0.473</td><td>32</td><td>0.24</td></tr></table>",
                "caption": "Table 6: Analysis of the BoW vocabulary."
            },
            {
                "block_id": 4,
                "type": "page_number",
                "bbox": [
                    1205,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "434"
            }
        ]
    }
]