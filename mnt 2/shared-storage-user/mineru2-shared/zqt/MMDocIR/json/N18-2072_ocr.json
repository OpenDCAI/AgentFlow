[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    483,
                    287,
                    1997,
                    420
                ],
                "angle": 0,
                "content": "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    925,
                    536,
                    1567,
                    715
                ],
                "angle": 0,
                "content": "Sosuke Kobayashi  \nPreferred Networks, Inc., Japan  \nsosk@preferred.jp"
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    654,
                    929,
                    846,
                    978
                ],
                "angle": 0,
                "content": "Abstract"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    357,
                    1031,
                    1143,
                    1975
                ],
                "angle": 0,
                "content": "We propose a novel data augmentation for labeled sentences called contextual augmentation. We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions. Words predicted according to a context are numerous but appropriate for the augmentation of the original words. Furthermore, we retrofit a language model with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves classifiers based on the convolutional or recurrent neural networks."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    290,
                    2027,
                    650,
                    2076
                ],
                "angle": 0,
                "content": "1 Introduction"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2118,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "Neural network-based models for NLP have been growing with state-of-the-art results in various tasks, e.g., dependency parsing (Dyer et al., 2015), text classification (Socher et al., 2013; Kim, 2014), machine translation (Sutskever et al., 2014). However, machine learning models often overfit the training data by losing their generalization. Generalization performance highly depends on the size and quality of the training data and regularizations. Preparing a large annotated dataset is very time-consuming. Instead, automatic data augmentation is popular, particularly in the areas of vision (Simard et al., 1998; Krizhevsky et al., 2012; Szegedy et al., 2015) and speech (Jaitly and Hinton, 2015; Ko et al., 2015). Data augmentation is basically performed based on human knowledge on invariances, rules, or heuristics, e.g., “even if a picture is flipped, the class of an object should be unchanged”."
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    1287,
                    922,
                    2178,
                    1799
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 1: Contextual augmentation with a bidirectional RNN language model, when a sentence \"the actors are fantastic\" is augmented by replacing only actors with words predicted based on the context."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1262,
                    2171,
                    2193,
                    3129
                ],
                "angle": 0,
                "content": "However, usage of data augmentation for NLP has been limited. In natural languages, it is very difficult to obtain universal rules for transformations which assure the quality of the produced data and are easy to apply automatically in various domains. A common approach for such a transformation is to replace words with their synonyms selected from a handcrafted ontology such as WordNet (Miller, 1995; Zhang et al., 2015) or word similarity calculation (Wang and Yang, 2015). Because words having exactly or nearly the same meanings are very few, synonym-based augmentation can be applied to only a small percentage of the vocabulary. Other augmentation methods are known but are often developed for specific domains with handcrafted rules or pipelines, with the loss of generality."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1312,
                    3136,
                    2190,
                    3192
                ],
                "angle": 0,
                "content": "In this paper, we propose a novel data aug-"
            },
            {
                "block_id": 10,
                "type": "page_number",
                "bbox": [
                    1200,
                    3230,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "452"
            },
            {
                "block_id": 11,
                "type": "footer",
                "bbox": [
                    850,
                    3301,
                    1620,
                    3346
                ],
                "angle": 0,
                "content": "Proceedings of NAACL-HLT 2018, pages 452-457"
            },
            {
                "block_id": 12,
                "type": "footer",
                "bbox": [
                    530,
                    3350,
                    1942,
                    3399
                ],
                "angle": 0,
                "content": "New Orleans, Louisiana, June 1 - 6, 2018. ©2018 Association for Computational Linguistics"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    266,
                    1223,
                    1280
                ],
                "angle": 0,
                "content": "mentation method called contextual augmentation. Our method offers a wider range of substitute words by using words predicted by a bidirectional language model (LM) according to the context, as shown in Figure 1. This contextual prediction suggests various words that have paradigmatic relations (Saussure and Riedlinger, 1916) with the original words. Such words can also be good substitutes for augmentation. Furthermore, to prevent word replacement that is incompatible with the annotated labels of the original sentences, we retrofit the LM with a label-conditional architecture. Through the experiment, we demonstrate that the proposed conditional LM produces good words for augmentation, and contextual augmentation improves classifiers using recurrent or convolutional neural networks (RNN or CNN) in various classification tasks."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    1326,
                    761,
                    1389
                ],
                "angle": 0,
                "content": "2 Proposed Method"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    1420,
                    1215,
                    2153
                ],
                "angle": 0,
                "content": "For performing data augmentation by replacing words in a text with other words, prior works (Zhang et al., 2015; Wang and Yang, 2015) used synonyms as substitute words for the original words. However, synonyms are very limited and the synonym-based augmentation cannot produce numerous different patterns from the original texts. We propose contextual augmentation, a novel method to augment words with more varied words. Instead of the synonyms, we use words that are predicted by a LM given the context surrounding the original words to be augmented, as shown in Figure 1."
            },
            {
                "block_id": 3,
                "type": "title",
                "bbox": [
                    287,
                    2199,
                    622,
                    2252
                ],
                "angle": 0,
                "content": "2.1 Motivation"
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    2273,
                    1215,
                    3069
                ],
                "angle": 0,
                "content": "First, we explain the motivation of our proposed method by referring to an example with a sentence from the Stanford Sentiment Treebank (SST) (Socher et al., 2013), which is a dataset of sentiment-labeled movie reviews. The sentence, \"the actors are fantastic.\", is annotated with a positive label. When augmentation is performed for the word (position) \"actors\", how widely can we augment it? According to the prior works, we can use words from a synset for the word actor obtained from WordNet (histrion, player, thespian, and role_player). The synset contains words that have meanings similar to the word actor on average. However, for data augmentation, the word"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    1265,
                    266,
                    2195,
                    603
                ],
                "angle": 0,
                "content": "actors can be further replaced with non-synonym words such as characters, movies, stories, and songs or various other nouns, while retaining the positive sentiment and naturalness. Considering the generalization, training with maximum patterns will boost the model performance more."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    1265,
                    606,
                    2195,
                    943
                ],
                "angle": 0,
                "content": "We propose using numerous words that have the paradigmatic relations with the original words. A LM has the desirable property to assign high probabilities to such words, even if the words themselves are not similar to the original word to be replaced."
            },
            {
                "block_id": 7,
                "type": "title",
                "bbox": [
                    1267,
                    992,
                    2059,
                    1045
                ],
                "angle": 0,
                "content": "2.2 Word Prediction based on Context"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1262,
                    1069,
                    2195,
                    2031
                ],
                "angle": 0,
                "content": "For our proposed method, we requires a LM for calculating the word probability at a position \\( i \\) based on its context. The context is a sequence of words surrounding an original word \\( w_{i} \\) in a sentence \\( S \\), i.e., cloze sentence \\( S \\setminus \\{w_{i}\\} \\). The calculated probability is \\( p(\\cdot | S \\setminus \\{w_{i}\\}) \\). Specifically, we use a bi-directional LSTM-RNN (Hochreiter and Schmidhuber, 1997) LM. For prediction at position \\( i \\), the model encodes the surrounding words individually rightward and leftward (see Figure 1). As well as typical uni-directional RNN LMs, the outputs from adjacent positions are used for calculating the probability at target position \\( i \\). The outputs from both the directions are concatenated and fed into the following feed-forward neural network, which produces words with a probability distribution over the vocabulary."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1262,
                    2034,
                    2200,
                    2939
                ],
                "angle": 0,
                "content": "In contextual augmentation, new substitutes for word \\( w_{i} \\) can be smoothly sampled from a given probability distribution, \\( p(\\cdot | S \\backslash \\{w_{i}\\}) \\), while prior works selected top-K words conclusively. In this study, we sample words for augmentation at each update during the training of a model. To control the strength of augmentation, we introduce temperature parameter \\( \\tau \\) and use an annealed distribution \\( p_{\\tau}(\\cdot | S \\backslash \\{w_{i}\\}) \\propto p(\\cdot | S \\backslash \\{w_{i}\\})^{1 / \\tau} \\). If the temperature becomes infinity (\\( \\tau \\to \\infty \\)), the words are sampled from a uniform distribution. If it becomes zero (\\( \\tau \\to 0 \\)), the augmentation words are always words predicted with the highest probability. The sampled words can be obtained at one time at each word position in the sentences. We replace each word simultaneously with a probability"
            },
            {
                "block_id": 10,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    2971,
                    2193,
                    3058
                ],
                "angle": 0,
                "content": "based approach further requires word sense disambiguation or some rules for selecting ideal synsets."
            },
            {
                "block_id": 11,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    3058,
                    2195,
                    3192
                ],
                "angle": 0,
                "content": "\\(^{2}\\) Bengio et al. (2015) reported that stochastic replacements with uniformly sampled words improved a neural encoder-decoder model for image captioning."
            },
            {
                "block_id": 12,
                "type": "list",
                "bbox": [
                    1265,
                    2971,
                    2195,
                    3192
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    287,
                    3097,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "1 Actually, the word actor has another synset containing other words such as doer and worker. Thus, this synonym-"
            },
            {
                "block_id": 14,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1282,
                    3279
                ],
                "angle": 0,
                "content": "453"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    287,
                    266,
                    1168,
                    319
                ],
                "angle": 0,
                "content": "as well as Wang and Yang (2015) for efficiency."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    357,
                    855,
                    410
                ],
                "angle": 0,
                "content": "2.3 Conditional Constraint"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    434,
                    1215,
                    1280
                ],
                "angle": 0,
                "content": "Finally, we introduce a novel approach to address the issue that context-aware augmentation is not always compatible with annotated labels. For understanding the issue, again, consider the example, \"the actors are fantastic,\" which is annotated with a positive label. If contextual augmentation, as described so far, is simply performed for the word (position of) fantastic, a LM often assigns high probabilities to words such as bad or terrible as well as good or entertaining, although they are mutually contradictory to the annotated labels of positive or negative. Thus, such a simple augmentation can possibly generate sentences that are implausible with respect to their original labels and harmful for model training."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    1280,
                    1215,
                    2010
                ],
                "angle": 0,
                "content": "To address this issue, we introduce a conditional constraint that controls the replacement of words to prevent the generated words from reversing the information related to the labels of the sentences. We alter a LM to a label-conditional LM, i.e., for position \\(i\\) in sentence \\(S\\) with label \\(y\\), we aim to calculate \\(p_{\\tau}(\\cdot | y, S \\backslash \\{w_i\\})\\) instead of the default \\(p_{\\tau}(\\cdot | S \\backslash \\{w_i\\})\\) within the model. Specifically, we concatenate each embedded label \\(y\\) with a hidden layer of the feed-forward network in the bidirectional LM, so that the output is calculated from a mixture of information from both the label and context."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    287,
                    2059,
                    630,
                    2115
                ],
                "angle": 0,
                "content": "3 Experiment"
            },
            {
                "block_id": 5,
                "type": "title",
                "bbox": [
                    287,
                    2146,
                    560,
                    2199
                ],
                "angle": 0,
                "content": "3.1 Settings"
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2220,
                    1213,
                    2494
                ],
                "angle": 0,
                "content": "We tested combinations of three augmentation methods for two types of neural models through six text classification tasks. The corresponding code is implemented by Chainer (Tokui et al., 2015) and available<sup>3</sup>."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2504,
                    1215,
                    3069
                ],
                "angle": 0,
                "content": "The benchmark datasets used are as follows: (1, 2) SST is a dataset for sentiment classification on movie reviews, which were annotated with five or two labels (SST5, SST2) (Socher et al., 2013). (3) Subjectivity dataset (Subj) was annotated with whether a sentence was subjective or objective (Pang and Lee, 2004). (4) MPQA is an opinion polarity detection dataset of short phrases rather than sentences (Wiebe et al., 2005). (5) RT is another movie review sentiment dataset (Pang"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    263,
                    2195,
                    540
                ],
                "angle": 0,
                "content": "and Lee, 2005). (6) TREC is a dataset for classification of the six question types (e.g., person, location) (Li and Roth, 2002). For a dataset without development data, we use \\(10\\%\\) of its training set for the validation set as well as Kim (2014)."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    547,
                    2195,
                    940
                ],
                "angle": 0,
                "content": "We tested classifiers using the LSTM-RNN or CNN, and both have exhibited good performances. We used typical architectures of classifiers based on the LSTM or CNN with dropout using hyperparameters found in preliminary experiments. The reported accuracies of the models were averaged over eight models trained from different seeds."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1265,
                    947,
                    2195,
                    1789
                ],
                "angle": 0,
                "content": "The tested augmentation methods are: (1) synonym-based augmentation, and (2, 3) contextual augmentation with or without a label-conditional architecture. The hyperparameters of the augmentation (temperature \\(\\tau\\) and probability of word replacement) were also selected by a grid-search using validation set, while retaining the hyperparameters of the models. For contextual augmentation, we first pretrained a bi-directional LSTM LM without the label-conditional architecture, on WikiText-103 corpus (Merit et al., 2017) from a subset of English Wikipedia articles. After the pretraining, the models are further trained on each labeled dataset with newly introduced label-conditional architectures."
            },
            {
                "block_id": 11,
                "type": "title",
                "bbox": [
                    1267,
                    1838,
                    1528,
                    1887
                ],
                "angle": 0,
                "content": "3.2 Results"
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    1915,
                    2193,
                    2476
                ],
                "angle": 0,
                "content": "Table 1 lists the accuracies of the models with or without augmentation. The results show that our contextual augmentation improves the model performances for various datasets from different domains more significantly than the prior synonym-based augmentation does. Furthermore, our label-conditional architecture boosted the performances on average and achieved the best accuracies. Our methods are effective even for datasets with more than two types of labels, SST5 and TREC."
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    2518,
                    2190,
                    2936
                ],
                "angle": 0,
                "content": "An RNN-based classifier has a single layer LSTM and word embeddings, whose output is fed into an output affine layer with the softmax function. A CNN-based classifier has convolutional filters of size \\(\\{3, 4, 5\\}\\) and word embeddings (Kim, 2014). The concatenated output of all the filters are applied with a max-pooling over time and fed into a two-layer feed-forward network with ReLU, followed by the softmax function. For both the architectures, training was performed by Adam and finished by early stopping with validation at each epoch."
            },
            {
                "block_id": 14,
                "type": "page_footnote",
                "bbox": [
                    1265,
                    2939,
                    2193,
                    3188
                ],
                "angle": 0,
                "content": "The hyperparameters of the models and training were selected by a grid-search using baseline models without data augmentation in each task's validation set individually. We used the best settings from the combinations by changing the learning rate, unit or filter size, embedding dimension, and dropout ratio."
            },
            {
                "block_id": 15,
                "type": "list",
                "bbox": [
                    1265,
                    2518,
                    2193,
                    3188
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 16,
                "type": "page_footnote",
                "bbox": [
                    287,
                    3097,
                    1131,
                    3188
                ],
                "angle": 0,
                "content": "3https://github.com/pfnet-research/contextual_augmentation"
            },
            {
                "block_id": 17,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "454"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 1,
                "type": "table",
                "bbox": [
                    302,
                    298,
                    1200,
                    648
                ],
                "angle": 0,
                "content": "<table><tr><td>CNN</td><td>41.3</td><td>79.5</td><td>92.4</td><td>86.1</td><td>75.9</td><td>90.0</td><td>77.53</td></tr><tr><td>w/ synonym</td><td>40.7</td><td>80.0</td><td>92.4</td><td>86.3</td><td>76.0</td><td>89.6</td><td>77.50</td></tr><tr><td>w/ context</td><td>41.9</td><td>80.9</td><td>92.7</td><td>86.7</td><td>75.9</td><td>90.0</td><td>78.02</td></tr><tr><td>+ label</td><td>42.1</td><td>80.8</td><td>93.0</td><td>86.7</td><td>76.1</td><td>90.5</td><td>78.20</td></tr><tr><td>RNN</td><td>40.2</td><td>80.3</td><td>92.4</td><td>86.0</td><td>76.7</td><td>89.0</td><td>77.43</td></tr><tr><td>w/ synonym</td><td>40.5</td><td>80.2</td><td>92.8</td><td>86.4</td><td>76.6</td><td>87.9</td><td>77.40</td></tr><tr><td>w/ context</td><td>40.9</td><td>79.3</td><td>92.8</td><td>86.4</td><td>77.0</td><td>89.3</td><td>77.62</td></tr><tr><td>+ label</td><td>41.1</td><td>80.1</td><td>92.8</td><td>86.4</td><td>77.4</td><td>89.2</td><td>77.83</td></tr></table>",
                "caption": "Table 1: Accuracies of the models for various benchmarks. The accuracies are averaged over eight models trained from different seeds."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    908,
                    1215,
                    2143
                ],
                "angle": 0,
                "content": "For investigating our label-conditional bidirectional LM, we show in Figure 2 the top-10 word predictions by the model for a sentence from the SST dataset. Each word in the sentence is frequently replaced with various words that are not always synonyms. We present two types of predictions depending on the label fed into the conditional LM. With a positive label, the word \"fantastic\" is frequently replaced with funny, honest, good, and entertaining, which are also positive expressions. In contrast, with a negative label, the word \"fantastic\" is frequently replaced with tired, forgettable, bad, and dull, which reflect a negative sentiment. At another position, the word \"the\" can be replaced with \"no\" (with the seventh highest probability), so that the whole sentence becomes \"no actors are fantastic,\" which seems negative as a whole. Aside from such inversions caused by labels, the parts unrelated to the labels (e.g., \"actors\") are not very different in the positive or negative predictions. These results also demonstrated that conditional architectures are effective."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    287,
                    2196,
                    677,
                    2252
                ],
                "angle": 0,
                "content": "4 Related Work"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    2290,
                    1215,
                    3192
                ],
                "angle": 0,
                "content": "Some works tried text data augmentation by using synonym lists (Zhang et al., 2015; Wang and Yang, 2015), grammar induction (Jia and Liang, 2016), task-specific heuristic rules (Fürstenau and Lapata, 2009; Kafle et al., 2017; Silfverberg et al., 2017), or neural decoders of autoencoders (Bergmanis et al., 2017; Xu et al., 2017; Hu et al., 2017) or encoder-decoder models (Kim and Rush, 2016; Sennrich et al., 2016; Xia et al., 2017). The works most similar to our research are Kolomiyets et al. (2011) and Fadaee et al. (2017). In a task of time expression recognition, Kolomiyets et al. replaced only the headwords under a task-specific assumption that temporal trigger words usually occur as headwords. They selected substitute words with top-K scores"
            },
            {
                "block_id": 6,
                "type": "image",
                "bbox": [
                    1285,
                    256,
                    2163,
                    1105
                ],
                "angle": 0,
                "content": null,
                "caption": "Figure 2: Words predicted with the ten highest probabilities by the conditional bi-directional LM applied to the sentence \"the actors are fantastic\". The squares above the sentence list the words predicted with a positive label. The squares below list the words predicted with a negative label."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1262,
                    1596,
                    2195,
                    2666
                ],
                "angle": 0,
                "content": "given by the Latent Words LM (Deschacht and Moens, 2009), which is a LM based on fixed-length contexts. Fadaee et al. (2017), focusing on the rare word problem in machine translation, replaced words in a source sentence with only rare words, which both of rightward and leftward LSTM LMs independently predict with top-K confidences. A word in the translated sentence is also replaced using a word alignment method and a rightward LM. These two works share the idea of the usage of language models with our method. We used a bi-directional LSTM LM which captures variable-length contexts with considering both the directions jointly. More importantly, we proposed a label-conditional architecture and demonstrated its effect both qualitatively and quantitatively. Our method is independent of any task-specific knowledge, and effective for classification tasks in various domains."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    2683,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "We use a label-conditional fill-in-the-blank context for data augmentation. Neural models using the fill-in-the-blank context have been invested in other applications. Kobayashi et al. (2016, 2017) proposed to extract and organize information about each entity in a discourse using the context. Fedus et al. (2018) proposed GAN (Goodfellow et al., 2014) for text generation and demonstrated that the mode collapse and training insta"
            },
            {
                "block_id": 10,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3279
                ],
                "angle": 0,
                "content": "455"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    287,
                    266,
                    1161,
                    319
                ],
                "angle": 0,
                "content": "bility can be relieved by in-filling-task training."
            },
            {
                "block_id": 1,
                "type": "title",
                "bbox": [
                    287,
                    361,
                    615,
                    413
                ],
                "angle": 0,
                "content": "5 Conclusion"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    452,
                    1210,
                    1006
                ],
                "angle": 0,
                "content": "We proposed a novel data augmentation using numerous words given by a bi-directional LM, and further introduced a label-conditional architecture into the LM. Experimentally, our method produced various words compatibly with the labels of original texts and improved neural classifiers more than the synonym-based augmentation. Our method is independent of any task-specific knowledge or rules, and can be generally and easily used for classification tasks in various domains."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    287,
                    1017,
                    1213,
                    1294
                ],
                "angle": 0,
                "content": "On the other hand, the improvement by our method is sometimes marginal. Future work will explore comparison and combination with other generalization methods exploiting datasets deeply as well as our method."
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    290,
                    1336,
                    692,
                    1392
                ],
                "angle": 0,
                "content": "Acknowledgments"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    287,
                    1424,
                    1213,
                    1652
                ],
                "angle": 0,
                "content": "I would like to thank the members of Preferred Networks, Inc., especially Takeru Miyato and Yuta Tsuboi, for helpful comments. I would also like to thank anonymous reviewers for helpful comments."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    290,
                    1743,
                    533,
                    1796
                ],
                "angle": 0,
                "content": "References"
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    292,
                    1824,
                    1210,
                    2013
                ],
                "angle": 0,
                "content": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS, pages 1171-1179."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    292,
                    2045,
                    1210,
                    2231
                ],
                "angle": 0,
                "content": "Toms Bergmanis, Katharina Kann, Hinrich Schütze, and Sharon Goldwater. 2017. Training data augmentation for low-resource morphological inflection. In CoNLL SIGMORPHON, pages 31-39."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    292,
                    2262,
                    1210,
                    2441
                ],
                "angle": 0,
                "content": "Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the latent words language model. In EMNLP, pages 21-29."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    292,
                    2480,
                    1210,
                    2669
                ],
                "angle": 0,
                "content": "Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition-based dependency parsing with stack long short-term memory. In ACL, pages 334-343."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    292,
                    2701,
                    1210,
                    2844
                ],
                "angle": 0,
                "content": "Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation. In ACL, pages 567-573."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    292,
                    2873,
                    1210,
                    3013
                ],
                "angle": 0,
                "content": "William Fedus, Ian Goodfellow, and Andrew M. Dai. 2018. MaskGAN: Better text generation via filling in the ______. In ICLR."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    292,
                    3048,
                    1210,
                    3185
                ],
                "angle": 0,
                "content": "Hagen Fürstenau and Mirella Lapata. 2009. Semi-supervised semantic role labeling. In EACL, pages 220-228."
            },
            {
                "block_id": 14,
                "type": "list",
                "bbox": [
                    292,
                    1824,
                    1210,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1272,
                    270,
                    2190,
                    452
                ],
                "angle": 0,
                "content": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In NIPS, pages 2672-2680."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1272,
                    487,
                    2190,
                    624
                ],
                "angle": 0,
                "content": "Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1272,
                    659,
                    2190,
                    841
                ],
                "angle": 0,
                "content": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017. Toward controlled generation of text. In ICML, pages 1587-1596."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1272,
                    877,
                    2190,
                    1020
                ],
                "angle": 0,
                "content": "Navdeep Jaitly and Geoffrey E Hinton. 2015. Vocal tract length perturbation (vtlp) improves speech recognition. In ICML."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1052,
                    2190,
                    1147
                ],
                "angle": 0,
                "content": "Robin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. In ACL, pages 12-22."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1178,
                    2190,
                    1322
                ],
                "angle": 0,
                "content": "Kushal Kafle, Mohammed Yousefhussien, and Christopher Kanan. 2017. Data augmentation for visual question answering. In INLG, pages 198-202."
            },
            {
                "block_id": 21,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1354,
                    2190,
                    1487
                ],
                "angle": 0,
                "content": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP, pages 1746-1751."
            },
            {
                "block_id": 22,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1525,
                    2190,
                    1659
                ],
                "angle": 0,
                "content": "Yoon Kim and Alexander M. Rush. 2016. Sequence-level knowledge distillation. In EMNLP, pages 1317-1327."
            },
            {
                "block_id": 23,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1697,
                    2190,
                    1880
                ],
                "angle": 0,
                "content": "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. 2015. Audio augmentation for speech recognition. In *INTERSPEECH*, pages 3586-3589."
            },
            {
                "block_id": 24,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1918,
                    2190,
                    2104
                ],
                "angle": 0,
                "content": "Sosuke Kobayashi, Naoaki Okazaki, and Kentaro Inui. 2017. A neural language model for dynamically representing the meanings of unknown words and entities in a discourse. In IJCNLP, pages 473-483."
            },
            {
                "block_id": 25,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2136,
                    2190,
                    2322
                ],
                "angle": 0,
                "content": "Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and Kentaro Inui. 2016. Dynamic entity representation with max-pooling improves machine reading. In Proceedings of NAACL-HLT, pages 850-855."
            },
            {
                "block_id": 26,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2353,
                    2190,
                    2536
                ],
                "angle": 0,
                "content": "Oleksandr Kolomiyets, Steven Bethard, and Marie-Francine Moens. 2011. Model-portability experiments for textual temporal analysis. In ACL, pages 271-276."
            },
            {
                "block_id": 27,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2574,
                    2190,
                    2753
                ],
                "angle": 0,
                "content": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097-1105."
            },
            {
                "block_id": 28,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2792,
                    2190,
                    2887
                ],
                "angle": 0,
                "content": "Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING, pages 1-7."
            },
            {
                "block_id": 29,
                "type": "ref_text",
                "bbox": [
                    1272,
                    2922,
                    2190,
                    3058
                ],
                "angle": 0,
                "content": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In ICLR."
            },
            {
                "block_id": 30,
                "type": "ref_text",
                "bbox": [
                    1272,
                    3094,
                    2190,
                    3185
                ],
                "angle": 0,
                "content": "George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39-41."
            },
            {
                "block_id": 31,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2190,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 32,
                "type": "page_number",
                "bbox": [
                    1203,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "456"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "ref_text",
                "bbox": [
                    292,
                    270,
                    1208,
                    406
                ],
                "angle": 0,
                "content": "Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACL."
            },
            {
                "block_id": 1,
                "type": "ref_text",
                "bbox": [
                    292,
                    442,
                    1210,
                    624
                ],
                "angle": 0,
                "content": "Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115-124."
            },
            {
                "block_id": 2,
                "type": "ref_text",
                "bbox": [
                    292,
                    666,
                    1210,
                    806
                ],
                "angle": 0,
                "content": "Charles Bally Albert Sechehaye Saussure, Ferdinand de and Albert Riedlinger. 1916. Cours de linguistique generale. Lausanne: Payot."
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    292,
                    841,
                    1210,
                    978
                ],
                "angle": 0,
                "content": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In ACL, pages 86-96."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    292,
                    1017,
                    1210,
                    1199
                ],
                "angle": 0,
                "content": "Miikka Silfverberg, Adam Wiemerslage, Ling Liu, and Lingshuang Jack Mao. 2017. Data augmentation for morphological reinfection. In CoNLL SIGMOR-PHON, pages 90-99."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    292,
                    1238,
                    1210,
                    1420
                ],
                "angle": 0,
                "content": "Patrice Y. Simard, Yann A. LeCun, John S. Denker, and Bernard Victorri. 1998. Transformation Invariance in Pattern Recognition — Tangent Distance and Tangent Propagation. Springer Berlin Heidelberg."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    292,
                    1459,
                    1210,
                    1687
                ],
                "angle": 0,
                "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pages 1631-1642."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    292,
                    1725,
                    1210,
                    1862
                ],
                "angle": 0,
                "content": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS, pages 3104-3112."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    292,
                    1901,
                    1210,
                    2125
                ],
                "angle": 0,
                "content": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In CVPR."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    292,
                    2164,
                    1210,
                    2350
                ],
                "angle": 0,
                "content": "Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on LearningSys in NIPS 28."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    292,
                    2388,
                    1210,
                    2616
                ],
                "angle": 0,
                "content": "William Yang Wang and Diyi Yang. 2015. That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets. In EMNLP, pages 2557–2563."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    292,
                    2655,
                    1210,
                    2837
                ],
                "angle": 0,
                "content": "Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. *Language Resources and Evaluation*, 39(2):165-210."
            },
            {
                "block_id": 12,
                "type": "ref_text",
                "bbox": [
                    292,
                    2876,
                    1210,
                    3013
                ],
                "angle": 0,
                "content": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. 2017. Dual supervised learning. In ICML, pages 3789-3798."
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    292,
                    3048,
                    1210,
                    3188
                ],
                "angle": 0,
                "content": "Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. 2017. Variational autoencoder for semi-supervised text classification. In AAAI, pages 3358-3364."
            },
            {
                "block_id": 14,
                "type": "list",
                "bbox": [
                    292,
                    270,
                    1210,
                    3188
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    1272,
                    270,
                    2188,
                    406
                ],
                "angle": 0,
                "content": "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In NIPS, pages 649-657."
            },
            {
                "block_id": 16,
                "type": "page_number",
                "bbox": [
                    1205,
                    3234,
                    1280,
                    3276
                ],
                "angle": 0,
                "content": "457"
            }
        ]
    }
]