[
    {
        "page_id": 0,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    558,
                    284,
                    1930,
                    424
                ],
                "angle": 0,
                "content": "Retrieve and Refine: Improved Sequence Generation Models For Dialogue"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    677,
                    533,
                    1821,
                    592
                ],
                "angle": 0,
                "content": "Jason Weston, Emily Dinan and Alexander H. Miller"
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    1017,
                    596,
                    1483,
                    648
                ],
                "angle": 0,
                "content": "Facebook AI Research"
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    677,
                    655,
                    1821,
                    712
                ],
                "angle": 0,
                "content": "jase@fb.com, edinan@fb.com, ahm@fb.com"
            },
            {
                "block_id": 4,
                "type": "title",
                "bbox": [
                    654,
                    929,
                    850,
                    982
                ],
                "angle": 0,
                "content": "Abstract"
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    354,
                    1031,
                    1143,
                    1831
                ],
                "angle": 0,
                "content": "Sequence generation models for dialogue are known to have several problems: they tend to produce short, generic sentences that are uninformative and unengaging. Retrieval models on the other hand can surface interesting responses, but are restricted to the given retrieval set leading to erroneous replies that cannot be tuned to the specific context. In this work we develop a model that combines the two approaches to avoid both their deficiencies: first retrieve a response and then refine it – the final sequence generator treating the retrieval as additional context. We show on the recent CONVAI2 challenge task our approach produces responses superior to both standard retrieval and generation models in human evaluations."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    290,
                    1978,
                    650,
                    2031
                ],
                "angle": 0,
                "content": "1 Introduction"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2069,
                    1215,
                    3030
                ],
                "angle": 0,
                "content": "Sequence generation models like Seq2Seq (Sutskever et al., 2014) are increasingly popular for tasks such as machine translation (MT) and summarization, where generation is suitably constrained by the source sentence. However, obtaining good performance on dialogue tasks, where the context still allows many interpretations, remains an open problem despite much recent work (Serban et al., 2016). Several authors report the issue that they produce short, generic sentences containing frequent words – the so-called “I don’t know” problem – as that response can work as a reply in many instances, but is uninformative and unengaging. Retrieval models (Ji et al., 2014) do not have this problem, but instead either produce engaging responses or else completely erroneous ones which they cannot"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1265,
                    929,
                    2190,
                    1038
                ],
                "angle": 0,
                "content": "tune to the specific context, as they can only produce a valid reply if it is in the retrieval set."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1265,
                    1045,
                    2195,
                    2227
                ],
                "angle": 0,
                "content": "In this work we propose a Retrieve and Refine model to gain the advantages of both methods, and avoid both their disadvantages. Models that produce an initial prediction and then refine it are growing in traction in NLP. They have been used in MT and summarization either for refinement of initial predictions (Junczys-Dowmunt and Grundkiewicz, 2017; Niehues et al., 2016; Novak et al., 2016; Xia et al., 2017; Grangier and Auli, 2017) or combining with retrieval (Gu et al., 2017; Cao et al., 2018), as well as for sentence correction or refinement without context (Guu et al., 2017; Schmaltz et al., 2017). There is little work in applying these methods to dialogue; one work we are aware of has been done concurrently with ours is Pandey et al. (2018). The usefulness of our approach is shown with detailed experiments on the ConvAI2 dataset<sup>1</sup> which is a chit-chat task to get to know the other speaker's profile, obtaining generations superior to both retrieval and sequence generation models in human evaluations."
            },
            {
                "block_id": 10,
                "type": "title",
                "bbox": [
                    1267,
                    2276,
                    1783,
                    2329
                ],
                "angle": 0,
                "content": "2 Retrieve and Refine"
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1265,
                    2367,
                    2193,
                    2652
                ],
                "angle": 0,
                "content": "The model we propose in this work is remarkably straightforward: we take a standard generative model and concatenate the output of a retrieval model to its usual input, and then generate as usual, training the model under this setting."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    1265,
                    2655,
                    2195,
                    3104
                ],
                "angle": 0,
                "content": "For the generator, we use a standard Seq2Seq model: a 2-layer LSTM with attention. For the retriever, we use the Key-Value Memory Network (Miller et al., 2016) already shown to perform well for this dataset (Zhang et al., 2018), which attends over the dialogue history, to learn input and candidate retrieval embeddings that match using cosine similarity. The top scoring utterance is provided"
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    285,
                    3058,
                    1215,
                    3185
                ],
                "angle": 0,
                "content": "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI 978-1-948087-75-9"
            },
            {
                "block_id": 14,
                "type": "page_footnote",
                "bbox": [
                    1322,
                    3139,
                    1731,
                    3188
                ],
                "angle": 0,
                "content": "http://convai.io/"
            },
            {
                "block_id": 15,
                "type": "page_number",
                "bbox": [
                    1215,
                    3230,
                    1270,
                    3279
                ],
                "angle": 0,
                "content": "87"
            },
            {
                "block_id": 16,
                "type": "footer",
                "bbox": [
                    277,
                    3301,
                    2195,
                    3350
                ],
                "angle": 0,
                "content": "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int'l Workshop on Search-Oriented Conversational AI, pages 87-92"
            },
            {
                "block_id": 17,
                "type": "footer",
                "bbox": [
                    565,
                    3350,
                    1912,
                    3395
                ],
                "angle": 0,
                "content": "Brussels, Belgium, October 31, 2018. ©2018 Association for Computational Linguistics"
            },
            {
                "block_id": 18,
                "type": "footer",
                "bbox": [
                    1039,
                    3399,
                    1438,
                    3444
                ],
                "angle": 0,
                "content": "ISBN 978-1-948087-75-9"
            }
        ]
    },
    {
        "page_id": 1,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    285,
                    263,
                    1215,
                    547
                ],
                "angle": 0,
                "content": "as input to our Seq2Seq model in order to refine it, pretended with a special separator token. For both models we use the code available in \\(\\mathrm{ParlAI}^2\\). At test time the retriever retrieves candidates from the training set."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    564,
                    1215,
                    1301
                ],
                "angle": 0,
                "content": "To train our model we first precompute the retrieval result for every dialogue turn in the training set, but instead of using the top ranking results we rerank the top 100 predictions of each by their similarity to the label (in embedding space). Following Guu et al. (2017) this should help avoid the problem of the refinement being too far away from the original retrieval. We then append the chosen utterances to the input sequences used to train Seq2Seq. We refer to our model as RetrieveN-Refine, or RetNRef for short. We also consider two variants of the model in the following that we found improve the results."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    285,
                    1392,
                    1215,
                    2010
                ],
                "angle": 0,
                "content": "Use Retriever More In our vanilla model, we noticed there was not enough attention being paid to the retrieval utterance by the generator. As the input to Seq2Seq is the dialogue history concatenated with the retrieval utterance, truncating the history is one way to pay more attention to the retrieval. In particular for the ConvAI2 dataset we clip the initial profile sentences at the start of the dialogue, forcing the model to more strongly rely on the retriever which still has them.<sup>3</sup> We refer to this modification as RetrieveNRefine+."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    285,
                    2104,
                    1215,
                    2781
                ],
                "angle": 0,
                "content": "Fix Retrieval Copy Errors Our model learns to sometimes ignore the retrieval (when it is bad), sometimes use it partially, and other times simply copy it. However, when it is mostly copied but only changes a word or two, we observed it made mistakes more often than not, leading to less meaningful utterances. We thus also consider a variant that exactly copies the retrieval if the model generates with large word overlap (we chose \\(>60\\%\\)). Otherwise, we leave the generation untouched.4 We refer to this as RetrieveNRefine++."
            },
            {
                "block_id": 4,
                "type": "table",
                "bbox": [
                    1391,
                    252,
                    2071,
                    606
                ],
                "angle": 0,
                "content": "<table><tr><td>RetNRef</td><td>Retrieval Method</td><td>PPL</td></tr><tr><td colspan=\"2\">None (Vanilla Seq2Seq)</td><td>31.4</td></tr><tr><td colspan=\"2\">Random label</td><td>32.0</td></tr><tr><td colspan=\"2\">Memory Network</td><td>31.8</td></tr><tr><td colspan=\"2\">True label&#x27;s neighbor</td><td>25.9</td></tr><tr><td colspan=\"2\">True label</td><td>9.2</td></tr></table>",
                "caption": "Table 1: Perplexity on the ConvAI2 task test set with different types of retriever for RetNRef, see text."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1267,
                    841,
                    1634,
                    905
                ],
                "angle": 0,
                "content": "3 Experiments"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1262,
                    936,
                    2195,
                    1669
                ],
                "angle": 0,
                "content": "We conduct experiments on the recent ConvAI2 challenge dataset which uses a modified version of the PersonaChat dataset (Zhang et al., 2018) (larger, and with different processing). The dataset consists of conversations between crowdworkers who were randomly paired and asked to act the part of a given persona (randomly assigned from 1155 possible personas, created by another set of workers), chat naturally, and get to know each other during the conversation. There are around 160,000 utterances in around 11,000 dialogues, with 2000 dialogues for validation and test, which use non-overlapping personas."
            },
            {
                "block_id": 8,
                "type": "title",
                "bbox": [
                    1265,
                    1708,
                    2074,
                    1764
                ],
                "angle": 0,
                "content": "3.1 Automatic Evaluation and Analysis"
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1262,
                    1782,
                    2193,
                    2515
                ],
                "angle": 0,
                "content": "Perplexity Dialogue is known to be notoriously hard to evaluate with automated metrics (Liu et al., 2016). In contrast to machine translation, there is much less constraint on the output with many valid answers with little word overlap, e.g. there are many answers to \"what are you doing tonight?\". Nevertheless many recent papers report perplexity results in addition to human judgments. For the retrieve and refine case, perplexity evaluation is particularly flawed: if the retrieval points the model to a response that is very different from (but equally valid as) the true response, the model might focus on refining that and get poor perplexity."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1262,
                    2515,
                    2195,
                    3195
                ],
                "angle": 0,
                "content": "We therefore test our model by considering various types of retrieval methods: (i) the best performing existing retriever model, the Memory Network approach from Zhang et al. (2018) (retrieving from the training set), (ii) a retriever that returns a random utterance from the training set, (iii) the true label given in the test set, and (iv) the closest nearest neighbor from the training set utterances to the true label, as measured by the embedding space of the Memory Network retriever model. While (iii) and (iv) cannot be used in a deployed system as they are unknown, they can be"
            },
            {
                "block_id": 11,
                "type": "page_footnote",
                "bbox": [
                    342,
                    2880,
                    682,
                    2929
                ],
                "angle": 0,
                "content": "\\(^{2}\\)http://parl.ai"
            },
            {
                "block_id": 12,
                "type": "page_footnote",
                "bbox": [
                    290,
                    2929,
                    1213,
                    3058
                ],
                "angle": 0,
                "content": "3 Architectural changes might also deal with this issue, e.g. treating the two inputs as independent sources to do attention over, but we take the simplest possible approach here."
            },
            {
                "block_id": 13,
                "type": "page_footnote",
                "bbox": [
                    290,
                    3058,
                    1213,
                    3188
                ],
                "angle": 0,
                "content": "4Other approaches might also help with this problem such as using an explicit copy mechanism or to use BPE tokenization (Fan et al., 2017), but we leave those for future work."
            },
            {
                "block_id": 14,
                "type": "list",
                "bbox": [
                    290,
                    2880,
                    1213,
                    3188
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 15,
                "type": "page_number",
                "bbox": [
                    1218,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "88"
            }
        ]
    },
    {
        "page_id": 2,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    307,
                    252,
                    1198,
                    722
                ],
                "angle": 0,
                "content": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Word cnt</td><td rowspan=\"2\">Char cnt</td><td colspan=\"2\">Rare Word %</td></tr><tr><td>&lt;100</td><td>&lt;1k</td></tr><tr><td>Seq2Seq</td><td>11.7</td><td>40.5</td><td>0.4%</td><td>5.8%</td></tr><tr><td>RetNRef</td><td>11.8</td><td>40.4</td><td>1.1%</td><td>6.9%</td></tr><tr><td>RetNRef+</td><td>12.1</td><td>45.0</td><td>1.7%</td><td>10.1%</td></tr><tr><td>RetNRef++</td><td>12.7</td><td>48.1</td><td>2.3%</td><td>10.9%</td></tr><tr><td>MemNet</td><td>13.1</td><td>54.5</td><td>4.0%</td><td>15.3%</td></tr><tr><td>Human</td><td>13.0</td><td>54.6</td><td>3.0%</td><td>11.5%</td></tr></table>",
                "caption": "Table 2: Output sequence statistics for the methods. Seq2Seq generates shorter sentences with more common words than humans, which RetNRef alleviates."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    297,
                    968,
                    1210,
                    1269
                ],
                "angle": 0,
                "content": "<table><tr><td>Method</td><td>&lt;30%</td><td>30-60%</td><td>60-80%</td><td>&gt;80%</td></tr><tr><td>Seq2Seq</td><td>56%</td><td>34%</td><td>7%</td><td>3%</td></tr><tr><td>RetNRef</td><td>41%</td><td>38%</td><td>13%</td><td>8%</td></tr><tr><td>RetNRef+</td><td>26%</td><td>20%</td><td>12%</td><td>42%</td></tr><tr><td>RetNRef++</td><td>26%</td><td>20%</td><td>0%</td><td>53%</td></tr></table>",
                "caption": "Table 3: Word overlap between retrieved and generated utterances in RetNRef, and between Seq2Seq and the Memory Network retriever (first row)."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    285,
                    1561,
                    1213,
                    1785
                ],
                "angle": 0,
                "content": "used as a sanity check: a useful retrieve and refine should improve perplexity if given these as input. We also compare to a standard Seq2Seq model, i.e. no retrieval."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    285,
                    1792,
                    1215,
                    2585
                ],
                "angle": 0,
                "content": "The results are given in Table 1. They show that the RetNRef model can indeed improve perplexity with label neighbors or the label itself. However, surprisingly there is almost no difference between using no retrieval, random labels or our best retriever. The RetNRef++ model – that truncates the dialogue history and focuses more on the retrieval utterance – does even worse in terms of perplexity: 48.4 using the Memory Network retriever. However, poor perplexity does not mean human judgments of the generated sequences will not improve; in fact we will see that they do in the next section. How to automatically evaluate these kinds of models still remains an open problem."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2627,
                    1220,
                    3195
                ],
                "angle": 0,
                "content": "Word Statistics Another way to measure the salience of a generation model is to compare it to human utterances in terms of word statistics. We analyze the word statistics of our models in Table 2. Seq2Seq models are known to produce short sentences with more common words than humans. The statistics on the ConvAI2 dataset bear this out, where the Seq2Seq model responses have lower word and character counts and use fewer rare words than the human responses. The RetNRef"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1262,
                    266,
                    2195,
                    996
                ],
                "angle": 0,
                "content": "model (using the Memory Network retriever, retrieving from the training set) makes some improvements in this regard, e.g. doubling the use of rare words (with frequency less than 100) and smaller gains for words with frequency less than 1000, but are still not close to human statistics. The RetNRef++ model which boosts the use of the retrieval does better in this regard, making the statistics much closer to human ones. Of course these metrics do not measure whether the utterances are semantically coherent, but it is encouraging to see a model using rare words as without this we believe it is hard for it to be engaging."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1262,
                    1006,
                    2198,
                    1904
                ],
                "angle": 0,
                "content": "Table 3 compares the word overlap between retrieved and generated sentences in the RetNRef variants in order to measure if RetNRef is either ignoring the retriever, or else paying too much attention to and copying it. As comparison, the first row also shows the overlap between the retriever and vanilla Seq2Seq which does not retrieve at all. The results show that RetNRef++ has \\(>80\\%\\) word overlap with the retriever output around half \\((53\\%)\\) of the time, whereas Seq2Seq and RetNRef very rarely overlap with the retriever (3% and \\(8\\%\\) of the time respectively have \\(>80\\%\\) overlap). This shows that our improved model RetNRef++ does use the retriever, but can also generate novel content when it wants to, which a standard retriever cannot."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1265,
                    1971,
                    2178,
                    2024
                ],
                "angle": 0,
                "content": "3.2 Evaluation by Human Judgement Scores"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1262,
                    2059,
                    2193,
                    2616
                ],
                "angle": 0,
                "content": "Following the protocol in Zhang et al. (2018), we asked humans to conduct short dialogues with our models (100 dialogues each of 10-20 turns, so 600 dialogues in total), and then measure the engagingness, consistency, and fluency (all scored out of 5) as well as to try to detect the persona that the model is using, given the choice between that and a random persona. Consistency measures the coherence of the dialogue, e.g. \"I have a dog\" followed by \"I have no pets\" is not consistent."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1262,
                    2627,
                    2195,
                    3195
                ],
                "angle": 0,
                "content": "The results are given in Table 4. They show engagingness scores superior to Seq2Seq for all RetNRef variants, and with RetNRef++ slightly outperforming the retriever which it conditions on. Importantly however, it maintains this performance whilst still being able to generate text which a retrieval model cannot. It also performs well in the other metrics, although like the Memory Network model, it is weaker at using the persona than Seq2Seq. Seq2Seq is inferior to the"
            },
            {
                "block_id": 12,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "89"
            }
        ]
    },
    {
        "page_id": 3,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "table",
                "bbox": [
                    471,
                    252,
                    2014,
                    663
                ],
                "angle": 0,
                "content": "<table><tr><td>Method</td><td>Engagingness</td><td>Fluency</td><td>Consistency</td><td>Persona</td></tr><tr><td>Seq2Seq (PPL)</td><td>2.70(1.17)</td><td>3.50(1.37)</td><td>3.90(1.37)</td><td>0.90(0.29)</td></tr><tr><td>Seq2Seq (100 epochs)</td><td>2.76(1.15)</td><td>3.53(1.14)</td><td>3.84(1.38)</td><td>0.85(0.35)</td></tr><tr><td>Memory Network</td><td>3.66(1.26)</td><td>3.83(1.26)</td><td>3.61(1.36)</td><td>0.73(0.44)</td></tr><tr><td>RetrieveNRefine</td><td>2.94(1.26)</td><td>3.65(1.28)</td><td>3.72(1.32)</td><td>0.90(0.30)</td></tr><tr><td>RetrieveNRefine+</td><td>3.50(1.33)</td><td>3.63(1.13)</td><td>3.55(1.33)</td><td>0.71(0.45)</td></tr><tr><td>RetrieveNRefine++</td><td>3.80(1.18)</td><td>3.74(1.19)</td><td>3.80(1.40)</td><td>0.65(0.47)</td></tr></table>",
                "caption": "Table 4: Human Evaluation scores for the models, scoring fluency, engagingness, consistency and persona detection, with standard deviation in parentheses. We consider engagingness to be the most important metric."
            },
            {
                "block_id": 2,
                "type": "table",
                "bbox": [
                    364,
                    845,
                    2108,
                    1311
                ],
                "angle": 0,
                "content": "<table><tr><td>Comparison (A vs. B)</td><td>Win Rate</td><td>A Wins</td><td>B Wins</td><td>Tie</td><td>p-value</td></tr><tr><td>RetrieveNRefine++ vs. Memory Network</td><td>54.5%</td><td>340</td><td>284</td><td>572</td><td>0.027</td></tr><tr><td>RetrieveNRefine++ vs. Seq2Seq</td><td>53.7%</td><td>571</td><td>492</td><td>203</td><td>0.016</td></tr><tr><td>RetrieveNRefine++(retrieved) vs Seq2Seq</td><td>53.8%</td><td>290</td><td>249</td><td>87</td><td></td></tr><tr><td>RetrieveNRefine++(generated) vs Seq2Seq</td><td>53.6%</td><td>281</td><td>243</td><td>116</td><td></td></tr><tr><td>RetrieveNRefine+ vs. Memory Network</td><td>51.63%</td><td>492</td><td>461</td><td>243</td><td></td></tr><tr><td>RetrieveNRefine+ vs. Human</td><td>30.13%</td><td>69</td><td>160</td><td>14</td><td></td></tr><tr><td>Seq2Seq vs. Human</td><td>26.84%</td><td>62</td><td>169</td><td>22</td><td></td></tr></table>",
                "caption": "Table 5: A/B testing by humans comparing model pairs, reporting win rates for A over B (excluding ties)."
            },
            {
                "block_id": 4,
                "type": "table",
                "bbox": [
                    307,
                    1441,
                    2178,
                    1978
                ],
                "angle": 0,
                "content": "<table><tr><td>Person 1\nPerson 2</td><td>hey, george here . lets be friends\nhi george, i&#x27;m regina . do you enjoy dance ?</td><td></td><td>yeah. well, i wish you luck there. we bought a cute 2 story house.\nnice, i don&#x27;t think i have ever left ohio.</td></tr><tr><td>Seq2Seq\nMemNet\nRetNRef+</td><td>i do not but i do have a lot of time to dance\nhello george my name is pink\ni do. i enjoy dance.</td><td></td><td>i am sorry. i am so scared of twins.\ni am from mississippi . just moved back here from austin texas\ni am from tennessee. just moved here from austin texas.</td></tr><tr><td colspan=\"4\"></td></tr><tr><td>Person 1\nPerson 2</td><td>drinking coffee? what time is it there?\n830 pm haha my dad is a construction worker\nso he works really late nights</td><td></td><td>thank you for that , you are what you believe ?\nlook energy isn’t physical , only spiritual</td></tr><tr><td colspan=\"4\"></td></tr><tr><td>Seq2Seq\nMemNet\nRetNRef+</td><td>i am a big guy so i can help you\nyep . moonlight is amazing but northern\nlight grab you too\noh wow, that is a long time</td><td></td><td>i am not sure what that means\ndo you do any sports ? swimming helps me keep my energy up.\ndo you do any sports ? swimming helps me keep my energy up</td></tr></table>",
                "caption": "Table 6: Example dialogues (left) where RetNRef outperforms MemNet, and (right) where RetNRef outperforms Seq2Seq – by either paying attention to the retriever or not. The first two rows are the previous two dialogue turns by Person 1 & 2, the following rows are the predictions for the next utterance of Person 1 by the various models."
            },
            {
                "block_id": 6,
                "type": "text",
                "bbox": [
                    285,
                    2248,
                    1213,
                    2585
                ],
                "angle": 0,
                "content": "Memory Network Retriever in terms of engagement, in line with previous results. We also tried overtraining the Seq2Seq for 100 epochs instead of early stopping by validation on perplexity as it may overfit training sentences and act more as a retriever, but this did not help."
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    285,
                    2627,
                    1215,
                    3195
                ],
                "angle": 0,
                "content": "Some example dialogues of the RetNRef++ model performing well (as scored by the evaluators) are shown in Tables 7 and 8. Longer sentences from the bot (person 2) with more nuanced entity information typically come from attending to the retriever, whereas the generator can also produce shorter replies independent of the retriever that fit the context well. There are still issues however, such as repeated phrases by the generator, and some tendency to copy the speaking"
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1267,
                    2252,
                    2029,
                    2308
                ],
                "angle": 0,
                "content": "partner's phrases that could be improved."
            },
            {
                "block_id": 9,
                "type": "title",
                "bbox": [
                    1267,
                    2371,
                    1845,
                    2424
                ],
                "angle": 0,
                "content": "3.3 A/B Testing by Humans"
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1262,
                    2459,
                    2193,
                    3192
                ],
                "angle": 0,
                "content": "To compute a statistically more meaningful test, and to evaluate models more clearly against each other, we also tested models by pairing them against each other (different to the protocol previously used for this dataset). We took human-human paired dialogues from the test set, up to a randomly chosen turn, and then computed the response of two rival models. The resulting dialogue history and the two choices are given to a third annotator to evaluate and asked which model is best; they can also choose \"unsure\". We collected over 1000 such examples, with each conversation being evaluated twice. The results are given in Table 5."
            },
            {
                "block_id": 11,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1270,
                    3279
                ],
                "angle": 0,
                "content": "90"
            }
        ]
    },
    {
        "page_id": 4,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "title",
                "bbox": [
                    473,
                    256,
                    640,
                    294
                ],
                "angle": 0,
                "content": "Persona 1"
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    476,
                    301,
                    774,
                    340
                ],
                "angle": 0,
                "content": "I am relatively tall."
            },
            {
                "block_id": 2,
                "type": "text",
                "bbox": [
                    476,
                    340,
                    783,
                    375
                ],
                "angle": 0,
                "content": "I like mexican food."
            },
            {
                "block_id": 3,
                "type": "text",
                "bbox": [
                    476,
                    382,
                    831,
                    420
                ],
                "angle": 0,
                "content": "I drive a subaru legacy."
            },
            {
                "block_id": 4,
                "type": "text",
                "bbox": [
                    476,
                    424,
                    880,
                    466
                ],
                "angle": 0,
                "content": "My dog's name is Drunky."
            },
            {
                "block_id": 5,
                "type": "text",
                "bbox": [
                    476,
                    466,
                    997,
                    508
                ],
                "angle": 0,
                "content": "I can touch my toes with my nose."
            },
            {
                "block_id": 6,
                "type": "title",
                "bbox": [
                    1037,
                    256,
                    1205,
                    294
                ],
                "angle": 0,
                "content": "Persona 2"
            },
            {
                "block_id": 7,
                "type": "text",
                "bbox": [
                    1037,
                    298,
                    1376,
                    340
                ],
                "angle": 0,
                "content": "I like to play baseball."
            },
            {
                "block_id": 8,
                "type": "text",
                "bbox": [
                    1037,
                    340,
                    1367,
                    378
                ],
                "angle": 0,
                "content": "I love eating seafood."
            },
            {
                "block_id": 9,
                "type": "text",
                "bbox": [
                    1037,
                    382,
                    1520,
                    424
                ],
                "angle": 0,
                "content": "I enjoy learning new languages."
            },
            {
                "block_id": 10,
                "type": "text",
                "bbox": [
                    1037,
                    424,
                    1694,
                    466
                ],
                "angle": 0,
                "content": "I'm currently going for a marketing degree."
            },
            {
                "block_id": 11,
                "type": "text",
                "bbox": [
                    1037,
                    466,
                    1548,
                    508
                ],
                "angle": 0,
                "content": "I want to design ads in the future."
            },
            {
                "block_id": 12,
                "type": "text",
                "bbox": [
                    473,
                    543,
                    2009,
                    589
                ],
                "angle": 0,
                "content": "[PERSON 1:] I'm driving to the mexican place tonight for dinner, but my Subaru is at the rental shop."
            },
            {
                "block_id": 13,
                "type": "text",
                "bbox": [
                    476,
                    589,
                    1535,
                    631
                ],
                "angle": 0,
                "content": "[PERSON 2:] Yummy. I love mexican food, but I never eat vegetables"
            },
            {
                "block_id": 14,
                "type": "text",
                "bbox": [
                    476,
                    631,
                    1282,
                    673
                ],
                "angle": 0,
                "content": "[PERSON 1:] Oh, I do, I think that's why i'm so tall!"
            },
            {
                "block_id": 15,
                "type": "text",
                "bbox": [
                    476,
                    673,
                    1535,
                    715
                ],
                "angle": 0,
                "content": "[PERSON 2:] how tall are you? I do not... I enjoy studying languages."
            },
            {
                "block_id": 16,
                "type": "text",
                "bbox": [
                    476,
                    715,
                    1359,
                    757
                ],
                "angle": 0,
                "content": "[PERSON 1:] I'm 6 foot, a lot taller than my dog, Drunky."
            },
            {
                "block_id": 17,
                "type": "text",
                "bbox": [
                    476,
                    757,
                    1396,
                    796
                ],
                "angle": 0,
                "content": "[PERSON 2:] I am sorry to hear that. I want to be 6 foot tall."
            },
            {
                "block_id": 18,
                "type": "text",
                "bbox": [
                    476,
                    796,
                    1615,
                    838
                ],
                "angle": 0,
                "content": "[PERSON 1:] Do you have any talents? I can reach my toes using my nose."
            },
            {
                "block_id": 19,
                "type": "text",
                "bbox": [
                    476,
                    838,
                    1897,
                    880
                ],
                "angle": 0,
                "content": "[PERSON 2:] Cool! I listen to audiobooks and speak one asian language. you like sushi? I do."
            },
            {
                "block_id": 20,
                "type": "text",
                "bbox": [
                    476,
                    880,
                    1833,
                    922
                ],
                "angle": 0,
                "content": "[PERSON 1:] Oh sushi is my next favorite, after Mexican. What kind of car do you have?"
            },
            {
                "block_id": 21,
                "type": "text",
                "bbox": [
                    476,
                    922,
                    1178,
                    964
                ],
                "angle": 0,
                "content": "[PERSON 2:] I have a Nissan Sentra. I love it."
            },
            {
                "block_id": 22,
                "type": "table_caption",
                "bbox": [
                    285,
                    1006,
                    2198,
                    1161
                ],
                "angle": 0,
                "content": "Table 7: Example dialog between a human (Person 1) and the RetNRef++ model (Person 2). References to never eating vegetables, listening to audiobooks and speaking one asian language and owning a Nissan Sentra appeared in the conditioned retrieved sentences, improving the overall engaging quality of the dialogue."
            },
            {
                "block_id": 23,
                "type": "title",
                "bbox": [
                    471,
                    1217,
                    637,
                    1259
                ],
                "angle": 0,
                "content": "Persona 1"
            },
            {
                "block_id": 24,
                "type": "text",
                "bbox": [
                    473,
                    1262,
                    898,
                    1304
                ],
                "angle": 0,
                "content": "I am a lacto-ovo vegetarian."
            },
            {
                "block_id": 25,
                "type": "text",
                "bbox": [
                    476,
                    1304,
                    674,
                    1343
                ],
                "angle": 0,
                "content": "I like to hike."
            },
            {
                "block_id": 26,
                "type": "text",
                "bbox": [
                    476,
                    1347,
                    704,
                    1385
                ],
                "angle": 0,
                "content": "I have red hair."
            },
            {
                "block_id": 27,
                "type": "text",
                "bbox": [
                    476,
                    1389,
                    880,
                    1427
                ],
                "angle": 0,
                "content": "My favorite drink is sprite."
            },
            {
                "block_id": 28,
                "type": "text",
                "bbox": [
                    476,
                    1431,
                    967,
                    1473
                ],
                "angle": 0,
                "content": "I have a pet parrot named Tasha."
            },
            {
                "block_id": 29,
                "type": "title",
                "bbox": [
                    1009,
                    1217,
                    1175,
                    1259
                ],
                "angle": 0,
                "content": "Persona 2"
            },
            {
                "block_id": 30,
                "type": "text",
                "bbox": [
                    1009,
                    1262,
                    1285,
                    1301
                ],
                "angle": 0,
                "content": "I have brown hair."
            },
            {
                "block_id": 31,
                "type": "text",
                "bbox": [
                    1009,
                    1304,
                    1195,
                    1343
                ],
                "angle": 0,
                "content": "I am Latina."
            },
            {
                "block_id": 32,
                "type": "text",
                "bbox": [
                    1009,
                    1347,
                    1667,
                    1385
                ],
                "angle": 0,
                "content": "My mother works for special needs clients.."
            },
            {
                "block_id": 33,
                "type": "text",
                "bbox": [
                    1009,
                    1389,
                    1218,
                    1427
                ],
                "angle": 0,
                "content": "I am a healer."
            },
            {
                "block_id": 34,
                "type": "text",
                "bbox": [
                    1009,
                    1431,
                    1270,
                    1473
                ],
                "angle": 0,
                "content": "I love my family."
            },
            {
                "block_id": 35,
                "type": "text",
                "bbox": [
                    473,
                    1508,
                    1530,
                    1554
                ],
                "angle": 0,
                "content": "[PERSON 1:] I dont know about you but I love sprite. Its my favorite."
            },
            {
                "block_id": 36,
                "type": "text",
                "bbox": [
                    476,
                    1557,
                    1498,
                    1596
                ],
                "angle": 0,
                "content": "[PERSON 2:] I like dr pepper a lot, but sprite is my second favorite."
            },
            {
                "block_id": 37,
                "type": "text",
                "bbox": [
                    476,
                    1596,
                    1510,
                    1638
                ],
                "angle": 0,
                "content": "[PERSON 1:] Sprites great! I take it with me every day on my hikes."
            },
            {
                "block_id": 38,
                "type": "text",
                "bbox": [
                    476,
                    1638,
                    1143,
                    1680
                ],
                "angle": 0,
                "content": "[PERSON 2:] I bet. I love to go to the water"
            },
            {
                "block_id": 39,
                "type": "text",
                "bbox": [
                    476,
                    1680,
                    1788,
                    1718
                ],
                "angle": 0,
                "content": "[PERSON 1:] Ya that sounds like a great time. I go with my pet parrot Tasha when I go"
            },
            {
                "block_id": 40,
                "type": "text",
                "bbox": [
                    476,
                    1722,
                    1305,
                    1761
                ],
                "angle": 0,
                "content": "[PERSON 2:] that is a good idea. I wish I had a parrot."
            },
            {
                "block_id": 41,
                "type": "text",
                "bbox": [
                    476,
                    1761,
                    2012,
                    1803
                ],
                "angle": 0,
                "content": "[PERSON 1:] Ya its fun. As a pet owner you learn to love animals; im even a lactoovo vegetarian now"
            },
            {
                "block_id": 42,
                "type": "text",
                "bbox": [
                    476,
                    1803,
                    1396,
                    1845
                ],
                "angle": 0,
                "content": "[PERSON 2:] I love animals. I am vegan so I eat vegan food."
            },
            {
                "block_id": 43,
                "type": "text",
                "bbox": [
                    476,
                    1845,
                    1543,
                    1887
                ],
                "angle": 0,
                "content": "[PERSON 1:] Sounds like me :D im vegan, im ginger, and i love sprite"
            },
            {
                "block_id": 44,
                "type": "text",
                "bbox": [
                    476,
                    1887,
                    1367,
                    1929
                ],
                "angle": 0,
                "content": "[PERSON 2:] I love animals. I am vegan so I eat a lot of it."
            },
            {
                "block_id": 45,
                "type": "table_caption",
                "bbox": [
                    285,
                    1971,
                    2198,
                    2174
                ],
                "angle": 0,
                "content": "Table 8: Example dialog between a human (Person 1) and the RetNRef++ model (Person 2). References to Dr. Pepper and wishing to have a parrot appeared in the conditioned retrieved sentences, improving the overall engaging quality of the dialogue. There are still issues however, such as repeated phrases by the generator, failure to use the persona adequately, and some tendency to copy the speaking partner's phrases that could be improved."
            },
            {
                "block_id": 46,
                "type": "text",
                "bbox": [
                    285,
                    2273,
                    1215,
                    2950
                ],
                "angle": 0,
                "content": "RetrieveNRefine obtains statistically significant wins over the retriever Memory Network model and the generator Seq2Seq model using a binomial two-tailed test, with win rates \\(\\sim 54\\%\\). Breaking down the wins between when RetNRef++ exactly copies the retrieval utterance vs. generates we see that it chooses them about equally, with wins about equal in both cases. This shows it can effectively learn when to choose the retrieval utterance (when it is good), and when to ignore it and generate instead (when it is bad). Table 6, which shows example outputs of our model, illustrates this."
            },
            {
                "block_id": 47,
                "type": "text",
                "bbox": [
                    285,
                    2967,
                    1215,
                    3195
                ],
                "angle": 0,
                "content": "RetNRef\\(^{+}\\) sometimes loses out when making small changes to the retrieved text, for example it made changes to “i once broke my nose trying to peak in on a jazz concert!” by replacing"
            },
            {
                "block_id": 48,
                "type": "text",
                "bbox": [
                    1265,
                    2273,
                    2195,
                    2609
                ],
                "angle": 0,
                "content": "peak with glacier. Recall that \\(\\mathrm{RetNRef}^{++}\\) fixes this problem by exactly copying the retrieved text when there is insignificant word overlap with the generated text; as such, it has a correspondingly larger win rate against Memory Networks (54.5% versus 51.63%)."
            },
            {
                "block_id": 49,
                "type": "text",
                "bbox": [
                    1265,
                    2613,
                    2195,
                    2781
                ],
                "angle": 0,
                "content": "We also computed a small sample of A/B tests directly against humans rather than models, and again see the win rate is higher for RetNRef."
            },
            {
                "block_id": 50,
                "type": "title",
                "bbox": [
                    1267,
                    2823,
                    1597,
                    2876
                ],
                "angle": 0,
                "content": "4 Conclusion"
            },
            {
                "block_id": 51,
                "type": "text",
                "bbox": [
                    1265,
                    2911,
                    2195,
                    3195
                ],
                "angle": 0,
                "content": "In conclusion, we showed that retrieval models can be successfully used to improve generation models in dialogue, helping them avoid common issues such as producing short sentences with frequent words that ultimately are not engaging. Our"
            },
            {
                "block_id": 52,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1267,
                    3279
                ],
                "angle": 0,
                "content": "91"
            }
        ]
    },
    {
        "page_id": 5,
        "ocr_results": [
            {
                "block_id": 0,
                "type": "text",
                "bbox": [
                    287,
                    263,
                    1213,
                    431
                ],
                "angle": 0,
                "content": "\\(\\mathrm{RetNRef}^{+ + }\\) model has similar statistics to human utterances and provides more engaging conversations according to human judgments."
            },
            {
                "block_id": 1,
                "type": "text",
                "bbox": [
                    285,
                    434,
                    1215,
                    887
                ],
                "angle": 0,
                "content": "Future work should investigate improved ways to incorporate retrieval in generation, both avoiding the heuristics we used here to improve performance, and seeing if more sophisticated approaches than concatenation plus attention improve the results, for example by more clearly treating the inputs as independent sources, or training the models jointly."
            },
            {
                "block_id": 2,
                "type": "title",
                "bbox": [
                    292,
                    989,
                    535,
                    1041
                ],
                "angle": 0,
                "content": "References"
            },
            {
                "block_id": 3,
                "type": "ref_text",
                "bbox": [
                    292,
                    1073,
                    1215,
                    1354
                ],
                "angle": 0,
                "content": "Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2018. Retrieve, rerank and rewrite: Soft template based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 152-161."
            },
            {
                "block_id": 4,
                "type": "ref_text",
                "bbox": [
                    292,
                    1396,
                    1215,
                    1536
                ],
                "angle": 0,
                "content": "Angela Fan, David Grangier, and Michael Auli. 2017. Controllable abstractive summarization. arXiv preprint arXiv:1711.05217."
            },
            {
                "block_id": 5,
                "type": "ref_text",
                "bbox": [
                    292,
                    1578,
                    1213,
                    1718
                ],
                "angle": 0,
                "content": "David Grangier and Michael Auli. 2017. Quicked: Editing text & translations via simple delete actions. arXiv preprint arXiv:1711.04805."
            },
            {
                "block_id": 6,
                "type": "ref_text",
                "bbox": [
                    292,
                    1761,
                    1215,
                    1950
                ],
                "angle": 0,
                "content": "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2017. Search engine guided non-parametric neural machine translation. arXiv preprint arXiv:1705.07267."
            },
            {
                "block_id": 7,
                "type": "ref_text",
                "bbox": [
                    292,
                    1992,
                    1215,
                    2174
                ],
                "angle": 0,
                "content": "Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2017. Generating sentences by editing prototypes. arXiv preprint arXiv:1709.08878."
            },
            {
                "block_id": 8,
                "type": "ref_text",
                "bbox": [
                    292,
                    2220,
                    1213,
                    2364
                ],
                "angle": 0,
                "content": "Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An information retrieval approach to short text conversation. arXiv preprint arXiv:1408.6988."
            },
            {
                "block_id": 9,
                "type": "ref_text",
                "bbox": [
                    292,
                    2406,
                    1213,
                    2592
                ],
                "angle": 0,
                "content": "Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2017. An exploration of neural sequence-to-sequence architectures for automatic post-editing. arXiv preprint arXiv:1706.04138."
            },
            {
                "block_id": 10,
                "type": "ref_text",
                "bbox": [
                    292,
                    2634,
                    1215,
                    2911
                ],
                "angle": 0,
                "content": "Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023."
            },
            {
                "block_id": 11,
                "type": "ref_text",
                "bbox": [
                    292,
                    2957,
                    1215,
                    3185
                ],
                "angle": 0,
                "content": "Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126."
            },
            {
                "block_id": 12,
                "type": "list",
                "bbox": [
                    292,
                    1073,
                    1215,
                    3185
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 13,
                "type": "ref_text",
                "bbox": [
                    1272,
                    270,
                    2190,
                    406
                ],
                "angle": 0,
                "content": "Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-translation for neural machine translation. arXiv preprint arXiv:1610.05243."
            },
            {
                "block_id": 14,
                "type": "ref_text",
                "bbox": [
                    1272,
                    442,
                    2190,
                    582
                ],
                "angle": 0,
                "content": "Roman Novak, Michael Auli, and David Grangier. 2016. Iterative refinement for machine translation. arXiv preprint arXiv:1610.06602."
            },
            {
                "block_id": 15,
                "type": "ref_text",
                "bbox": [
                    1272,
                    617,
                    2190,
                    894
                ],
                "angle": 0,
                "content": "Gaurav Pandey, Danish Contractor, Vineet Kumar, and Sachindra Joshi. 2018. Exemplar encoder-decoder for neural conversation generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1329-1338."
            },
            {
                "block_id": 16,
                "type": "ref_text",
                "bbox": [
                    1272,
                    929,
                    2190,
                    1108
                ],
                "angle": 0,
                "content": "Allen Schmaltz, Yoon Kim, Alexander M Rush, and Stuart M Shieber. 2017. Adapting sequence models for sentence correction. arXiv preprint arXiv:1707.09067."
            },
            {
                "block_id": 17,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1147,
                    2190,
                    1333
                ],
                "angle": 0,
                "content": "Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016. Generative deep neural networks for dialogue: A short review. arXiv preprint arXiv:1611.06216."
            },
            {
                "block_id": 18,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1368,
                    2190,
                    1554
                ],
                "angle": 0,
                "content": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112."
            },
            {
                "block_id": 19,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1589,
                    2190,
                    1820
                ],
                "angle": 0,
                "content": "Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation networks: Sequence generation beyond one-pass decoding. In Advances in Neural Information Processing Systems, pages 1782-1792."
            },
            {
                "block_id": 20,
                "type": "ref_text",
                "bbox": [
                    1272,
                    1852,
                    2190,
                    2041
                ],
                "angle": 0,
                "content": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243."
            },
            {
                "block_id": 21,
                "type": "list",
                "bbox": [
                    1272,
                    270,
                    2190,
                    2041
                ],
                "angle": 0,
                "content": null
            },
            {
                "block_id": 22,
                "type": "page_number",
                "bbox": [
                    1215,
                    3234,
                    1270,
                    3279
                ],
                "angle": 0,
                "content": "92"
            }
        ]
    }
]